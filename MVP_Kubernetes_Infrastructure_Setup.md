# MVP Kubernetes Infrastructure Setup

This document tracks the setup and testing of the Kubernetes infrastructure components needed for the MVP data pipeline demonstration.

## Overview

**Goal:** Stand up the essential Kubernetes infrastructure to test our generated DAGs and demonstrate the MVP data pipeline with the data change simulator.

**Components to Deploy:**
- ‚úÖ PostgreSQL database (for Airflow metadata and data platform storage)
- ‚úÖ Airflow (scheduler, webserver, executor)
- ‚úÖ DataSurface job container (for SnapshotMergeJob execution)
- ‚úÖ Data Change Simulator (in its own pod for easy start/stop)

**NOT Included Yet:** Kafka, Kafka Connect (SQL snapshot ingestion only)

## Prerequisites

- ‚úÖ Docker Desktop with Kubernetes enabled
- ‚úÖ Generated MVP infrastructure artifacts in `src/tests/yellow_dp_tests/mvp_model/generated_output/`
- ‚úÖ Working `customer_db` database on localhost
- ‚úÖ Tested data change simulator
- üîê **GitHub Personal Access Token** with access to `billynewport/mvpmodel` repository

**üõ°Ô∏è Security Requirements:**
- Replace all instances of `MASKED_PAT` in commands with your actual GitHub token
- Never commit actual token values to version control
- Use environment variables or secure secret management in production

## Phase 1: Docker Container Preparation

### Task 1.1: Build Current DataSurface Container ‚úÖ **COMPLETED**

**Objective:** ‚úÖ Build a current Docker image with the latest DataSurface code including MVP features.

**Steps:**
1. ‚úÖ **Review and update Dockerfile.datasurface if needed**
   - ‚úÖ All dependencies included (psycopg2-binary, libpq-dev, etc.)
   - ‚úÖ Python 3.13-slim compatibility verified
   - ‚úÖ Latest src/ code included in build

2. ‚úÖ **Build the Docker image**
   ```bash
   docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
   # ‚úÖ Built successfully in 32.8s
   ```

3. ‚úÖ **Test the container locally**
   ```bash
   docker run --rm datasurface/datasurface:latest python -c "
   import datasurface
   from datasurface.cmd.platform import handleModelMerge
   print('DataSurface imports working')
   print('Version info and capabilities check complete')
   "
   # ‚úÖ Output: DataSurface imports working, Version info and capabilities check complete
   ```

4. ‚úÖ **Final container build for GitHub-based model access**
   ```bash
   docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
   # ‚úÖ Container ready for GitHub cloning (no model baked in)
   ```

5. ‚úÖ **Verified GitHub-based model access**
   ```bash
   docker run --rm -e GIT_TOKEN=MASKED_PAT datasurface/datasurface:latest bash -c "
   cd /workspace/model
   git clone https://\$GIT_TOKEN@github.com/billynewport/mvpmodel.git .
   python -c 'import sys; sys.path.append(\"/workspace/model\"); from eco import createEcosystem; eco = createEcosystem(); print(f\"‚úÖ Ecosystem loaded from GitHub: {eco.name}\"); print(f\"‚úÖ Platforms: {list(eco.dataPlatforms.keys())}\")'
   "
   # ‚úÖ Output: MVP model successfully loaded from billynewport/mvpmodel repository!
   ```

**Success Criteria:**
- ‚úÖ Docker image builds without errors
- ‚úÖ DataSurface imports work correctly
- ‚úÖ MVP ecosystem model loads successfully in container
- ‚úÖ All required dependencies available

### Task 1.2: Test Data Change Simulator in Container ‚úÖ **COMPLETED**

**Objective:** ‚úÖ Verify the data change simulator works in a containerized environment.

**Steps:**
1. ‚úÖ **Test simulator in container with external database**
   ```bash
   docker run --rm --network host \
     -v $(pwd)/src/tests:/app/tests \
     datasurface/datasurface:latest \
     python /app/tests/data_change_simulator.py \
     --host host.docker.internal \
     --database customer_db \
     --max-changes 3 \
     --min-interval 1 \
     --max-interval 1 \
     --verbose
   # ‚úÖ Output: Successfully created 1 customer, added 2 addresses, all changes persisted
   ```

**Success Criteria:**
- ‚úÖ Simulator connects to external database successfully
- ‚úÖ Database changes are persisted correctly
- ‚úÖ Container networking works for database access

**Test Results:**
- ‚úÖ Added address A52696520823 for customer C52688413877 (set as billing)
- ‚úÖ Created customer C52696521245 (Sam Davis) with address A52696521336  
- ‚úÖ Added address A52696522407 for customer CUST001 (set as billing)
- ‚úÖ All 3 changes completed successfully and database connection closed properly

## Phase 2: Kubernetes Secrets and Configuration ‚úÖ **COMPLETED**

### Task 2.1: Create Required Kubernetes Secrets ‚úÖ **COMPLETED**

**Objective:** ‚úÖ Set up all secrets needed for the MVP infrastructure.

**Steps:**
1. ‚úÖ **Create namespace**
   ```bash
   kubectl create namespace ns-kub-pg-test
   # ‚úÖ Namespace already existed from previous testing
   ```

2. ‚úÖ **Create PostgreSQL credentials secret**
   ```bash
   kubectl create secret generic postgres \
     --namespace ns-kub-pg-test \
     --from-literal=username=postgres \
     --from-literal=password=datasurface-test-123 \
     --from-literal=POSTGRES_USER=postgres \
     --from-literal=POSTGRES_PASSWORD=datasurface-test-123
   # ‚úÖ Updated secret to include both DAG-expected keys and standard PostgreSQL keys
   ```

3. ‚úÖ **Git, Slack, Connect credentials already exist**
   ```bash
   # ‚úÖ All required secrets verified present: git, slack, connect, airflow
   ```

4. ‚úÖ **Verify secrets created**
   ```bash
   kubectl get secrets -n ns-kub-pg-test
   # ‚úÖ Output: 5 secrets (airflow, connect, git, postgres, slack)
   ```

**Success Criteria:**
- ‚úÖ All required secrets exist in the namespace
- ‚úÖ Secret keys match what's expected by the generated DAGs (username/password)
- ‚úÖ No sensitive data exposed in commands or logs

### Task 2.2: GitHub-Based Model Access ‚úÖ **COMPLETED** (Production Approach)

**Objective:** ‚úÖ Set up proper GitHub-based model access for production deployment.

**Final Solution:** Clone MVP ecosystem model from GitHub repository at runtime.

**Why This Approach is Correct:**
- ‚úÖ **Production-Ready:** Models are versioned in GitHub as intended
- ‚úÖ **Secure:** Uses GitHub personal access tokens for private repository access
- ‚úÖ **Flexible:** Model changes can be deployed without rebuilding containers
- ‚úÖ **Scalable:** Standard GitOps pattern for configuration management

**Repository Setup:**
- ‚úÖ **Repository:** `billynewport/mvpmodel` (private)
- ‚úÖ **Contents:** `eco.py`, `dsg_platform_mapping.json`
- ‚úÖ **Access:** GitHub personal access token with repo permissions

**Implementation Steps:**
1. ‚úÖ **Created GitHub secret with correct key name:**
   ```bash
   kubectl create secret generic git \
     --namespace ns-kub-pg-test \
     --from-literal=token=MASKED_PAT
   # ‚úÖ Secret created with 'token' key as expected by generated DAGs
   ```

2. ‚úÖ **Verified GitHub repository access and cloning:**
   ```bash
   # Test cloning from billynewport/mvpmodel
   docker run --rm -e GIT_TOKEN=MASKED_PAT datasurface/datasurface:latest bash -c "
   cd /workspace/model
   git clone https://\$GIT_TOKEN@github.com/billynewport/mvpmodel.git .
   ls -la  # ‚úÖ Shows: eco.py, dsg_platform_mapping.json
   "
   ```

3. ‚úÖ **Verified ecosystem model loading from GitHub:**
   ```bash
   # ‚úÖ Output: Ecosystem loaded from GitHub: Test
   # ‚úÖ Output: Platforms: ['YellowLive', 'YellowForensic']
   # ‚úÖ Output: MVP model successfully loaded from billynewport/mvpmodel repository!
   ```

4. ‚úÖ **Created ConfigMaps for DAG volume expectations:**
   ```bash
   kubectl create configmap yellowlive-git-config \
     --namespace ns-kub-pg-test \
     --from-literal=repo_url=https://github.com/billynewport/mvpmodel.git
   
   kubectl create configmap yellowforensic-git-config \
     --namespace ns-kub-pg-test \
     --from-literal=repo_url=https://github.com/billynewport/mvpmodel.git
   # ‚úÖ ConfigMaps created to satisfy generated DAG volume mount requirements
   ```

**Success Criteria:**
- ‚úÖ GitHub repository `billynewport/mvpmodel` accessible with provided token
- ‚úÖ Repository contains correct MVP ecosystem model files
- ‚úÖ Model loads successfully from cloned repository in container
- ‚úÖ Kubernetes secrets and ConfigMaps ready for DAG execution

**üîê Security Note:**
- **GitHub PAT tokens are masked in this documentation** as `MASKED_PAT`
- **Actual token values should never be committed to version control**
- **In production, use secure secret management** (Kubernetes secrets, HashiCorp Vault, etc.)
- **Rotate tokens regularly** and use least-privilege access principles

## Phase 3: Core Infrastructure Deployment

### Task 3.1: Deploy PostgreSQL ‚è≥ **IN PROGRESS**

**Objective:** Deploy PostgreSQL for Airflow metadata and data platform storage.

**Steps:**
1. ‚è≥ **Extract PostgreSQL configuration from generated YAML**
   ```bash
   # Extract postgres section from YellowLive kubernetes-bootstrap.yaml
   # Review configuration for any needed modifications
   ```

2. ‚è≥ **Deploy PostgreSQL**
   ```bash
   # Apply the PostgreSQL portions of the generated kubernetes-bootstrap.yaml
   # OR use a simplified PostgreSQL deployment for testing
   ```

3. ‚è≥ **Verify PostgreSQL deployment**
   ```bash
   kubectl get pods -n ns-kub-pg-test -l app=postgres
   kubectl logs -n ns-kub-pg-test deployment/pg-data
   ```

4. ‚è≥ **Test PostgreSQL connectivity**
   ```bash
   kubectl run postgres-client --rm -i --tty --image postgres:16 \
     --namespace ns-kub-pg-test -- \
     psql -h pg-data -U postgres -d postgres
   ```

**Success Criteria:**
- [ ] PostgreSQL pod is running and healthy
- [ ] Database is accessible from within the cluster
- [ ] Credentials work correctly
- [ ] Required databases can be created

### Task 3.2: Deploy Airflow ‚è≥ **IN PROGRESS**

**Objective:** Deploy Airflow scheduler and webserver for DAG execution.

**Steps:**
1. ‚è≥ **Extract Airflow configuration from generated YAML**
   ```bash
   # Review airflow sections in kubernetes-bootstrap.yaml
   # Identify necessary components (scheduler, webserver, executor)
   ```

2. ‚è≥ **Deploy Airflow components**
   ```bash
   # Apply Airflow portions of the generated kubernetes-bootstrap.yaml
   # Ensure proper database connectivity
   ```

3. ‚è≥ **Initialize Airflow database**
   ```bash
   # Run airflow db init if needed
   # Create admin user
   ```

4. ‚è≥ **Verify Airflow deployment**
   ```bash
   kubectl get pods -n ns-kub-pg-test -l app=airflow
   kubectl port-forward -n ns-kub-pg-test service/airflow 8080:8080
   # Access http://localhost:8080
   ```

**Success Criteria:**
- [ ] Airflow scheduler is running
- [ ] Airflow webserver is accessible
- [ ] DAGs directory is properly mounted
- [ ] Database connectivity works

## Phase 4: DAG Deployment and Testing

### Task 4.1: Deploy Generated DAGs ‚è≥ **IN PROGRESS**

**Objective:** Make the generated ingestion and infrastructure DAGs available to Airflow.

**Steps:**
1. ‚è≥ **Create DAG ConfigMaps**
   ```bash
   kubectl create configmap yellowlive-ingestion-dag \
     --namespace ns-kub-pg-test \
     --from-file=yellowlive__Store1_ingestion.py=src/tests/yellow_dp_tests/mvp_model/generated_output/YellowLive/yellowlive__Store1_ingestion.py
   
   kubectl create configmap yellowlive-infrastructure-dag \
     --namespace ns-kub-pg-test \
     --from-file=yellowlive_infrastructure_dag.py=src/tests/yellow_dp_tests/mvp_model/generated_output/YellowLive/yellowlive_infrastructure_dag.py
   
   kubectl create configmap yellowforensic-ingestion-dag \
     --namespace ns-kub-pg-test \
     --from-file=yellowforensic__Store1_ingestion.py=src/tests/yellow_dp_tests/mvp_model/generated_output/YellowForensic/yellowforensic__Store1_ingestion.py
   
   kubectl create configmap yellowforensic-infrastructure-dag \
     --namespace ns-kub-pg-test \
     --from-file=yellowforensic_infrastructure_dag.py=src/tests/yellow_dp_tests/mvp_model/generated_output/YellowForensic/yellowforensic_infrastructure_dag.py
   ```

2. ‚è≥ **Mount DAGs in Airflow deployment**
   ```bash
   # Modify Airflow deployment to mount DAG ConfigMaps
   # Ensure DAGs are visible in Airflow UI
   ```

3. ‚è≥ **Verify DAGs loaded**
   ```bash
   # Check Airflow UI for DAG visibility
   # Ensure no parse errors
   ```

**Success Criteria:**
- [ ] All 4 DAGs are visible in Airflow UI
- [ ] No parsing errors in DAGs
- [ ] DAG configuration looks correct

### Task 4.2: Test Individual DAG Components ‚è≥ **IN PROGRESS**

**Objective:** Verify each DAG component works before full pipeline testing.

**Steps:**
1. ‚è≥ **Test KubernetesPodOperator configuration**
   ```bash
   # Manually trigger a simple task to verify pod creation
   # Check that secrets and ConfigMaps are properly mounted
   ```

2. ‚è≥ **Test credential access**
   ```bash
   # Verify that jobs can access postgres, git, and other secrets
   # Test ConfigMap mounting for ecosystem model
   ```

3. ‚è≥ **Test DataSurface job execution**
   ```bash
   # Run a simple DataSurface command in a pod
   # Verify ecosystem model loading
   ```

**Success Criteria:**
- [ ] Pods can be created successfully
- [ ] Secrets are accessible from job pods
- [ ] ConfigMaps are mounted correctly
- [ ] DataSurface commands execute successfully

## Phase 5: Data Change Simulator Pod

### Task 5.1: Deploy Simulator as Kubernetes Job ‚è≥ **IN PROGRESS**

**Objective:** Run the data change simulator in its own pod for easy management.

**Steps:**
1. ‚è≥ **Create simulator deployment YAML**
   ```yaml
   # Create a simple deployment or job for the simulator
   # Include database connectivity
   # Allow for easy start/stop
   ```

2. ‚è≥ **Deploy simulator**
   ```bash
   kubectl apply -f simulator-deployment.yaml -n ns-kub-pg-test
   ```

3. ‚è≥ **Test simulator connectivity**
   ```bash
   # Verify simulator can connect to customer_db
   # Test database modifications
   ```

**Success Criteria:**
- [ ] Simulator pod runs successfully
- [ ] Database connectivity works
- [ ] Changes are persisted to customer_db
- [ ] Pod can be easily stopped and started

## Phase 6: Integration Testing

### Task 6.1: Test Complete Infrastructure ‚è≥ **IN PROGRESS**

**Objective:** Verify all components work together correctly.

**Steps:**
1. ‚è≥ **Start data change simulator**
   ```bash
   # Deploy simulator with continuous changes
   ```

2. ‚è≥ **Manually trigger ingestion DAGs**
   ```bash
   # Trigger YellowLive and YellowForensic ingestion DAGs
   # Monitor job execution
   ```

3. ‚è≥ **Verify data processing**
   ```bash
   # Check that SnapshotMergeJob executes successfully
   # Verify data platform storage receives data
   ```

4. ‚è≥ **Test DAG self-triggering**
   ```bash
   # Verify that DAGs reschedule correctly based on return codes
   # Test continuous processing capability
   ```

**Success Criteria:**
- [ ] Simulator generates continuous database changes
- [ ] Ingestion DAGs execute successfully
- [ ] Data is processed and stored correctly
- [ ] Self-triggering mechanism works
- [ ] Both live and forensic platforms process data

## Success Criteria for Complete Phase

‚úÖ **Infrastructure Ready Checklist:**
- [ ] DataSurface container built and tested
- [ ] All Kubernetes secrets created correctly
- [ ] PostgreSQL and Airflow deployed and operational
- [ ] All 4 generated DAGs loaded and parseable
- [ ] Data change simulator running in pod
- [ ] Manual DAG execution successful
- [ ] Data flows from customer_db through to data platforms
- [ ] Ready for end-to-end pipeline validation

## Next Steps After Completion

Once this infrastructure setup is complete, we'll be ready for:
1. **Task 3.2: End-to-End Pipeline Validation** - Full automated pipeline testing
2. **Consumer database and view creation testing**
3. **Performance and latency validation**
4. **Integration with MERGE Handler (Task 4.2)**

## Troubleshooting Notes

**Common Issues:**
- Database connectivity from pods
- Secret mounting and environment variables
- ConfigMap file permissions
- Airflow DAG parsing errors
- Resource limits and scheduling

**Debugging Commands:**
```bash
# Check pod logs
kubectl logs -n ns-kub-pg-test <pod-name>

# Exec into pod for debugging
kubectl exec -it -n ns-kub-pg-test <pod-name> -- /bin/bash

# Check secret contents
kubectl get secret -n ns-kub-pg-test <secret-name> -o yaml

# Verify ConfigMap contents
kubectl get configmap -n ns-kub-pg-test <configmap-name> -o yaml
```

---

**Status:** üöÄ **Ready to Begin Infrastructure Setup**
**Estimated Time:** 2-3 hours for complete setup and testing
**Dependencies:** Docker Desktop Kubernetes, customer_db database, generated artifacts 