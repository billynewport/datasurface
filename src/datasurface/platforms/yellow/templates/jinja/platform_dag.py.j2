"""
Platform DAG for {{ platform_name }}
Generated automatically by DataSurface KubernetesPGStarter Platform

This DAG handles all ingestion streams for the platform. Each stream has its own task
that runs the SnapshotMergeJob and handles the return code to decide whether to
reschedule immediately, wait for next trigger, or fail.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.operators.empty import EmptyOperator
from kubernetes.client import models as k8s
from airflow.operators.python import BranchPythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.utils.trigger_rule import TriggerRule

# Default arguments for the DAG
default_args = {
    'owner': 'datasurface',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,  # No retries - the job handles its own retry logic
    'retry_delay': timedelta(minutes=5),
}

# Create the DAG
dag = DAG(
    '{{ platform_name }}_ingestion',
    default_args=default_args,
    description='Platform DAG for {{ platform_name }} - handles all ingestion streams',
    schedule='@hourly',  # External trigger schedule
    catchup=False,
    tags=['datasurface', 'ingestion', '{{ platform_name }}']
)

# Start task
start_task = EmptyOperator(
    task_id='start_platform',
    dag=dag
)

# Environment variables for all tasks
common_env_vars = {
    # Postgres credentials (for merge store)
    '{{ postgres_credential_secret_name }}_USER': {
        'secret_name': '{{ postgres_credential_secret_name }}',
        'secret_key': 'username'
    },
    '{{ postgres_credential_secret_name }}_PASSWORD': {
        'secret_name': '{{ postgres_credential_secret_name }}',
        'secret_key': 'password'
    },
    # Git credentials
    '{{ git_credential_secret_name }}_TOKEN': {
        'secret_name': '{{ git_credential_secret_name }}',
        'secret_key': 'token'
    },
    # Slack credentials
    '{{ slack_credential_secret_name }}_TOKEN': {
        'secret_name': '{{ slack_credential_secret_name }}',
        'secret_key': 'token'
    },
    # Platform configuration
    'DATASURFACE_PLATFORM_NAME': '{{ platform_name }}',
    'DATASURFACE_NAMESPACE': '{{ namespace_name }}',
    'DATASURFACE_SLACK_CHANNEL': '{{ slack_channel_name }}'
}

# Function to determine next action based on job return code
def determine_next_action(**context):
    """Determine the next action based on the job return code"""
    task_instance = context['task_instance']
    return_code = task_instance.xcom_pull(task_ids=context['task'].upstream_task_ids[0].task_id)
    
    if return_code == 1:  # KEEP_WORKING
        return 'reschedule_immediately'
    elif return_code == 0:  # DONE
        return 'wait_for_trigger'
    else:  # ERROR (-1)
        return 'fail'

# Create tasks for each ingestion stream
{% for stream in ingestion_streams %}
# Task for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_job = KubernetesPodOperator(
    task_id='{{ stream.stream_key | replace("-", "_") }}_job',
    name='{{ platform_name }}-{{ stream.stream_key | replace("-", "_") }}-job',
    namespace='{{ namespace_name }}',
    image='{{ datasurface_docker_image }}',
    cmds=['python', '-m', 'datasurface.platforms.kubpgstarter.jobs'],
    arguments=[
        '--platform-name', '{{ platform_name }}',
        '--store-name', '{{ stream.store_name }}',
        {% if stream.dataset_name %}
        '--dataset-name', '{{ stream.dataset_name }}',
        {% endif %}
        '--operation', 'snapshot-merge',
        '--git-repo-path', '/workspace/model'
    ],
    env_vars=common_env_vars,  # type: ignore
    volumes=[
        k8s.V1Volume(
            name='git-workspace',
            config_map=k8s.V1ConfigMapVolumeSource(name='{{ platform_name }}-git-config')
        )
    ],
    volume_mounts=[
        k8s.V1VolumeMount(
            name='git-workspace',
            mount_path='/workspace/model',
            read_only=True
        )
    ],
    is_delete_operator_pod=True,
    get_logs=True,
    dag=dag
)

# Branch operator for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_branch = BranchPythonOperator(
    task_id='{{ stream.stream_key | replace("-", "_") }}_branch',
    python_callable=determine_next_action,
    dag=dag
)

# Reschedule immediately for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_reschedule = TriggerDagRunOperator(
    task_id='{{ stream.stream_key | replace("-", "_") }}_reschedule',
    trigger_dag_id='{{ platform_name }}_ingestion',
    conf={'triggered_by': '{{ stream.stream_key }}_reschedule'},
    dag=dag
)

# Wait for trigger for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_wait = EmptyOperator(
    task_id='{{ stream.stream_key | replace("-", "_") }}_wait',
    dag=dag
)

# Fail for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_fail = EmptyOperator(
    task_id='{{ stream.stream_key | replace("-", "_") }}_fail',
    dag=dag
)

# Set up dependencies for {{ stream.stream_key }}
{{ stream.stream_key | replace("-", "_") }}_job >> {{ stream.stream_key | replace("-", "_") }}_branch
{{ stream.stream_key | replace("-", "_") }}_branch >> [{{ stream.stream_key | replace("-", "_") }}_reschedule, {{ stream.stream_key | replace("-", "_") }}_wait, {{ stream.stream_key | replace("-", "_") }}_fail]

{% endfor %}

# End task
end_task = EmptyOperator(
    task_id='end_platform',
    trigger_rule=TriggerRule.NONE_FAILED_MIN_ONE_SUCCESS,
    dag=dag
)

# Connect all wait tasks to end task
{% for stream in ingestion_streams %}
{{ stream.stream_key | replace("-", "_") }}_wait >> end_task
{% endfor %}

# Connect start task to all jobs
start_task >> [
{% for stream in ingestion_streams %}
    {{ stream.stream_key | replace("-", "_") }}_job{% if not loop.last %},{% endif %}
{% endfor %}
] 