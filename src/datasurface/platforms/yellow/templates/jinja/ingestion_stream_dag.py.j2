"""
Ingestion Stream DAG for {{ platform_name }}__{{ stream_key }}
Generated automatically by DataSurface KubernetesPGStarter Platform

This DAG handles a single ingestion stream. It runs the SnapshotMergeJob and handles
the return code to decide whether to reschedule immediately, wait for next trigger, or fail.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.utils.trigger_rule import TriggerRule
from kubernetes.client import models as k8s

# Default arguments for the DAG
default_args = {
    'owner': 'datasurface',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,  # No retries - the job handles its own retry logic
    'retry_delay': timedelta(minutes=5),
}

# Create the DAG
dag = DAG(
    '{{ platform_name }}__{{ stream_key }}_ingestion',
    default_args=default_args,
    description='Ingestion Stream DAG for {{ platform_name }}__{{ stream_key }}',
    schedule='@hourly',  # External trigger schedule
    catchup=False,
    tags=['datasurface', 'ingestion', '{{ platform_name }}', '{{ stream_key }}']
)

# Environment variables for the job - combining literal and secret-based vars
env_vars = [
    # Platform configuration (literal values)
    k8s.V1EnvVar(name='DATASURFACE_PLATFORM_NAME', value='{{ platform_name }}'),
    k8s.V1EnvVar(name='DATASURFACE_NAMESPACE', value='{{ namespace_name }}'),
    k8s.V1EnvVar(name='DATASURFACE_SLACK_CHANNEL', value='{{ slack_channel_name }}'),
    
    # Postgres credentials (for merge store)
    k8s.V1EnvVar(
        name='{{ postgres_credential_secret_name }}_USER',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ postgres_credential_secret_name }}',
                key='username'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ postgres_credential_secret_name }}_PASSWORD',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ postgres_credential_secret_name }}',
                key='password'
            )
        )
    ),
    # Git credentials
    k8s.V1EnvVar(
        name='{{ git_credential_secret_name }}_TOKEN',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ git_credential_secret_name }}',
                key='token'
            )
        )
    ),
    # Slack credentials
    k8s.V1EnvVar(
        name='{{ slack_credential_secret_name }}_TOKEN',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ slack_credential_secret_name }}',
                key='token'
            )
        )
    )
]

{% if ingestion_type == "sql_snapshot" and source_credential_secret_name and source_credential_secret_name != postgres_credential_secret_name %}
# Source database credentials for SQL snapshot ingestion (only if different from merge store)
env_vars.extend([
    k8s.V1EnvVar(
        name='{{ source_credential_secret_name }}_USER',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ source_credential_secret_name }}',
                key='username'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ source_credential_secret_name }}_PASSWORD',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ source_credential_secret_name }}',
                key='password'
            )
        )
    )
])
{% endif %}

{% if ingestion_type == "kafka" and kafka_connect_credential_secret_name %}
# Kafka Connect credentials for Kafka ingestion
env_vars.extend([
    k8s.V1EnvVar(
        name='{{ kafka_connect_credential_secret_name }}_API_KEY',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ kafka_connect_credential_secret_name }}',
                key='api_key'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ kafka_connect_credential_secret_name }}_API_SECRET',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ kafka_connect_credential_secret_name }}',
                key='api_secret'
            )
        )
    )
])
{% endif %}

# Function to determine next action based on job return code
def determine_next_action(**context):
    """Determine the next action based on the job return code parsed from logs"""
    import re
    
    task_instance = context['task_instance']
    job_task_id = 'snapshot_merge_job'
    
    # Get logs from XCom (KubernetesPodOperator with do_xcom_push=True captures logs)
    logs = task_instance.xcom_pull(task_ids=job_task_id)
    
    # Parse logs to find DATASURFACE_RESULT_CODE=X
    if logs:
        match = re.search(r'DATASURFACE_RESULT_CODE=(-?\d+)', str(logs))
        return_code = int(match.group(1)) if match else -1
    else:
        return_code = -1  # Default to error if no logs
    
    print(f"Parsed result code: {return_code} from logs")
    
    if return_code == 1:  # KEEP_WORKING
        return 'reschedule_immediately'
    elif return_code == 0:  # DONE
        return 'wait_for_trigger'
    else:  # ERROR (-1)
        raise Exception(f"SnapshotMergeJob failed with code {return_code} - manual intervention required")

# Job task
job = KubernetesPodOperator(
    task_id='snapshot_merge_job',
    name='{{ platform_name }}-{{ stream_key | replace("-", "_") }}-job',
    namespace='{{ namespace_name }}',
    image='{{ datasurface_docker_image }}',
    cmds=['python', '-m', 'datasurface.platforms.yellow.jobs'],
    arguments=[
        '--platform-name', 'YellowLive',
        '--store-name', '{{ store_name }}',
        {% if dataset_name %}
        '--dataset-name', '{{ dataset_name }}',
        {% endif %}
        '--operation', 'snapshot-merge',
        '--git-repo-path', '/workspace/model'
    ],
    env_vars=env_vars,  # type: ignore
    image_pull_policy='Always',
    volumes=[
        k8s.V1Volume(
            name='git-workspace',
            empty_dir=k8s.V1EmptyDirVolumeSource()
        )
    ],
    volume_mounts=[
        k8s.V1VolumeMount(
            name='git-workspace',
            mount_path='/workspace/model',
            read_only=False
        )
    ],
    is_delete_operator_pod=True,
    get_logs=True,
    do_xcom_push=True,
    container_resources=k8s.V1ResourceRequirements(
        requests={'memory': '256Mi', 'cpu': '100m'}
    ),
    on_finish_action="delete_pod",
    dag=dag
)

# Branch operator
branch = BranchPythonOperator(
    task_id='check_result',
    python_callable=determine_next_action,
    dag=dag
)

# Reschedule immediately
reschedule = TriggerDagRunOperator(
    task_id='reschedule_immediately',
    trigger_dag_id='{{ platform_name }}__{{ stream_key }}_ingestion',  # Trigger the SAME DAG
    conf={'triggered_by': 'reschedule'},
    dag=dag
)

# Wait for trigger
wait = EmptyOperator(
    task_id='wait_for_trigger',
    dag=dag
)

# Set up dependencies
job >> branch
branch >> [reschedule, wait] 