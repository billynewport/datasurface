"""
Ingestion Stream DAG for {{ platform_name }}__{{ stream_key }}
Generated automatically by DataSurface KubernetesPGStarter Platform

This DAG handles a single ingestion stream. It runs the SnapshotMergeJob and handles
the return code to decide whether to reschedule immediately, wait for next trigger, or fail.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from kubernetes.client import models as k8s

# Default arguments for the DAG
default_args = {
    'owner': 'datasurface',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 0,  # No retries - the job handles its own retry logic
    'retry_delay': timedelta(minutes=5),
}

# Create the DAG
dag = DAG(
    '{{ platform_name }}__{{ stream_key }}_ingestion',
    default_args=default_args,
    description='Ingestion Stream DAG for {{ platform_name }}__{{ stream_key }}',
    schedule='@hourly',  # External trigger schedule
    catchup=False,
    tags=['datasurface', 'ingestion', '{{ platform_name }}', '{{ stream_key }}']
)

# Environment variables for the job - combining literal and secret-based vars
env_vars = [
    # Platform configuration (literal values)
    k8s.V1EnvVar(name='DATASURFACE_PLATFORM_NAME', value='{{ platform_name }}'),
    k8s.V1EnvVar(name='DATASURFACE_NAMESPACE', value='{{ namespace_name }}'),
    k8s.V1EnvVar(name='DATASURFACE_SLACK_CHANNEL', value='{{ slack_channel_name }}'),
    
    # Postgres credentials (for merge store)
    k8s.V1EnvVar(
        name='{{ postgres_credential_secret_name }}_USER',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ postgres_credential_secret_name }}',
                key='username'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ postgres_credential_secret_name }}_PASSWORD',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ postgres_credential_secret_name }}',
                key='password'
            )
        )
    ),
    # Git credentials
    k8s.V1EnvVar(
        name='{{ git_credential_secret_name }}_TOKEN',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ git_credential_secret_name }}',
                key='token'
            )
        )
    ),
    # Slack credentials
    k8s.V1EnvVar(
        name='{{ slack_credential_secret_name }}_TOKEN',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ slack_credential_secret_name }}',
                key='token'
            )
        )
    )
]

{% if ingestion_type == "sql_snapshot" and source_credential_secret_name and source_credential_secret_name != postgres_credential_secret_name %}
# Source database credentials for SQL snapshot ingestion (only if different from merge store)
env_vars.extend([
    k8s.V1EnvVar(
        name='{{ source_credential_secret_name }}_USER',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ source_credential_secret_name }}',
                key='username'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ source_credential_secret_name }}_PASSWORD',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ source_credential_secret_name }}',
                key='password'
            )
        )
    )
])
{% endif %}

{% if ingestion_type == "kafka" and kafka_connect_credential_secret_name %}
# Kafka Connect credentials for Kafka ingestion
env_vars.extend([
    k8s.V1EnvVar(
        name='{{ kafka_connect_credential_secret_name }}_API_KEY',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ kafka_connect_credential_secret_name }}',
                key='api_key'
            )
        )
    ),
    k8s.V1EnvVar(
        name='{{ kafka_connect_credential_secret_name }}_API_SECRET',
        value_from=k8s.V1EnvVarSource(
            secret_key_ref=k8s.V1SecretKeySelector(
                name='{{ kafka_connect_credential_secret_name }}',
                key='api_secret'
            )
        )
    )
])
{% endif %}

# Function to determine next action based on job return code
def determine_next_action(**context):
    """Determine the next action based on the job return code parsed from Airflow task logs"""
    import re
    import os
    
    dag_run = context['dag_run']
    
    # Build path to the snapshot_merge_job log file
    log_dir = f"/opt/airflow/logs/dag_id={dag_run.dag_id}/run_id={dag_run.run_id}/task_id=snapshot_merge_job"
    
    return_code = -1  # Default to error
    
    try:
        # Find the latest attempt log file
        if os.path.exists(log_dir):
            attempt_files = [f for f in os.listdir(log_dir) if f.startswith('attempt=') and f.endswith('.log')]
            if attempt_files:
                latest_attempt = max(attempt_files)
                log_file = os.path.join(log_dir, latest_attempt)
                
                if os.path.exists(log_file):
                    with open(log_file, 'r') as f:
                        logs = f.read()
                    
                    # Parse logs to find DATASURFACE_RESULT_CODE=X
                    match = re.search(r'DATASURFACE_RESULT_CODE=(-?\d+)', logs)
                    if match:
                        return_code = int(match.group(1))
                        print(f"Found result code: {return_code} in task logs")
                    else:
                        print("No DATASURFACE_RESULT_CODE found in logs")
                else:
                    print(f"Log file not found: {log_file}")
            else:
                print(f"No attempt log files found in {log_dir}")
        else:
            print(f"Log directory not found: {log_dir}")
    except Exception as e:
        print(f"Error reading task logs: {e}")
    
    print(f"Final result code: {return_code}")
    
    if return_code == 1:  # KEEP_WORKING
        return 'reschedule_immediately'
    elif return_code == 0:  # DONE
        return 'wait_for_trigger'
    else:  # ERROR (-1)
        raise Exception(f"SnapshotMergeJob failed with code {return_code} - manual intervention required")

# Job task
job = KubernetesPodOperator(
    task_id='snapshot_merge_job',
    name='{{ platform_name }}-{{ stream_key | replace("-", "_") }}-job',
    namespace='{{ namespace_name }}',
    image='{{ datasurface_docker_image }}',
    cmds=['python', '-m', 'datasurface.platforms.yellow.jobs'],
    arguments=[
        '--platform-name', '{{ original_platform_name }}',
        '--store-name', '{{ store_name }}',
        {% if dataset_name %}
        '--dataset-name', '{{ dataset_name }}',
        {% endif %}
        '--operation', 'snapshot-merge',
        '--git-repo-path', '/workspace/model',
        '--git-repo-owner', '{{ git_repo_owner }}',
        '--git-repo-name', '{{ git_repo_repo_name }}',
        '--git-repo-branch', '{{ git_repo_branch }}'
    ],
    env_vars=env_vars,  # type: ignore
    image_pull_policy='Always',
    volumes=[
        k8s.V1Volume(
            name='git-workspace',
            empty_dir=k8s.V1EmptyDirVolumeSource()
        )
    ],
    volume_mounts=[
        k8s.V1VolumeMount(
            name='git-workspace',
            mount_path='/workspace/model',
            read_only=False
        )
    ],
    is_delete_operator_pod=True,
    get_logs=True,
    do_xcom_push=False,  # Disabled to avoid RBAC issues with pods/exec
    container_resources=k8s.V1ResourceRequirements(
        requests={'memory': '256Mi', 'cpu': '100m'}
    ),
    on_finish_action="delete_pod",
    dag=dag
)

# Branch operator
branch = BranchPythonOperator(
    task_id='check_result',
    python_callable=determine_next_action,
    dag=dag
)

# Reschedule immediately
reschedule = TriggerDagRunOperator(
    task_id='reschedule_immediately',
    trigger_dag_id='{{ platform_name }}__{{ stream_key }}_ingestion',  # Trigger the SAME DAG
    conf={'triggered_by': 'reschedule'},
    dag=dag
)

# Wait for trigger
wait = EmptyOperator(
    task_id='wait_for_trigger',
    dag=dag
)

# Set up dependencies
job >> branch
branch >> [reschedule, wait] 