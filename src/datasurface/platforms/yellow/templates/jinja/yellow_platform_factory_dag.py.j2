"""
Yellow Platform Dynamic DAG Factory for {{ platform_name }}
Generated automatically by DataSurface YellowDataPlatform

This factory reads from {{ platform_name }}'s {{ platform_name }}_airflow_dsg table
and creates individual DAGs for each ingestion stream.
"""

from datetime import datetime, timedelta
from airflow import DAG
from airflow.providers.cncf.kubernetes.operators.pod import KubernetesPodOperator
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator
from airflow.operators.trigger_dagrun import TriggerDagRunOperator
from airflow.providers.postgres.hooks.postgres import PostgresHook
from kubernetes.client import models as k8s
import json

def create_ingestion_stream_dag(platform_config: dict, stream_config: dict) -> DAG:
    """Create a single ingestion stream DAG from configuration"""
    platform_name = platform_config['platform_name']
    stream_key = stream_config['stream_key']
    
    # Default arguments for the DAG
    default_args = {
        'owner': 'datasurface',
        'depends_on_past': False,
        'start_date': datetime(2025, 1, 1),
        'email_on_failure': True,
        'email_on_retry': False,
        'retries': 0,  # No retries - the job handles its own retry logic
        'retry_delay': timedelta(minutes=5),
    }
    
    dag = DAG(
        f'{platform_name}__{stream_key}_ingestion',
        default_args=default_args,
        description=f'Ingestion Stream DAG for {platform_name}__{stream_key}',
        schedule='@hourly',
        catchup=False,
        tags=['datasurface', 'ingestion', platform_name, stream_key]
    )
    
    # Environment variables for the job - combining literal and secret-based vars
    env_vars = [
        # Platform configuration (literal values)
        k8s.V1EnvVar(name='DATASURFACE_PLATFORM_NAME', value=platform_config['original_platform_name']),
        k8s.V1EnvVar(name='DATASURFACE_NAMESPACE', value=platform_config['namespace']),
        k8s.V1EnvVar(name='DATASURFACE_SLACK_CHANNEL', value=platform_config['slack_channel_name']),
        
        # Postgres credentials (for merge store)
        k8s.V1EnvVar(
            name=f"{platform_config['postgres_credential_secret_name']}_USER",
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name=platform_config['postgres_credential_secret_name'],
                    key='username'
                )
            )
        ),
        k8s.V1EnvVar(
            name=f"{platform_config['postgres_credential_secret_name']}_PASSWORD",
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name=platform_config['postgres_credential_secret_name'],
                    key='password'
                )
            )
        ),
        # Git credentials
        k8s.V1EnvVar(
            name=f"{platform_config['git_credential_secret_name']}_TOKEN",
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name=platform_config['git_credential_secret_name'],
                    key='token'
                )
            )
        ),
        # Slack credentials
        k8s.V1EnvVar(
            name=f"{platform_config['slack_credential_secret_name']}_TOKEN",
            value_from=k8s.V1EnvVarSource(
                secret_key_ref=k8s.V1SecretKeySelector(
                    name=platform_config['slack_credential_secret_name'],
                    key='token'
                )
            )
        )
    ]
    
    # Add source database credentials for SQL snapshot ingestion (if different from merge store)
    if (stream_config.get('ingestion_type') == 'sql_snapshot' and 
        stream_config.get('source_credential_secret_name') and 
        stream_config['source_credential_secret_name'] != platform_config['postgres_credential_secret_name']):
        env_vars.extend([
            k8s.V1EnvVar(
                name=f"{stream_config['source_credential_secret_name']}_USER",
                value_from=k8s.V1EnvVarSource(
                    secret_key_ref=k8s.V1SecretKeySelector(
                        name=stream_config['source_credential_secret_name'],
                        key='username'
                    )
                )
            ),
            k8s.V1EnvVar(
                name=f"{stream_config['source_credential_secret_name']}_PASSWORD",
                value_from=k8s.V1EnvVarSource(
                    secret_key_ref=k8s.V1SecretKeySelector(
                        name=stream_config['source_credential_secret_name'],
                        key='password'
                    )
                )
            )
        ])
    
    # Add Kafka Connect credentials for Kafka ingestion
    if (stream_config.get('ingestion_type') == 'kafka' and 
        stream_config.get('kafka_connect_credential_secret_name')):
        env_vars.extend([
            k8s.V1EnvVar(
                name=f"{stream_config['kafka_connect_credential_secret_name']}_API_KEY",
                value_from=k8s.V1EnvVarSource(
                    secret_key_ref=k8s.V1SecretKeySelector(
                        name=stream_config['kafka_connect_credential_secret_name'],
                        key='api_key'
                    )
                )
            ),
            k8s.V1EnvVar(
                name=f"{stream_config['kafka_connect_credential_secret_name']}_API_SECRET",
                value_from=k8s.V1EnvVarSource(
                    secret_key_ref=k8s.V1SecretKeySelector(
                        name=stream_config['kafka_connect_credential_secret_name'],
                        key='api_secret'
                    )
                )
            )
        ])
    
    # Function to determine next action based on job return code
    def determine_next_action(**context):
        """Determine the next action based on the job return code parsed from Airflow task logs"""
        import re
        import os
        
        dag_run = context['dag_run']
        
        # Build path to the snapshot_merge_job log file
        log_dir = f"/opt/airflow/logs/dag_id={dag_run.dag_id}/run_id={dag_run.run_id}/task_id=snapshot_merge_job"
        
        return_code = -1  # Default to error
        
        try:
            # Find the latest attempt log file
            if os.path.exists(log_dir):
                attempt_files = [f for f in os.listdir(log_dir) if f.startswith('attempt=') and f.endswith('.log')]
                if attempt_files:
                    latest_attempt = max(attempt_files)
                    log_file = os.path.join(log_dir, latest_attempt)
                    
                    if os.path.exists(log_file):
                        with open(log_file, 'r') as f:
                            logs = f.read()
                        
                        # Parse logs to find DATASURFACE_RESULT_CODE=X
                        match = re.search(r'DATASURFACE_RESULT_CODE=(-?\d+)', logs)
                        if match:
                            return_code = int(match.group(1))
                            print(f"Found result code: {return_code} in task logs")
                        else:
                            print("No DATASURFACE_RESULT_CODE found in logs")
                    else:
                        print(f"Log file not found: {log_file}")
                else:
                    print(f"No attempt log files found in {log_dir}")
            else:
                print(f"Log directory not found: {log_dir}")
        except Exception as e:
            print(f"Error reading task logs: {e}")
        
        print(f"Final result code: {return_code}")
        
        if return_code == 1:  # KEEP_WORKING
            return 'reschedule_immediately'
        elif return_code == 0:  # DONE
            return 'wait_for_trigger'
        else:  # ERROR (-1)
            raise Exception(f"SnapshotMergeJob failed with code {return_code} - manual intervention required")
    
    # Build job arguments
    job_args = [
        '--platform-name', platform_config['original_platform_name'],
        '--store-name', stream_config['store_name'],
        '--operation', 'snapshot-merge',
        '--git-repo-path', '/workspace/model',
        '--git-repo-owner', platform_config['git_repo_owner'],
        '--git-repo-name', platform_config['git_repo_repo_name'],
        '--git-repo-branch', platform_config['git_repo_branch']
    ]
    
    # Add dataset name if present
    if stream_config.get('dataset_name'):
        job_args.extend(['--dataset-name', stream_config['dataset_name']])
    
    # Main job task
    job = KubernetesPodOperator(
        task_id='snapshot_merge_job',
        name=f"{platform_name}-{stream_key.replace('-', '_')}-job",
        namespace=platform_config['namespace'],
        image=platform_config['datasurface_docker_image'],
        cmds=['python', '-m', 'datasurface.platforms.yellow.jobs'],
        arguments=job_args,
        env_vars=env_vars,  # type: ignore
        image_pull_policy='Always',
        volumes=[
            k8s.V1Volume(
                name='git-workspace',
                empty_dir=k8s.V1EmptyDirVolumeSource()
            )
        ],
        volume_mounts=[
            k8s.V1VolumeMount(
                name='git-workspace',
                mount_path='/workspace/model',
                read_only=False
            )
        ],
        is_delete_operator_pod=True,
        get_logs=True,
        do_xcom_push=False,  # Disabled to avoid RBAC issues with pods/exec
        container_resources=k8s.V1ResourceRequirements(
            requests={'memory': '256Mi', 'cpu': '100m'}
        ),
        on_finish_action="delete_pod",
        dag=dag
    )
    
    # Branch operator
    branch = BranchPythonOperator(
        task_id='check_result',
        python_callable=determine_next_action,
        dag=dag
    )
    
    # Reschedule immediately
    reschedule = TriggerDagRunOperator(
        task_id='reschedule_immediately',
        trigger_dag_id=f'{platform_name}__{stream_key}_ingestion',  # Trigger the SAME DAG
        conf={'triggered_by': 'reschedule'},
        dag=dag
    )
    
    # Wait for trigger
    wait = EmptyOperator(
        task_id='wait_for_trigger',
        dag=dag
    )
    
    # Set up dependencies
    job >> branch
    branch >> [reschedule, wait]
    
    return dag

def load_platform_configurations():
    """Load configurations from database and create DAG objects"""
    generated_dags = {}
    
    try:
        # Connect to the merge database to read configurations
        postgres_hook = PostgresHook('{{ postgres_credential_secret_name }}')
        
        # Read from the platform-specific airflow_dsg table
        table_name = '{{ platform_name }}_airflow_dsg'
        configs = postgres_hook.get_records(f"""
            SELECT stream_key, config_json 
            FROM {table_name}
            WHERE status = 'active'
        """)
        
        # Platform configuration (same for all streams)
        platform_config = {
            'platform_name': '{{ platform_name }}',
            'original_platform_name': '{{ original_platform_name }}',
            'namespace': '{{ namespace_name }}',
            'datasurface_docker_image': '{{ datasurface_docker_image }}',
            'slack_channel_name': '{{ slack_channel_name }}',
            'postgres_credential_secret_name': '{{ postgres_credential_secret_name }}',
            'git_credential_secret_name': '{{ git_credential_secret_name }}',
            'slack_credential_secret_name': '{{ slack_credential_secret_name }}',
            'git_repo_owner': '{{ git_repo_owner }}',
            'git_repo_repo_name': '{{ git_repo_repo_name }}',
            'git_repo_branch': '{{ git_repo_branch }}'
        }
        
        # Create a DAG for each active stream
        for stream_key, config_json in configs:
            stream_config = json.loads(config_json)
            dag = create_ingestion_stream_dag(platform_config, stream_config)
            dag_id = f"{{ platform_name }}__{stream_key}_ingestion"
            generated_dags[dag_id] = dag
                
    except Exception as e:
        print(f"Error loading platform configurations: {e}")
        return {}
    
    return generated_dags

# Execute when Airflow scans this file
all_platform_dags = load_platform_configurations()

# Make DAGs available to Airflow
for dag_id, dag_object in all_platform_dags.items():
    globals()[dag_id] = dag_object 