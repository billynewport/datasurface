"""
// Copyright (c) William Newport
// SPDX-License-Identifier: BUSL-1.1
"""


import unittest
from datasurface.md import DataPlatform, DataTransformerNode, Ecosystem, EcosystemPipelineGraph, \
    ExportNode, IngestionNode, PipelineNode, PlatformPipelineGraph, TriggerNode, Workspace, PrioritizedWorkloadTier

from tests.nwdb.eco import createEcosystem
from typing import cast
from datasurface.md import WorkloadTier, WorkspacePriority
from datasurface.md import DatasetGroup, DatasetSink


class Test_PlatformGraphs(unittest.TestCase):

    def test_PipelineGraph(self):
        eco: Ecosystem = createEcosystem()

        legacyA: DataPlatform = eco.getDataPlatformOrThrow("LegacyA")
        self.assertEqual(eco.getDefaultDataPlatform(), legacyA)

        graph: EcosystemPipelineGraph = EcosystemPipelineGraph(eco)

        self.assertIsNotNone(graph.roots.get(legacyA.name))

        pi: PlatformPipelineGraph = graph.roots[legacyA.name]
        self.assertEqual(len(pi.workspaces), 3)

        self.assertEqual(len(pi.dataContainerConsumers), 1)

        # Left hand side of pipeline graph should just be ingestions
        ingestionRoots: set[PipelineNode] = pi.getLeftSideOfGraph()
        for ir in ingestionRoots:
            self.assertTrue(str(ir).startswith("Ingest"))

        # Right hand side of pipeline graph should be exports to assets
        rightHandLeafs: set[PipelineNode] = pi.getRightSideOfGraph()
        for rh in rightHandLeafs:
            self.assertTrue(str(rh).startswith("Export"))

        # Check every ingest is followed by an export
        self.assertTrue(pi.checkNextStepsForStepType(IngestionNode, ExportNode))
        # Check exports are only followed by a trigger
        self.assertTrue(pi.checkNextStepsForStepType(ExportNode, TriggerNode))
        # Check triggers are only followed by a datatransformer
        self.assertTrue(pi.checkNextStepsForStepType(TriggerNode, DataTransformerNode))
        # Check DataTransformers are followed by ingestion
        self.assertTrue(pi.checkNextStepsForStepType(DataTransformerNode, IngestionNode))

        graphStr: str = pi.graphToText()

        print(graphStr)

    def test_storesToIngestMatchesIngestionNodesInGraph(self):
        eco: Ecosystem = createEcosystem()
        legacyA: DataPlatform = eco.getDataPlatformOrThrow("LegacyA")
        graph: EcosystemPipelineGraph = EcosystemPipelineGraph(eco)
        pi: PlatformPipelineGraph = graph.roots[legacyA.name]

        # Ingestion nodes are all the nodes on the left hand side of the graph but can also occur within the graph.
        ingestionNodes: set[IngestionNode] = set()
        for node in pi.nodes.values():
            if isinstance(node, IngestionNode):
                ingestionNodes.add(node)

        # Just make a set of the store names
        ingestionStoreNames: set[str] = set()
        for node in ingestionNodes:
            ingestionStoreNames.add(node.storeName)

        self.assertEqual(len(pi.storesToIngest), len(ingestionStoreNames))

    def test_WorkspacePriority(self):
        p1: WorkspacePriority = PrioritizedWorkloadTier(WorkloadTier.CRITICAL)
        p2: WorkspacePriority = PrioritizedWorkloadTier(WorkloadTier.HIGH)
        self.assertTrue(p1.isMoreImportantThan(p2))
        self.assertFalse(p2.isMoreImportantThan(p1))

        p1 = PrioritizedWorkloadTier(WorkloadTier.MEDIUM)
        p2 = PrioritizedWorkloadTier(WorkloadTier.MEDIUM)
        self.assertFalse(p1.isMoreImportantThan(p2))

    def test_PipelineGraph_Priority(self):
        eco: Ecosystem = createEcosystem()

        legacyA: DataPlatform = eco.getDataPlatformOrThrow("LegacyA")
        self.assertEqual(eco.getDefaultDataPlatform(), legacyA)

        # There are already 3 workspaces in the ecosystem
        workspaceA: Workspace = eco.cache_getWorkspaceOrThrow("ProductLiveAdhocReporting").workspace
        workspaceB: Workspace = eco.cache_getWorkspaceOrThrow("MaskCustomersWorkSpace").workspace
        workspaceC: Workspace = eco.cache_getWorkspaceOrThrow("WorkspaceUsingTransformerOutput").workspace

        # A and B are both using the prime datastore, NW_Data.
        # C is using data produced by the transformer in B, Masked_NW_Data

        # These all use the default priority initially so verify its UNKNOWN
        p: PrioritizedWorkloadTier = cast(PrioritizedWorkloadTier, workspaceA.priority)
        self.assertEqual(p.priority, WorkloadTier.UNKNOWN)
        p: PrioritizedWorkloadTier = cast(PrioritizedWorkloadTier, workspaceB.priority)
        self.assertEqual(p.priority, WorkloadTier.UNKNOWN)
        p: PrioritizedWorkloadTier = cast(PrioritizedWorkloadTier, workspaceC.priority)
        self.assertEqual(p.priority, WorkloadTier.UNKNOWN)

        # Now set the priority of workspaceA and B to MEDIUM, set C to LOW
        workspaceA.add(PrioritizedWorkloadTier(WorkloadTier.MEDIUM))
        workspaceB.add(PrioritizedWorkloadTier(WorkloadTier.MEDIUM))
        workspaceC.add(PrioritizedWorkloadTier(WorkloadTier.LOW))

        # Recalculate the graph priorities
        graph: EcosystemPipelineGraph = EcosystemPipelineGraph(eco)
        pi: PlatformPipelineGraph = graph.roots[legacyA.name]
        pi.propagateWorkspacePriorities()

        # Check every node has a priority
        for node in pi.nodes.values():
            if node.priority is None:   
                self.fail(f"Node {node} has no priority")

        # Workspace A's left hand nodes (exports)should all be MEDIUM
        work_a_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceA)
        for en in work_a_export_nodes:
            self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.MEDIUM)

        # Workspace B's left hand nodes (exports) should all be MEDIUM
        work_b_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceB)
        for en in work_b_export_nodes:
            self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.MEDIUM)

        # Workspace C's left hand nodes should all be LOW
        work_c_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceC)
        for en in work_c_export_nodes:
            self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.LOW)

        # Now set the priority of workspaceA and B to MEDIUM, set C to LOW
        workspaceA.add(PrioritizedWorkloadTier(WorkloadTier.MEDIUM))
        workspaceB.add(PrioritizedWorkloadTier(WorkloadTier.MEDIUM))
        workspaceC.add(PrioritizedWorkloadTier(WorkloadTier.CRITICAL))

        # Recalculate the graph priorities
        graph = EcosystemPipelineGraph(eco)
        pi = graph.roots[legacyA.name]
        pi.propagateWorkspacePriorities()

        # Workspace A's left hand nodes should all be MEDIUM
        work_a_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceA)
        for en in work_a_export_nodes:
            # export customers is critical because the mask transformer uses it
            if en.datasetName == "customers":
                self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.CRITICAL)
            else:
                self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.MEDIUM)

        work_b_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceB)
        for en in work_b_export_nodes:
            self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.CRITICAL)

        def checkLeftwardNodePriorityIs(node: PipelineNode, expectedPriority: WorkloadTier):
            for nn in node.leftHandNodes.values():
                if nn.priority is None:
                    self.fail(f"Node {nn} has no priority")
                else:
                    self.assertTrue(cast(PrioritizedWorkloadTier, nn.priority).priority == expectedPriority)
                    checkLeftwardNodePriorityIs(nn, expectedPriority)

        # Masked_NW_Data.customers uses NW_Data.customers
        # NW_Data is used by A, B and indirectly by C.
        # NW_Data is multi-dataset ingestion so all datasets use a single node/priority.
        work_c_export_nodes: set[ExportNode] = pi.findAllExportNodesForWorkspace(workspaceC)
        for en in work_c_export_nodes:
            self.assertEqual(cast(PrioritizedWorkloadTier, en.priority).priority, WorkloadTier.CRITICAL)
            # Walk leftwards checking all nodes are CRITICAL
            checkLeftwardNodePriorityIs(en, WorkloadTier.CRITICAL)

        # Check ingestion of NW_Data is CRITICAL

    def test_datatransformer_self_reference_within_workspace(self):
        """Test that a DataTransformer workspace can have DatasetSinks that reference its own output datastore,
        creating a self-reference within the same workspace."""
        eco: Ecosystem = createEcosystem()
        
        # Get the existing MaskCustomersWorkSpace and modify it to add self-reference
        mask_workspace: Workspace = eco.cache_getWorkspaceOrThrow("MaskCustomersWorkSpace").workspace
        mask_dsg: DatasetGroup = mask_workspace.dsgs["MaskCustomers"]
        
        # Add a self-reference: DataTransformer uses its own output as input
        # Output datastore: "Masked_NW_Data", output dataset: "customers"
        # Add sink: Masked_NW_Data#customers (self-reference!)
        mask_dsg.sinks["Masked_NW_Data:customers"] = DatasetSink("Masked_NW_Data", "customers")
        
        print("Added self-reference to MaskCustomersWorkSpace:")
        print("  Original sinks: NW_Data:customers") 
        print("  Added self-ref: Masked_NW_Data:customers")
        
        # Re-validate ecosystem with self-reference
        tree = eco.lintAndHydrateCaches()
        if tree.hasErrors():
            tree.printTree()
            self.fail("Ecosystem validation failed with DataTransformer self-reference")
        
        # Generate pipeline graph
        legacyA: DataPlatform = eco.getDataPlatformOrThrow("LegacyA")
        graph: EcosystemPipelineGraph = EcosystemPipelineGraph(eco)
        pi: PlatformPipelineGraph = graph.roots[legacyA.name]
        
        # Verify the DataTransformer workspace is included
        self.assertIn("MaskCustomersWorkSpace", pi.workspaces.keys(),
                      "DataTransformer workspace with self-reference should be included")
        
        # Verify the self-reference exists in the modified workspace
        updated_sinks = list(mask_dsg.sinks.keys())
        self_ref_sinks = [sink for sink in updated_sinks if "Masked_NW_Data" in sink]
        self.assertTrue(len(self_ref_sinks) > 0,
                        f"Expected self-reference sink, but found sinks: {updated_sinks}")
        
        # Verify no infinite loops occurred during graph generation
        self.assertGreater(len(pi.nodes), 0, "Pipeline graph should have nodes")
        
        print("✅ DataTransformer self-reference test passed!")
        print(f"   Workspace sinks: {updated_sinks}")
        print(f"   Self-references: {self_ref_sinks}")
        print(f"   Pipeline nodes: {len(pi.nodes)}")
        print("   Recursive algorithm correctly handles self-reference within workspace")
        
        # ENHANCED: Examine the actual pipeline graph structure
        print("\n🔍 Pipeline Graph Structure Analysis:")
        print("=" * 60)
        
        # Categorize nodes by type
        ingestion_nodes = [n for n in pi.nodes.values() if isinstance(n, IngestionNode)]
        export_nodes = [n for n in pi.nodes.values() if isinstance(n, ExportNode)]
        trigger_nodes = [n for n in pi.nodes.values() if isinstance(n, TriggerNode)]
        transformer_nodes = [n for n in pi.nodes.values() if isinstance(n, DataTransformerNode)]
        
        print("Node Type Breakdown:")
        print(f"  • Ingestion Nodes: {len(ingestion_nodes)}")
        print(f"  • Export Nodes: {len(export_nodes)}")
        print(f"  • Trigger Nodes: {len(trigger_nodes)}")
        print(f"  • DataTransformer Nodes: {len(transformer_nodes)}")
        
        # Find self-reference specific nodes
        self_ref_exports = [n for n in export_nodes if "Masked_NW_Data" in str(n)]
        mask_triggers = [n for n in trigger_nodes if n.workspace.name == "MaskCustomersWorkSpace"]
        
        print("\nSelf-Reference Pattern Analysis:")
        print(f"  • Self-reference exports: {len(self_ref_exports)}")
        for exp in self_ref_exports:
            print(f"    - {exp}")
        
        print(f"  • MaskCustomers triggers: {len(mask_triggers)}")
        for trig in mask_triggers:
            print(f"    - {trig}")
        
        # Analyze data flow for self-reference
        if mask_triggers:
            trigger = mask_triggers[0]
            print(f"\nData Flow Analysis for {trigger}:")
            print(f"  Inputs (left-hand): {len(trigger.leftHandNodes)}")
            for left_key, left_node in trigger.leftHandNodes.items():
                print(f"    ← {left_node}")
                # Check if this is the self-reference input
                if "Masked_NW_Data" in str(left_node):
                    print("      *** SELF-REFERENCE INPUT DETECTED ***")
            
            print(f"  Outputs (right-hand): {len(trigger.rightHandNodes)}")
            for right_key, right_node in trigger.rightHandNodes.items():
                print(f"    → {right_node}")
        
        # Verify expected data flow pattern
        print("\nExpected vs Actual Data Flow:")
        print("Expected:")
        print("  1. External: NW_Data#customers → Export → Trigger")
        print("  2. Self-Ref: Masked_NW_Data#customers → Export → Trigger")
        print("  3. Trigger → DataTransformer → Ingestion → Export")
        print("  4. Export feeds back to step 2 (cycle)")
        
        # Check if we have the expected components
        has_external_export = any("NW_Data" in str(n) and "customers" in str(n) for n in export_nodes)
        has_self_ref_export = any("Masked_NW_Data" in str(n) and "customers" in str(n) for n in export_nodes)
        has_mask_transformer = any(n.workspace.name == "MaskCustomersWorkSpace" for n in transformer_nodes)
        
        print("\nActual Flow Verification:")
        print(f"  ✅ External export (NW_Data#customers): {has_external_export}")
        print(f"  ✅ Self-ref export (Masked_NW_Data#customers): {has_self_ref_export}")
        print(f"  ✅ MaskCustomers transformer: {has_mask_transformer}")
        
        if has_external_export and has_self_ref_export and has_mask_transformer:
            print("  🎯 SUCCESS: All expected components present!")
        else:
            print("  ❌ WARNING: Missing expected components")
        
        # Final assessment
        print("\n📊 Graph Correctness Assessment:")
        print("  • Graph generation: ✅ No infinite recursion")
        print(f"  • Self-reference detection: ✅ Found {len(self_ref_sinks)} self-references")
        print("  • Expected nodes present: ✅ All key components found")
        print(f"  • Data flow structure: {'✅ Appears correct' if has_external_export and has_self_ref_export else '❌ May have issues'}")
