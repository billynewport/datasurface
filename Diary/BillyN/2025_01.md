# January 2025

## 2025/01/01

Happy new year! Today, I'm examining how to get the following pipeline implementation working:

* Kakfa Connect
* Debezium

I want to be able to configure debezium to capture all the tables in a database and stream them to Kafka and then create a kafka listener to take the kafka messages and write them to an iceberg based table. I want to be able to generate a terraform file to create the various kafka topics and connectors easily. The plan is to use the Mongey/kafka terraform provider to manage the kafka topics and connectors. The ZeroDataPlatform will need to use jinja to generate this terraform file from the ingestion graph. This terraform file will reflect the current reality of whats defined in the ecosystem model. Terraform will then take that, look at whats currently configured in kafka and kafka connect and modify that to match the desired state contained in the terraform file.

I need to create a manual terraform file to setup a demo pipeline. Once thats working then I can generate it from the transistion graph. At that point, we can generate the pipelines for potentially thousands of pipelines. I have a working local Confluence Kafka Connect service working on my local Dell docker machine. I will get the debezium tutorial working using Terraform next. I'm currently in Colorado Springs and my Dell is in Ocala Florida. My home network and laptop use tailscale which makes it look like everything is local to me.

### Thoughts on how to deploy a dataplatform

The github or gitlab action handlers maintain the integrity of the ecosystem model stored in the primary branch. Periodically or in response to commit events, the updated model should be pushed through the various dataplatforms to make the data flow pipelines a reality and reflect whats stored in the model. The Zero Platform just needs to check out the primary branch of the repository and then generate the various terraform artifacts and then apply them. All validations should have been done in the action handlers. If changes make it to the primary branch then the model is assumed valid. It passed all the DataPlatform validations during the action processing.

Therefore, a cron job or Airflow periodic task can be executed periodically or triggered when a commit happens. The advantage of terraform is that this job can just be run periodically. If there are no changes since last time then the generated terraform should be identical and as a result applying terraform would be a noop.

### How fast should changes be applied?

My own experience is that production pushes are typically manual where as developers doing normal development loop stuff want changes immediately.

## 2025/01/03

Researching Kafka/Debezium iceberg sink. This would give me CDC to iceberg based staging data on which I can then run the MERGE jobs to the corpus tables. Ingestion pattern is to setup a CDC connector and then a sink connector to the staging iceberg tables. So, each ingestion node in the intention graph will add this pattern to a terraform file named after the dataplatform name.

## 2025/01/08

Installing kafka connect on my laptop to do initial tests of a prototype pipeline. I'm using the following page:

* <https://docs.confluent.io/platform/current/get-started/platform-quickstart.html>

``` bash
wget https://raw.githubusercontent.com/confluentinc/cp-all-in-one/7.8.0-post/cp-all-in-one-kraft/docker-compose.yml
```

wget wasn't installed on my mac so I did a brew install wget which is installing/updating a LOT of stuff. This always makes me nervous. The wget now works and docker compose up is running.

I don't want to screw up my dell docker containers for now, so I'm do this prototyping on my m2max. I'll work through the tutorial describe on confluent above to verify my installation is working first. And everything worked. So confluent kafka connect works fine using docker on a mac. Note to self, the tutorial docker system prune command deletes all the images also so it's downloading them again now when I do a fresh compose up.

What I want to do is to create a working prototype data pipeline from postgres to minio using debezium. I just want to see the staging files being created. I want to provision this pipeline using terraform with a manually create terraform file. This file will become the templates for the ZeroDataPlatform version. Ultimately, I want to be able to provision the same pipeline using DataSurface with the ZeroDataPlatform.