# May 2025

## 2025/05/07

I changed my mind and decided to start with kubernetes. I'm testing it on my M2 Max with Docker Desktop. I will also test on the kubernetes on my home server, the Dell once it's working.

Started on getting a manual kubernetes pipeline working, then I will template it.

Added a kubernetes yml to create a secret with a postgres database instance. Tested it and it works.

```bash
kubectl apply -f postgres.yaml
```

Verify secret was created:

```bash
kubectl get secret postgres-secret -o jsonpath='{.data.POSTGRES_USER}' | base64 --decode
echo # for a newline
kubectl get secret postgres-secret -o jsonpath='{.data.POSTGRES_PASSWORD}' | base64 --decode
echo # for a newline
```

Start port forwarding:

```bash
kubectl port-forward svc/postgres-service 5432:5432
```

Connect to the database:

```bash
psql -h localhost -U your-username -d postgres
```

Database is running and accessible. Next steps:

* is to create another postgres instance for a source database.
* Then create a Kafka connect cluster and configure debezium to CDC the source database.
* Then create another kafka connect cluster
* and use a topic sync to copy the topic data from the first kafka cluster to this one
* then configure a postgres sink to copy the messages to a staging table on the staging postgres instance.
* Then, we run logic to MERGE the staging table to the LIVE table.
* Some kind of monitoring, rows inserted, rows updated, rows deleted. Time taken to merge. End to end latency (original kafka message time to merge time). I don't like prometheus, I prefer logging raw data in a database and then querying that.

At this point, we have a working pipeline. I will use airflow to schedule the merge job. The other steps run by themselves in kafka connect.

Now, we need to create a DataPlatform for this. It will be configured with a kafka connect cluster and a single postgres "merge" database. This is generated by the DataPlatform. The DataPlatform will create a terraform file which will:

* Create topic replicators for every source database ingestion using kafka topics.
* Create postgres sinks for each of those topics.
* Create staging and merge tables for each dataset being ingested.
* Create a view for every Datasetsink in a Workspace.
* Create an Airflow DAG for each merge job.

This is a basic working system with no security provisioning yet. We need to add more ingestions after this. SQL based ingestions, parameterized pulls, scrap tables.

## 2025/05/08

Todays task. Create a kubernetes yaml file with a staging/merge postgres instance, a kafka connect cluster, an airflow instance. Add a docker container build script to create an image which can run the DataSurface code. The server.py code is the API to interact with the code within this container.

## 2025/05/18

I am going to do or try to do a kubernetes version first then. I will likely change the SimpleDataPlatform to be KubernetesPGDataPlatform or something like that. I've been looking at secret management. In the Ecosystem model, ingestions/transformers allow a credential to be specified. This lets the DataPlatform know how to authenticate to the source database or what credential the transformer should run as. DataPlatforms also need credentials to authorize to storage systems or DataContainers that they need to use for staging/merge or for exporting data to consumer DataContainers. These credentials must be available in the CredentialStore used by the DataPlatform. In this case, the DataPlatform will use kubernetes so we will be using kubernetes secrets for this stuff. This means, every credential specified on the DataPlatform as well as the intention graph must be available in the CredentialStore aka as a kubernetes secret. The merge hook will need to verify that the credentials are actually available and it not then it needs to fail with a set of errors indicating which ones have what issue. I think the merge time handler is the right place to do this. The git action for commit could be running on github so it's not really the place to check credentials on a possibly remote kubernetes cluster for example. This DataPlatform will use airflow so running the merge job as a airflow job makes sense and we can leverage airflow to show the status/errors of the merge handler job.

So, I need to write code available to the merge handler to verify all credentials needed are valid or at least defined. As a second step, the handler could also try to connect to the various data containers to verify the credential will be accepted also. The mergehandler is going to run as a Kerberos pod operator. So, it will be a docker container image and this also means every secret needs to be available in that pod. When we generate the Airflow DAG then we'll need to enumerate every credential/secret and add it to the DAG job. People may look at this as a security issue. Every secret used by the DataPlatform needs to be available to do this check. The code in that container is controlled. That code is also the code which will connect to the various data containers regardless to pull data and push data to them. So, I don't see a big security issue. Any user code would just get the credentials that it needs, for example, a DataTransformer would run at the specified credential or that credential would be available to it. I am looking at just supporting environment variables for Credentials for now. This means every secret will need an appropriately named environment variable which the platform knows about.

I could do this by providing a consistent naming mechanism which the DataPlatform or DataTransformer uses to get the environment variable names for a specific Credential. There would usually be 2, one for the username and one for the password. A function which returns a dict of environment variables for a specific credential would be needed. The dict keys would be username and password, the values would be the environment variable names. Or, the function could just look up the values and return the actual secret values in the dict values. This might be better as it hides how the secrets are retrieved from the user code. Kubernetes typically uses env variables but docker swarm uses files for example.

So, the CredentialStore will need that function, resolveCredential or similar which takes a credential name and returns a dict of secrets with standard key names like 'username' and 'password'.

Yup, I use "So" too much...

### Bootstrap handling

This also brings up where does the initial merge job configuration come from? In this case, the SimpleDataPlatform uses kubernetes and airflow. So, some kind of bootstrap needs to be executed initially. An initial kuberenetes yaml file and terraform file to bootstrap the initial merge job configuration. Each DataPlatform would have a different one potentially. There should be a docker command to execute the image against current model and it then generates the initial kubernetes installation template. This would likely need to be manually customized and then applied to the kubernetes cluster. In this case, that initial cluster would include an airflow installation with a bootstrap DAG which has the merge job and it should start running once installed. An Ecosystem would have multiple DataPlatform instances configured. There might be a dev kubernetes cluster and a prod kubernetes cluster for example. This is valid and the model could have both prod and dev datastores specified. Policies can be used to make sure dev datastores are not used in prod or that only cleaned prod datastores are used in dev. A user could also have multiple Ecosystem models for dev and prod if they chose. The flexibility is there.

So, for each configured DataPlatform instance in an Ecosystem, there should be a bootstrap command to generate files in the current directory that can be used to bootstrap each DataPlatform instance.

For SimpleDataPlatform, we would generate a kubernetes yaml file. The user would then apply this against the target kubernetes cluster after customization (storage volumes, credentials needed and so on). Potentially, we can generate one of these for each configured SimpleDataPlatform instance in the ecosystem model. The DataPlatform would have a namespace string to use for the kubernetes resources.

This needs work also. Right now, SimpleDataPlatform lets the user specify a postgres database and a kafka connect cluster. These names should be used to generate the kubernetes bootstrap resources. This would then leave the administrator to create correctly named secrets, assign storage volumes and so on. The code can then calculate hostnames dynamically based on the configured names in the model. I'll need to allow the user to specify that the data container is already created and not managed by the DataPlatform also.

The kafka connect container image would need to be a custom one based on a standard image. It would include all the connectors needed/supported by the DataPlatform.

The bootstrap command should really just return a dict of keys to file contents. The caller would then write the files to the current directory. The files could be anything. The administrator would use context and documentation to understand what each file is for. If the bootstrap command runs in a docker container then the folder for the files would need to be mounted as a volume or the administrator couldn't access them. I need to add a generateBootstrapArtifacts method to DataPlatforms. It should just return a dict of keys and values. The caller would be an Ecosystem level method. This method would interate over all the DataPlatforms, call their generate methods and then write the files to the current directory within a folder named after the DataPlatform name. If the folder already exists then it should be deleted and recreated.

A good convention would be that one of the DataPlatform artifacts was a README.md file which would explain the files in the folder and how to use them. It could include in SimpleDataPlatforms case, all the expected secrets. Secrets could be shared between DataPlatforms running on the same kubernetes cluster likely in different namespaces.

In general, the renderGraph method on DataPlatforms and the bootstrap methods will need to create files in a folder accessible from outside the container hosting the code. I should use the same pattern for both and have a Ecosystem method for calling the methods when needed.

## 2025/05/20

Did some refactoring today on governance.py. Splitting it up and doing some minor changes to avoid circular imports. Connected google jules to the repository today for a play, I'll see what i can do with it.

The SimpleDataPlatform will produce 2 kubernetes yaml files. One is the base infrastructure, it's static and generated as the bootstrap artifact yaml. The other is generated from the ingestion graph and primarily configures the kafka connect cluster connectors and airflow DAGs. There will be 2 Airflow DAGs, one for running the infrastructure rendering (the infra DAG), one for generating a new graph kubernetes yaml and terraform files (the pipeline DAG). The merge handler job in this infra DAG will be triggered by a timer or when git commits are detected on the model primary branch. The the pipeline DAG, is generated from the ingestion graph and is the data pipeline DAG.

## 2025/05/22

Changing the name of the data platform from SimpleDataPlatform to KubernetesPGStarterDataPlatform.

Just looking at this now, I had the DataPlatform accepting parameters for the kafak connect cluster and postgres database. I'm now thinking this is wrong. The DataPlatform may will take hostnames and the namespace to use but it provisions the actual database and cluster itself. Maybe, if we wanted it to use an existing database and cluster then this is how it should work but for now, lets provision the stuff in the DataPlatform.

## 2025/05/28

Thinking about how slow it is to construct large Ecosystem DSL models. I have made the capture of defining source filename and linenumber optional. It can be turned off and on. It would be on for actiona handlers but off for when we just want to load the model. I also converted DDLColumn/Dataset to use named parameters instead of *args. This should be much faster also and it works with code completion better in code editors.

I could convert everything to use this but I'm just doing it for the high cardinality classes for now, those are the biggest performance bottlenecks.

I'm using claude 4 in agent mode with cursor for these changes. It's hit and miss. Lots of backing out changes and then trying again. Frequent commits to git because it cannot recover lost methods during edits.
