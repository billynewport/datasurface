# November 2024

## 11/04/2024

Switched to using Claude 3.5 Sonnet with github copilot/vscode, it seems better.

Have airflow running on the Dell. It was in a restart doom loop. Based on the docs [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html), I executed

```bash
docker compose up airflow-init
docker compuse up
```

and now it's working with the default user, airflow/airflow. I need to create an ingestion pipeline manually using AirFlow/Spark/Debezium/Postgres in and Minio. Then I work on automating it using DataSurface. This basically means building a DataPlatform which takes an intention DAG from DataSurface and then generates the airflow DAG for the pipeline and getting the existing spark ingestion code to run merging debezium changes in to an s3 deltalake hosted in Minio locally on the Dell.

I will package this first DataPlatform as part of the core DataSurface python module for now. I will need to work through seperating it later, possibly packaging it as a Docker container with a standard interface.

I need to pick a stack for this initial pipeline:

* Gitlab
* Airflow
* Spark V4
* Debezium
* Postgres input
* Minio for S3/iceberg storage
* Storage based iceberg Data Catalog
* Trino as consumer sql endpoint

So, I want a pipeline to generate data continously in postgres. Do CDC with debezium and push the deltas to s3/minio. Run a spark job to ingest those deltas in batches to delta lake tables stored in s3/minio. Create tables in Trino to query the data from s3/minio. This seems a decent first step and gives people a basic data lake/data warehouse for starters.

## 11/05/2024

Plan is to spend today making sure all the various elements in the stack are working and then manually build the pipeline and verify that data flows through it. I will then work on automating the pipeline using DataSurface.

On my Dell, the services right now are:

* 192.168.4.157:9000 - Portainer
* 192.168.4.157:9200 - Minio
* 192.168.4.157:11080 - Spark master (1 spark-worker)
* 192.168.4.157:80 - Gitlab (2 runners)
* 192.168.4.157:8080 - Airflow
* 192.168.4.157:12080 - Trino

## 11/06/2024

Discussing how to use Airflow with myself/copilot. I'm imagining a source database and am using CDC/Debezium to push delta events in to an s3 folder. There are then Spark jobs to read these deltas and apply the updates to an ice berg table also in s3 which can be queried by trino later. This is the basic pipeline. I have built systems where there was a spark job per table ingested for example but at scale this runs in to a few problems principally around resources used to ingest N tables and job latency when schedulers become overloaded by many tasks (millions). Alternatives would be to have a single spark job handle all tables within a single database ingestion for example. This collapses tasks from N to one but may add latency depending on how the job tackles the tables, is it serial, is it in parallel. When it's in parallel then that complicates managing resources globally as different jobs use different amounts of threads for example. So, it's harder to say I can execute N jobs at once from a memory point of view but may run out of threads. Really, a fine grained scheduling solution is needed and Spark likely isn't suitable for this at all, it's designed for batch jobs.

There is also the matter than Spark has overhead. If we're running on top of a SQL store (Snowflake) then we possibly only need a single JVM/python job to use odbc to issue the SQL calls against the database so we don't need a parallel computation framework to run the job. All the partitioning and executing the SQL in parallel is handled by Snowflake so Spark is redundant.

The discussion is really some kind of cluster of stateless event workers servicing a queue of lightweight tasks. This would be YARN and spark jobs, it could be AWS Lambda or something home grown. My own experience is that while 90% of tasks can likely run just fine on a single thread event, the remaining 10% require exponentially more resources. You might need a small job running on up to 4 servers for 92% of jobs to be covered. The last 1% though may need 100 server job.

The problem then is can you have a single event handler which can handle the single thread OR the multi-thread or fully clustered event task. Thats tricky. Standardizing on a single implementation which works for both like a Spark or Flink job is attractive when working on something like Iceberg or Delta Lake. If working with an active storage or thick storage layer like snowflake then that implementation can push down code in the following of SQL and allow snowflake to manage scaling that and resources. Thats the principle problem with iceberg and delta lake for me, they are currently dumb storage. All the compute runs in the client program/job. This is changing but it's not there yet. Delta Engine for example still runs within the client spark job not remotely like with snowflake. There are so many issues associated with running the database engine within the client but thats another story. This is one of my major gripes with Iceberg/delta lake/delta engine versus snowflake. Keeping the storage layer/query code external to your application is a major advantage for Snowflake vs Spark for now.

Delta Lake 4.0 is trying to be a remote execution layer for data access/manipulation but is brand new. Building a mission critical app for this year then maybe not the best idea... My biggest issue with spark/data bricks is they have no regardless for keeping storage layers, binary runtimes and APIs stable over a 5-10 year window. If I build an enterprise level system then I can't afford to rewrite it completely every 3-4 years when Spark N+1 ships which isn't compatible with the old one and writes from the new jobs cannot be read by the older jobs. Jobs need rewrites to switch APIs if moving up to latest version. Once you have 1000s of such jobs, rewriting simply isn't an option and now you are wishing you used SQL or a managed service like Snowflake.


