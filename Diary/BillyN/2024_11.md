# November 2024

## 11/04/2024

Switched to using Claude 3.5 Sonnet with github copilot/vscode, it seems better.

Have airflow running on the Dell. It was in a restart doom loop. Based on the docs [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html), I executed

```bash
docker compose up airflow-init
docker compuse up
```

and now it's working with the default user, airflow/airflow. I need to create an ingestion pipeline manually using AirFlow/Spark/Debezium/Postgres in and Minio. Then I work on automating it using DataSurface. This basically means building a DataPlatform which takes an intention DAG from DataSurface and then generates the airflow DAG for the pipeline and getting the existing spark ingestion code to run merging debezium changes in to an s3 deltalake hosted in Minio locally on the Dell.

I will package this first DataPlatform as part of the core DataSurface python module for now. I will need to work through seperating it later, possibly packaging it as a Docker container with a standard interface.

I need to pick a stack for this initial pipeline:

* Gitlab
* Airflow
* Spark V4
* Debezium
* Postgres input
* Minio for S3/iceberg storage
* Storage based iceberg Data Catalog
* Trino as consumer sql endpoint

So, I want a pipeline to generate data continously in postgres. Do CDC with debezium and push the deltas to s3/minio. Run a spark job to ingest those deltas in batches to delta lake tables stored in s3/minio. Create tables in Trino to query the data from s3/minio. This seems a decent first step and gives people a basic data lake/data warehouse for starters.

## 11/05/2024

Plan is to spend today making sure all the various elements in the stack are working and then manually build the pipeline and verify that data flows through it. I will then work on automating the pipeline using DataSurface.

On my Dell, the services right now are:

* 192.168.4.157:9000 - Portainer
* 192.168.4.157:9200 - Minio
* 192.168.4.157:11080 - Spark master (1 spark-worker)
* 192.168.4.157:80 - Gitlab (2 runners)
* 192.168.4.157:8080 - Airflow
* 192.168.4.157:12080 - Trino

## 11/06/2024

Discussing how to use Airflow with myself/copilot. I'm imagining a source database and am using CDC/Debezium to push delta events in to an s3 folder. There are then Spark jobs to read these deltas and apply the updates to an ice berg table also in s3 which can be queried by trino later. This is the basic pipeline. I have built systems where there was a spark job per table ingested for example but at scale this runs in to a few problems principally around resources used to ingest N tables and job latency when schedulers become overloaded by many tasks (millions). Alternatives would be to have a single spark job handle all tables within a single database ingestion for example. This collapses tasks from N to one but may add latency depending on how the job tackles the tables, is it serial, is it in parallel. When it's in parallel then that complicates managing resources globally as different jobs use different amounts of threads for example. So, it's harder to say I can execute N jobs at once from a memory point of view but may run out of threads. Really, a fine grained scheduling solution is needed and Spark likely isn't suitable for this at all, it's designed for batch jobs.

There is also the matter than Spark has overhead. If we're running on top of a SQL store (Snowflake) then we possibly only need a single JVM/python job to use odbc to issue the SQL calls against the database so we don't need a parallel computation framework to run the job. All the partitioning and executing the SQL in parallel is handled by Snowflake so Spark is redundant.

The discussion is really some kind of cluster of stateless event workers servicing a queue of lightweight tasks. This would be YARN and spark jobs, it could be AWS Lambda or something home grown. My own experience is that while 90% of tasks can likely run just fine on a single thread event, the remaining 10% require exponentially more resources. You might need a small job running on up to 4 servers for 92% of jobs to be covered. The last 1% though may need 100 server job.

The problem then is can you have a single event handler which can handle the single thread OR the multi-thread or fully clustered event task. Thats tricky. Standardizing on a single implementation which works for both like a Spark or Flink job is attractive when working on something like Iceberg or Delta Lake. If working with an active storage or thick storage layer like snowflake then that implementation can push down code in the following of SQL and allow snowflake to manage scaling that and resources. Thats the principle problem with iceberg and delta lake for me, they are currently dumb storage. All the compute runs in the client program/job. This is changing but it's not there yet. Delta Engine for example still runs within the client spark job not remotely like with snowflake. There are so many issues associated with running the database engine within the client but thats another story. This is one of my major gripes with Iceberg/delta lake/delta engine versus snowflake. Keeping the storage layer/query code external to your application is a major advantage for Snowflake vs Spark for now.

Delta Lake 4.0 is trying to be a remote execution layer for data access/manipulation but is brand new. Building a mission critical app for this year then maybe not the best idea... My biggest issue with spark/data bricks is they have no regardless for keeping storage layers, binary runtimes and APIs stable over a 5-10 year window. If I build an enterprise level system then I can't afford to rewrite it completely every 3-4 years when Spark N+1 ships which isn't compatible with the old one and writes from the new jobs cannot be read by the older jobs. Jobs need rewrites to switch APIs if moving up to latest version. Once you have 1000s of such jobs, rewriting simply isn't an option and now you are wishing you used SQL or a managed service like Snowflake.

Delta Lake 4.0 is basically Snowflake from Data bricks. It should allow storage format updates without needing to recompile the client jobs using it. So, you can benefit from performance/reliability improvements without needing to rewrite your jobs to use the latest spark/delta lake jars. The question is will they get it right for 4.0. As an example, look at Oracle database. The client JDBC/ODBC drivers work multiple versions backwards. You can run a 20 year old client against a new database. This is the kind of compatibility I need from a storage layer. Will Delta Lake promise this kind of backwards compatibility? It's easy for tech executives to be wined and dined and look at marketecture slides/PDFs and not realize what they are actually getting mostly because they will move to the next job before the problems become apparent. Technical debt is fine so long as you aren't around to deal with it in the future, after all...

## 11/07/2024

I'm just thinking about the easiest way to get this working. I have decided I likely do not need to use Spark. If I use SQL and especially the new commands such as MERGE then
I can push almost all the processing down to the database and not need parallism at the job layer. The code then becomes relatively portable. I can test it against Delta Lake 4 (remote/local),
Snowflake, regular databases such as Postgres (which simplifies day to day testing enormously). I'd written and tested a simple LIVE table MERGE Spark job which uses SQL but I'm staring at it
now from this point of view. Does it need to be Spark? I did it in Java but maybe, it's better in Python especially if there is little computation happening in it as most computation is pushed
down as SQL. I'd thought about this before I retired 3 or 4 years ago after trying to do these kinds of jobs in Spark first, then MR and finally Flink. Spark was too hard to tune and a version
nightmare. MR was much faster, easier to tune and used significantly less resources than Spark. Flink was easier to tune and offered Kappa type promises. I considered Apache Beam also to get more
portability but it seemed too risky in terms of layer on a layer on a layer and it's relatively new and google was behind it. Google being behind it is a problem because they have a history of
abandoning stuff when it's useful for them. Enterprise software needs to last at least a decade and be backwards compatible the whole time, no one can afford to add non backwards compatible
technologies in an environment with shared data used by old and new code. You are doomed if you do this.

But, all these technologies work on top of thin storage, hdfs or s3. Snowflake or Hudi offer thick storage. Storage with an SQL API. This thick storage is more attractive to me. Apache Iceberg and the current
delta lakes are still thin storage. Snowflake and Google BigTable are thick storage and offer significant advantages in this space in terms of longevity, automatic compute scale out for queries and thin client
support. People do not realize how important thin client support is to storage. If you are shipping the storage engine with your application then thats a problem. Imagine if an application which used Oracle
shipped a copy of Oracle RDMS with the application and the application was hard linked with it. Imagine every application you use did this and there would be N versions of Oracle floating around, how do you
upgrade Oracle? By recompiling/linking every application? You would agree thats insane but thats exactly what happens when you use Apache Iceberg or Delta lake today.

## 11/08/2024

Researching delta lake 4 more, the problem with it is that they state new versions will be able to read and more importantly write old delta table formats. This is a step forward but doesn't really solve the problem. This capability basically allows new data clients to use newer spark/delta versions but the storage layer is trapped at the lowest
version until older code can be decommisioned or rewritten to use new APIs. This means new features/some optimizatons and so on cannot be taken advantage of because the storage layer version is not advancing.

Again, this is why we need a client/server approach. The details of reading and writing the storage layer should not be visible to the clients so it can advance independently of the client version.
Being able to write old formats from the client is not this. Delta connect from Data bricks is in preview and promises to allow this but we will see how long it takes for Data bricks to allow ANY job using
any API to work with Delta connect.

## 11/10/2024

For system processing in pipelines, I think this singlethread JDBC style client pushing down SQL to a thick storage layer is likely good enough. The problems with this approach tend to be when you need application
logic to manipulate data or produce derivative data. You can make some kind of UDF which can itself be pushed down to the storage layer but functionality will be limited to stateless operations and no connectivity. This is kind
of where Spark/Flink come in to play but I think the way they work today is backwards. Today, the Spark job runs splits Dataframes in to partitions and runs workers against records in those partitions producing thederivative data
which is then combined to the output DataFrame. This is harder than it looks. How many works/partitions should the job use? Well, it depends on how many records there are and how much time you have to do the work.
This can change on a run by run basis. You would need to know how large the incoming DataFrame is to really figure how how many workers to use on a query by query basis. If your workload is very uniform then this is easy
but what when it's not? What about when it uniform mostly but periodically things goes crazy and your job blows up with OOM errors or is too slow? Do you want people retuning jobs when this happens in the critical path? No,
I'd prefer we didn't do that.

Whats really needed here is for roles to shift. Snowflake, for example, receives the query algebra which includes the java code grabbing the DataFrame. Snowflake needs to host the computation which it sizes based
on the incoming data and decides how many workers and uses a crazy efficient memory mapper to zero serialize the input data and run the java code against it. This is the only way this can work. I'm amazed that they
haven't done it yet. Now, this are easier for the customer. Here is the code DAG I want you to process and push it down. A single thread constructs that DAG and pushes it down. Machine learning and LLM style compute
DAGs work this way already and data processing code is no different. Keeping the management of the compute resources independently of the data engine makes no sense. The data engine needs to manage the compute resources
to execute the transformation DAG or what ever we want to call this.

## 11/11/2024

Downloaded the new Qwen-2.5-Instruct-Coder-32B-Q8 model today with LM Studio. The context window is limited to only 32K but thats still big enough to push the markdown files for data surface in a chat session and then ask questions. It
runs at about 3.5 tok/sec on my M2 Max and then answers were all good, it didn't get confused so I think this is the best local LLM model that I have tested. Even if I owned an M4 Max, the performance would still be around 6 tok/sec which is still barely reading speed so I don't know if I would change my mind and upgrade yet. Qwen uses about 40GB with the Q8 model and a 32K token context window when running.

I also used it with the diary entries in this repository as a less demanding test. I copied and pasted October and November which totals about 6.1k tokens. This took 40 seconds to process and answering prompts runs at 7.1 tok/sec for the first prompt. Subsequent prompts run at 6.9 tok/sec and take about 9.68 seconds to start the response.

Again, looking at benchmarks, the M2 Max is rated at 13.7 tera flops and the M4 Max seems to be around 18. It's faster but not by enough to make a difference at least for me right now. I suspect the M5 Max will be around 21/22 and the M6 around 26 when it arrives in late 2026.

## 11/12/2024

Trying Qwen-2.5-14B-Coder-instruct-Q8 today and it's pretty good also. Using it with the a cat of all the markdown files for the project and it's not bad at all. When configured with a 128K context, it still takes 38GB of RAM. This is about the same as the 32B model with a 32K window. Speed wise I'm seeing about 6 tok/sec which is double the 32B. I imagine the M4 Max running the 32B model at about the same speed as the 14B model on my M2 Max.

Is 6 tok/sec fast enough though? I think it is but if I was creating blocks of code then now it becomes slow. The output context size is 8k tokens which can be a problem. For example, I took the Spark Java code for merging deltas to a snapshot and asked it to make a version which used a SQL database (postgres) instead of S3/Iceberg. It did it, I then asked it for a version that worked with Snowflake. I then asked it to refactor to a base class with subclasses for postgres and snowflake and it fails there because the output is bigger than 8K tokens. I also hit this limit when using Git Copilot with GPT 4o. I can ask for them one by one but you see the problem. Also, 8K tokens at 6 tok/sec is about 360 tok/minute or 3 minutes / 1K tokens. Not great but not terrible.

## 11/14/2024

I was brainstorming a little with a friend and we came up with the idea of using SQL Lite on top of s3 instead of parquet or Iceberg. This would provide a super lightweight store for all the records for a dataset and allow easy updates/queries against it. For most small datasets, this doesn't actually seem that bad if you are using a singlethread client to do the ingestion. The vast majority of datasets can easily be handled by such a store and likely provide better performance than Iceberg or delta lake. Spark like or normal clients can push down SQL to the store for fast joins. I can also imagine using a single SQLLite databases for all datasets from the same data source and ingest them together. This would support joins/filters on data from a single source easily.

I'm thinking I could ingest data from sources directly in to a table (temp table if you want) and then use SQL MERGE commands to take those delta records and apply them to the primary data table in the SQL Lite database. This would likely be more efficient that using iceberg tables for ingestion and primary tables. It would potentially handle large datasets but certainly datasets up to maybe even 10s of GB of data. The problem with SQLLite
on an S3 like object store is s3 objects are immutable. We'd basically need to read the database, modify it and then write it to a new
s3 object. Thus, we'd see an increasing sequence of SQLite databases, one per file being stored in an s3 folder for a dataset.

This is manageable. Lets discuss alternatives so we can compare and constrast.

Option 1: Lets write delta records from ingestion to a parquet file in s3 with a batch id as the object name. We can use and
iceberg table as the current live table. Iceberg has the same limitations as described above for s3. Iceberg writes the changes
to a new parquet file and also creates a new manifest file which lists all the files holding records for the table.

So, we're not copying all the data, just writing records for delete markers and inserts/updates. This allows fast writes. periodically
how ever, read performance drops because we need to jump around all the record files to iterate over the current records. This
is usually called MERGE on READ. Fast writes but writes slow down as the number of mutations increase. Another approach is
called MERGE on WRITE. Here, we copy all the data to a new file with the new changes. This is slower for storing the
writes but much faster for READS as you just need to scan the file and every record is good.

MERGE on READ tries to manage this performance problem by running periodic conflation jobs which read through all the live records
and then write them to a single file and thus client reads become fast again. There is a trade off between how often you
do conflation and how fast READs are. Clearly, MERGE on WRITE is the maximal case because we're conflating on every write operation
which is optimal for READs but slow for writes. If you have a lot of READERs, MERGE on WRITE may be more efficient overall
than MERGE on READ.

So, we're writing deltas in to parquet files per batch and we ingest them in to an iceberg table using either MERGE on READ
or MERGE on WRITE. We will also likely have a small sequenced set of files tracking the current state of the ingestion
process. Every batch writes a new file with the current state.

Option 2: We abandon parquet/iceberg. We instead choose a full copy SQLite database for the dataset. This has a table
for the deltas seen and a table for the live records in the dataset. If we're ingesting multiple tables at once transactionally
from a source then we could keep all these tables for a single source within a single SQLite database. Each batch of
deltas will result in us opening the latest SQLite database in s3, writing the new delta records and then executing SQL to
update the live table with the changes. This database then needs to be written back to a new s3 object with a new sequenced file
name. This is MERGE on WRITE and it's harder to do MERGE on READ. We can do it but it's more complex than with iceberg.

Conclusion:
Which is best? Given SQLite is all C++, it's possible than the SQLite database version will be faster than the iceberg implementation.
SQLite allows multiple concurrent readers but one writer. This is fine for our use case, the only writer is ingestion and
that is serialized. If we have very large datasets and we trade off to MERGE on READ then iceberg may outperform as if
we need to parallel process large datasets on reads then spark/iceberg may be faster but again, Spark has overheads and
if the SQL can be pushed down to SQLite and it has indexes for fast filters, I don't know if it will outperform. A big issue
with sqlite is that each reader will need to copy the database to a local fiel to read it. This is expensive. It's
a local file system also unless you have somekind of shared filesystem where you could copy it once and the all readers can
share it. This is a big advantage of iceberg, it's a shared storage layer. SQLite is not.

Basically, for SQLite to work, we would abandon s3/minio and use a shared filesystem of somekind instead. I don't think it's
practical otherwise.

The huge advantage for an iceberg based approach is that datasets can be directly mapped to external tables in the various
catalogs and then queried directly by trino/spark/databricks even snowflake. This is a zero copy approach to leveraging
these technologies for clients. SQLite is essentially a proprietary format which would require constant copies or
incremental copies to an iceberg table or similar so these clients would be 1-copy rather than 0-copy away from the live tables
post ingestion.

This basically pushes me back to iceberg away from sqlite for this usecase for now.

## 11/16/2024

Switching to cursor for the next phase of development instead of vscode. Using Claude 3.5 Sonnet with cursor/copilot. It seems better
and have already found a few bugs in the code using full file chat. Trying simple chats with it like "Are there any bugs in this file?"
actually showed issues which I'm fixing. It also highlighted a potential problem indirectly. The StructType is meant as an attribute
type in a dataset. We worry about backwards compatibility when allowing schema changes. However, the StructType is interesting in
this regard. What is backwards compatible for a StructType? Fields are not nullable, thats an attribute property and the properties
in a StructType are not dataset attributes right now. You can't delete a field from a StructType. You cannot add one either because
thats the value in the old version of the StructType. Do I need a default value to allow adding new fields? We can modify a field
to have a more broad type (String[20] -> String[30]) but we can also use types which are backwards compatible. I can see how
without some changes here, StructType will be a problem in practise.

## 11/17/2024

Thinking through adoption. I think the first phase is to describe the legacy data flows using the DSL. Pick a set of
consumers and then create the Workspaces. Figure out the datasets that are required and the data producers who own them.

The consumer Workspaces should specify a fixed platform chooser pointing at the LegacyDataPlatform. The existing storage
devices with the data should be specified on this DataPlatform.

At this point, the DataSurface policy and governance are active and flagging issues. The legacy data flows are now being tracked but
the next step is to specify capture metadata for every data producer. The capture metadata is necessary for a data surface
dataplatform to be able to pull the data and feed it to an asset which the consumers will eventually use to consume the
data.

Next, change the chooser for a consumer to a real DataPlatform. When this change works through then the data will start
flowing towards the consumer. The customer can now compare the data arriving in that consumer with the original database
the consumer was using and verify that the data is the same and arriving in a timely and acceptable manner.

The customer can then deprecate the existing data pipelines and transistion to the new consumer database. Now, we repeat
the process migrating more consumers. As more consumers migrate, the key data producer datasets will start to all be
available in the new DataSurface ecosystem making further migrations easier.

