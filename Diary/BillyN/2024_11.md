# November 2024

## 11/04/2024

Switched to using Claude 3.5 Sonnet with github copilot/vscode, it seems better.

Have airflow running on the Dell. It was in a restart doom loop. Based on the docs [here](https://airflow.apache.org/docs/apache-airflow/stable/howto/docker-compose/index.html), I executed

```bash
docker compose up airflow-init
docker compuse up
```

and now it's working with the default user, airflow/airflow. I need to create an ingestion pipeline manually using AirFlow/Spark/Debezium/Postgres in and Minio. Then I work on automating it using DataSurface. This basically means building a DataPlatform which takes an intention DAG from DataSurface and then generates the airflow DAG for the pipeline and getting the existing spark ingestion code to run merging debezium changes in to an s3 deltalake hosted in Minio locally on the Dell.

I will package this first DataPlatform as part of the core DataSurface python module for now. I will need to work through seperating it later, possibly packaging it as a Docker container with a standard interface.

I need to pick a stack for this initial pipeline:

* Gitlab
* Airflow
* Spark V4
* Debezium
* Postgres input
* Minio for S3/iceberg storage
* Storage based iceberg Data Catalog
* Trino as consumer sql endpoint

So, I want a pipeline to generate data continously in postgres. Do CDC with debezium and push the deltas to s3/minio. Run a spark job to ingest those deltas in batches to delta lake tables stored in s3/minio. Create tables in Trino to query the data from s3/minio. This seems a decent first step and gives people a basic data lake/data warehouse for starters.

## 11/05/2024

Plan is to spend today making sure all the various elements in the stack are working and then manually build the pipeline and verify that data flows through it. I will then work on automating the pipeline using DataSurface.

On my Dell, the services right now are:

* 192.168.4.157:9000 - Portainer
* 192.168.4.157:9200 - Minio
* 192.168.4.157:11080 - Spark master (1 spark-worker)
* 192.168.4.157:80 - Gitlab (2 runners)
* 192.168.4.157:8080 - Airflow
* 192.168.4.157:12080 - Trino

## 11/06/2024

Discussing how to use Airflow with myself/copilot. I'm imagining a source database and am using CDC/Debezium to push delta events in to an s3 folder. There are then Spark jobs to read these deltas and apply the updates to an ice berg table also in s3 which can be queried by trino later. This is the basic pipeline. I have built systems where there was a spark job per table ingested for example but at scale this runs in to a few problems principally around resources used to ingest N tables and job latency when schedulers become overloaded by many tasks (millions). Alternatives would be to have a single spark job handle all tables within a single database ingestion for example. This collapses tasks from N to one but may add latency depending on how the job tackles the tables, is it serial, is it in parallel. When it's in parallel then that complicates managing resources globally as different jobs use different amounts of threads for example. So, it's harder to say I can execute N jobs at once from a memory point of view but may run out of threads. Really, a fine grained scheduling solution is needed and Spark likely isn't suitable for this at all, it's designed for batch jobs.

There is also the matter than Spark has overhead. If we're running on top of a SQL store (Snowflake) then we possibly only need a single JVM/python job to use odbc to issue the SQL calls against the database so we don't need a parallel computation framework to run the job. All the partitioning and executing the SQL in parallel is handled by Snowflake so Spark is redundant.

The discussion is really some kind of cluster of stateless event workers servicing a queue of lightweight tasks. This would be YARN and spark jobs, it could be AWS Lambda or something home grown. My own experience is that while 90% of tasks can likely run just fine on a single thread event, the remaining 10% require exponentially more resources. You might need a small job running on up to 4 servers for 92% of jobs to be covered. The last 1% though may need 100 server job.

The problem then is can you have a single event handler which can handle the single thread OR the multi-thread or fully clustered event task. Thats tricky. Standardizing on a single implementation which works for both like a Spark or Flink job is attractive when working on something like Iceberg or Delta Lake. If working with an active storage or thick storage layer like snowflake then that implementation can push down code in the following of SQL and allow snowflake to manage scaling that and resources. Thats the principle problem with iceberg and delta lake for me, they are currently dumb storage. All the compute runs in the client program/job. This is changing but it's not there yet. Delta Engine for example still runs within the client spark job not remotely like with snowflake. There are so many issues associated with running the database engine within the client but thats another story. This is one of my major gripes with Iceberg/delta lake/delta engine versus snowflake. Keeping the storage layer/query code external to your application is a major advantage for Snowflake vs Spark for now.

Delta Lake 4.0 is trying to be a remote execution layer for data access/manipulation but is brand new. Building a mission critical app for this year then maybe not the best idea... My biggest issue with spark/data bricks is they have no regardless for keeping storage layers, binary runtimes and APIs stable over a 5-10 year window. If I build an enterprise level system then I can't afford to rewrite it completely every 3-4 years when Spark N+1 ships which isn't compatible with the old one and writes from the new jobs cannot be read by the older jobs. Jobs need rewrites to switch APIs if moving up to latest version. Once you have 1000s of such jobs, rewriting simply isn't an option and now you are wishing you used SQL or a managed service like Snowflake.

Delta Lake 4.0 is basically Snowflake from Data bricks. It should allow storage format updates without needing to recompile the client jobs using it. So, you can benefit from performance/reliability improvements without needing to rewrite your jobs to use the latest spark/delta lake jars. The question is will they get it right for 4.0. As an example, look at Oracle database. The client JDBC/ODBC drivers work multiple versions backwards. You can run a 20 year old client against a new database. This is the kind of compatibility I need from a storage layer. Will Delta Lake promise this kind of backwards compatibility? It's easy for tech executives to be wined and dined and look at marketecture slides/PDFs and not realize what they are actually getting mostly because they will move to the next job before the problems become apparent. Technical debt is fine so long as you aren't around to deal with it in the future, after all...

## 11/07/2024

I'm just thinking about the easiest way to get this working. I have decided I likely do not need to use Spark. If I use SQL and especially the new commands such as MERGE then
I can push almost all the processing down to the database and not need parallism at the job layer. The code then becomes relatively portable. I can test it against Delta Lake 4 (remote/local),
Snowflake, regular databases such as Postgres (which simplifies day to day testing enormously). I'd written and tested a simple LIVE table MERGE Spark job which uses SQL but I'm staring at it
now from this point of view. Does it need to be Spark? I did it in Java but maybe, it's better in Python especially if there is little computation happening in it as most computation is pushed
down as SQL. I'd thought about this before I retired 3 or 4 years ago after trying to do these kinds of jobs in Spark first, then MR and finally Flink. Spark was too hard to tune and a version
nightmare. MR was much faster, easier to tune and used significantly less resources than Spark. Flink was easier to tune and offered Kappa type promises. I considered Apache Beam also to get more
portability butit seemed too risky in terms of layer on a layer on a layer and it's relatively new and google was behind it. Google being behind it is a problem because they have a history of
abandoning stuff when it's useful for them. Enterprise software needs to last at least a decade and be backwards compatible the whole time, no one can afford to add non backwards compatible
technologies in an environment with shared data used by old and new code. You are doomed if you do this.

But, all these technologies work on top of thin storage, hdfs or s3. Snowflake or Hudi offer thick storage. Storage with an SQL API. This thick storage is more attractive to me. Apache Iceberg and the current
delta lakes are still thin storage. Snowflake and Google BigTable are thick storage and offer significant advantages in this space in terms of longevity, automatic compute scale out for queries and thin client
support. People do not realize how important thin client support is to storage. If you are shipping the storage engine with your application then thats a problem. Imagine if an application which used Oracle
shipped a copy of Oracle RDMS with the application and the application was hard linked with it. Imagine every application you use did this and there would be N versions of Oracle floating around, how do you
upgrade Oracle? By recompiling/linking every application? You would agree thats insane but thats exactly what happens when you use Apache Iceberg or Delta lake today.

## 11/08/2024

Researching delta lake 4 more, the problem with it is that they state new versions will be able to read and more importantly write old delta table formats. This is a step forward but doesn't really solve the problem. This capability basically allows new data clients to use newer spark/delta versions but the storage layer is trapped at the lowest
version until older code can be decommisioned or rewritten to use new APIs. This means new features/some optimizatons and so on cannot be taken advantage of because the storage layer version is not advancing.

Again, this is why we need a client/server approach. The details of reading and writing the storage layer should not be visible to the clients so it can advance independently of the client version.
Being able to write old formats from the client is not this. Delta connect from Data bricks is in preview and promises to allow this but we will see how long it takes for Data bricks to allow ANY job using
any API to work with Delta connect.

## 11/10/2024

For system processing in pipelines, I think this singlethread JDBC style client pushing down SQL to a thick storage layer is likely good enough. The problems with this approach tend to be when you need application
logic to manipulate data or produce derivative data. You can make some kind of UDF which can itself be pushed down to the storage layer but functionality will be limited to stateless operations and no connectivity. This is kind
of where Spark/Flink come in to play but I think the way they work today is backwards. Today, the Spark job runs splits Dataframes in to partitions and runs workers against records in those partitions producing thederivative data
which is then combined to the output DataFrame. This is harder than it looks. How many works/partitions should the job use? Well, it depends on how many records there are and how much time you have to do the work.
This can change on a run by run basis. You would need to know how large the incoming DataFrame is to really figure how how many workers to use on a query by query basis. If your workload is very uniform then this is easy
but what when it's not? What about when it uniform mostly but periodically things goes crazy and your job blows up with OOM errors or is too slow? Do you want people retuning jobs when this happens in the critical path? No,
I'd prefer we didn't do that.

Whats really needed here is for roles to shift. Snowflake, for example, receives the query algebra which includes the java code grabbing the DataFrame. Snowflake needs to host the computation which it sizes based
on the incoming data and decides how many workers and uses a crazy efficient memory mapper to zero serialize the input data and run the java code against it. This is the only way this can work. I'm amazed that they
haven't done it yet. Now, this are easier for the customer. Here is the code DAG I want you to process and push it down. A single thread constructs that DAG and pushes it down. Machine learning and LLM style compute
DAGs work this way already and data processing code is no different. Keeping the management of the compute resources independently of the data engine makes no sense. The data engine needs to manage the compute resources
to execute the transformation DAG or what ever we want to call this.

