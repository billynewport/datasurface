# August 2025

## 2025/08/01

I have made good progress on componentizing the kubernetes services yaml now in the Yellow PSP. I have 2 YellowDataPlatforms in the yellow starter project. One is LIVE and the other is MILESTONED. They both need a "stack" thats common to host the airflow and merge database. I'd like the ability to change the stack to be outside the YellowDataPlatform instances, 2 database or 2 databases, the namespace, nothing to do with them really. It's the PSP which is the common stack. However, ideally even the PSP I have now is not abstract enough to be a generic kubernetes environment to build the BlueDataPlatform which may be kubernetes based but completely different from Yellow, it might have an different job scheduler, it might run in minio for s3 compatible storage and a SQL database on top of S3. But, I don't want to have to redo all the kubernetes stuff when I need to do this.

Assembly and Component are basically a jinja template assembly framework and not kubernetes specific. I have the different Components for name spaces, postgres, airflow, logging and so on which can be reused. Maybe this is enough?

Working through the ripple on this, it's big. Always a lift...

Ok, it's working again. Like I said, big ripple for this but it's a big improvement in the area of making the kubernetes yaml stuff much more usable for different stacks. Lot of secret consistency issues fixed.

The YellowPlatformServiceProvider with YellowDataPlatform is now the Yellow stack. There is also an Assembly and Component classes which are used to assemble the components needed to run the Yellow stack. I have 2 assemblies, one with the postgres database shared between Airflow and the merge engine and another with them in seperate databases.

### Take a knee

So, where am I now? I have a working Yellow stack. It's easier to create other stacks also thanks to the assembly and component classes. I can demonstrate changing the model and the running Yellow stack will automatically update when the merge handler is called.

Short term goals:

* Allow postgres to specify a storage capacity.
* NFS container to hold the git clone cache for all the jobs
* Test Yellow with a larger model. 50 source databases, 200 datasets per source database and a 100 consumer Workspaces. This would be 50 ingestion streams. Not that big of a test when I think about it. It would be 4k tables in the merge database, staging+merge per dataset.
* Add extra supported ingestions. I just have SQL snapshot for now. I need maybe:
  * Ingest CSV from S3 (minio)
  * Incremental SQL ingestion
  * Ingest data from a JSON API.
* Scripted test to show a system evolving over time and making sure Yellow works as expected.
* Make Datasurface a MCP service.
* AI based operators watching Yellow's operational state and taking action.
* Cut a V1.0 release soon. Decide on which of the above are important enough to include. Maybe what I already have is enough for V1.0 except for the NFS and storage size tasks.

## 2025/08/03

Implemented StorageRequirement on SQLDatabase and it's subclasses, tests and added support for storage size in the Yellow stack for Postgres databases.

I'd like to get the NFS container support working for  the git cache in Yellow.

### Multi-architecture docker container images

I was only pushing arm64 images. Fixed this now with:

```bash
docker buildx build --platform linux/amd64,linux/arm64 -f Dockerfile.datasurface -t datasurface/datasurface:latest --push .
```

### Docker desktop limitations

I could not get docker desktop kubernetes to work with my NFS stuff today. I ended up testing it on my Dell running Ubuntu 24 and full k8s.

### Docker back off issues

The infrastructure_dag.py.j2 uses pull ways for its image policy. This needs to be changed to pull if not present.

## 2025/08/04

Gave up on NFS for the git clone cache shared PVs. It seems to be a kubernetes/NFS issue that hasn't been fixed. I switched to longhorn on my remote kubernetes cluster and it seems to just work.

Container image pulls are an issue.

With longhorn as the default storage provider, postgres wouldn't start because the data volume has a lost+found in it and it requires an empty folder or it wont start. I modified the component to put the database in the data subfolder and that fixed it.

Need to modify the psp to have a storage class name for postgres/ dags/logs etc.

I'm a little inconsistent with the secret stuff in infrastructure_dag.py.j2. Sometimes I use the secret name as a template variable and sometimes I use the context to get the secret name.

### Longhorn issues

It's working well now but longhorn caused some issues. First, I only gave my kubernetes cluster 32GB of disk space. Longhorn wants to reserve capacity for all its volumes and this exceeded 32GB. I added more disk space, enlarged the file system to 100GB and that fixed that. Then I found the default replica count on longhorn volumes was 3. I only have 1 node in the cluster so that caused issues also. I set the replica count to 1 and that fixed that.

### Job sizing parameterization

I need a way to specify how much capacity to limit a transformer or ingestion job to. I can have a system wide default. Normal ingestions are just python jobs running SQL within the database so they are not resource intensive. DataTransformers can be more resource intensive. Either way, we need a way to specify the resource limits for an ingestion or transformer job. Job settings are basically:

```yaml
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "1000m"
```

These 4 values need to be configurable per ingestion or transformer job.

Airflow web and scheduler need the same as well as Postgres. Postgres also needs a storage size which was already there.

handle model merge needs to write sizing requests to the database for each ingestion and transformer job.

### DataTransformer DAG overlap with DT INGESTION DAG

This is an issue. These need to run sequentially and never overlap. This seems impossible to do with 2 seperate DAGs. This is complicated because I wrote the ingestion to be non blocking airflow. So, ingestion and MERGE can run seperately. A single batch thus runs as 2 DAG runs, the DAG first does ingestion and then it does MERGE. The batch state is checked in the job to see what needs to happen.

This seems too complex for airflow. I need to make the ingestion job run till its finished.

I will also provide a way to allow resource overrides on a stream by stream and transformer by transformer basis.

## 2025/08/05

I implemented a general job hinting system for ingestion and transformer jobs. It's working now.

### Longhorn delaying container starts

I see jobs in airflow sometimes running in 10-15 seconds and other times taking 40-60 seconds. The difference is how long longhorn takes to mount the git clone cache volume. It seems related to longhorn's readiness probe mistakenly killing the share manager. I see the share manager pod restarting every minute of so and this delays the container start when the timing is right. Things work and are stable but, the inconsistency in container times is frustrating. The issue appears to be that the storage manager checks after 3 seconds for a PID file and if its not there then it kills the share manager and restarts it. The NFS server used in the share manager is a bit slow to start up and needs 7-10 seconds. This 3 second initialDelay time is hard coded in longhorn amazingly. So, I may be stuck with it. It's not a stability thing, the system works with no errors. The share manager doesn't fail and break pods with existing mounts. But, while it's restarting, new pods are delayed for 20-30 seconds. This isn't a Yellow issue in general, it's a longhorn issue. It'll be interesting to see on Azure etc, what the vloume mound delays are if any.

### Current testing environments

I'm currently testing datasurface on Ubuntu 24 with Kubernetes 1.33 and Longhorn 1.9.1 for my PVC storage manager (Postgres, Airflow, git clone cache). I'm running kubernetes on an x86 virtual machine with 32GB of RAM and 100GB of disk space and 8 cores.I don't expect longhorn to work on docker desktop on my mac. I can run git clone cache disabled if I need to run on my macbook.

### Integrate DataTransformer DAG with the output ingestion DAG as a task.

This is the next task. Fix ingestion/merge jobs in general to so ingestion (copy to staging) and merge in a single task and then do this for data transformers as a task rather than a seperate dependant DAG like now.

### YellowAzure thoughts

I'm thinking of creating the first cloud version. Azure has managed kubernetes AKS, cosmos DB for postgres and a managed airflow also. Secret management needs to change to use Azure key vault. It would be a good first cloud version and allow refactoring of Yellow to be more friendly to cloud implementations. There all look easy to do, AWS/GCP and Azure.

### Added image pull policy to the PSP

I added an image pull policy to the PSP. It's working now. Most important for the airflow kubernetes jobs. They were always pulling the image even if it was already present. Changed the default to IfNotPresent.

## 2025/08/06

Working on a large example ecosystem model. I'm using Microsofts WWI (World Wide Imports) example. This also lead to adding support for Geography types in the model.

### Spent the day trying to move up with Airflow 3.0.3

and it as a huge time suck. The kubernetes pod operator doesn't work as far as I can tell as at least I couldn't get it to work with claude 4 helping, it could not create the containers. Thats 8 hours of my time. I'm going to stick with 2.8.1 for now.
