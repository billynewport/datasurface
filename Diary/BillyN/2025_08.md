# August 2025

## 2025/08/01

I have made good progress on componentizing the kubernetes services yaml now in the Yellow PSP. I have 2 YellowDataPlatforms in the yellow starter project. One is LIVE and the other is MILESTONED. They both need a "stack" thats common to host the airflow and merge database. I'd like the ability to change the stack to be outside the YellowDataPlatform instances, 2 database or 2 databases, the namespace, nothing to do with them really. It's the PSP which is the common stack. However, ideally even the PSP I have now is not abstract enough to be a generic kubernetes environment to build the BlueDataPlatform which may be kubernetes based but completely different from Yellow, it might have an different job scheduler, it might run in minio for s3 compatible storage and a SQL database on top of S3. But, I don't want to have to redo all the kubernetes stuff when I need to do this.

Assembly and Component are basically a jinja template assembly framework and not kubernetes specific. I have the different Components for name spaces, postgres, airflow, logging and so on which can be reused. Maybe this is enough?

Working through the ripple on this, it's big. Always a lift...

Ok, it's working again. Like I said, big ripple for this but it's a big improvement in the area of making the kubernetes yaml stuff much more usable for different stacks. Lot of secret consistency issues fixed.

The YellowPlatformServiceProvider with YellowDataPlatform is now the Yellow stack. There is also an Assembly and Component classes which are used to assemble the components needed to run the Yellow stack. I have 2 assemblies, one with the postgres database shared between Airflow and the merge engine and another with them in seperate databases.

### Take a knee

So, where am I now? I have a working Yellow stack. It's easier to create other stacks also thanks to the assembly and component classes. I can demonstrate changing the model and the running Yellow stack will automatically update when the merge handler is called.

Short term goals:

* Allow postgres to specify a storage capacity.
* NFS container to hold the git clone cache for all the jobs
* Test Yellow with a larger model. 50 source databases, 200 datasets per source database and a 100 consumer Workspaces. This would be 50 ingestion streams. Not that big of a test when I think about it. It would be 4k tables in the merge database, staging+merge per dataset.
* Add extra supported ingestions. I just have SQL snapshot for now. I need maybe:
  * Ingest CSV from S3 (minio)
  * Incremental SQL ingestion
  * Ingest data from a JSON API.
* Scripted test to show a system evolving over time and making sure Yellow works as expected.
* Make Datasurface a MCP service.
* AI based operators watching Yellow's operational state and taking action.
* Cut a V1.0 release soon. Decide on which of the above are important enough to include. Maybe what I already have is enough for V1.0 except for the NFS and storage size tasks.

## 2025/08/03

Implemented StorageRequirement on SQLDatabase and it's subclasses, tests and added support for storage size in the Yellow stack for Postgres databases.

I'd like to get the NFS container support working for  the git cache in Yellow.

### Multi-architecture docker container images

I was only pushing arm64 images. Fixed this now with:

```bash
docker buildx build --platform linux/amd64,linux/arm64 -f Dockerfile.datasurface -t datasurface/datasurface:latest --push .
```

### Docker desktop limitations

I could not get docker desktop kubernetes to work with my NFS stuff today. I ended up testing it on my Dell running Ubuntu 24 and full k8s.

### Docker back off issues

The infrastructure_dag.py.j2 uses pull ways for its image policy. This needs to be changed to pull if not present.

## 2025/08/04

Gave up on NFS for the git clone cache shared PVs. It seems to be a kubernetes/NFS issue that hasn't been fixed. I switched to longhorn on my remote kubernetes cluster and it seems to just work.

Container image pulls are an issue.

With longhorn as the default storage provider, postgres wouldn't start because the data volume has a lost+found in it and it requires an empty folder or it wont start. I modified the component to put the database in the data subfolder and that fixed it.

Need to modify the psp to have a storage class name for postgres/ dags/logs etc.

I'm a little inconsistent with the secret stuff in infrastructure_dag.py.j2. Sometimes I use the secret name as a template variable and sometimes I use the context to get the secret name.

### Longhorn issues

It's working well now but longhorn caused some issues. First, I only gave my kubernetes cluster 32GB of disk space. Longhorn wants to reserve capacity for all its volumes and this exceeded 32GB. I added more disk space, enlarged the file system to 100GB and that fixed that. Then I found the default replica count on longhorn volumes was 3. I only have 1 node in the cluster so that caused issues also. I set the replica count to 1 and that fixed that.

### Job sizing parameterization

I need a way to specify how much capacity to limit a transformer or ingestion job to. I can have a system wide default. Normal ingestions are just python jobs running SQL within the database so they are not resource intensive. DataTransformers can be more resource intensive. Either way, we need a way to specify the resource limits for an ingestion or transformer job. Job settings are basically:

```yaml
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "1000m"
```

These 4 values need to be configurable per ingestion or transformer job.

Airflow web and scheduler need the same as well as Postgres. Postgres also needs a storage size which was already there.

handle model merge needs to write sizing requests to the database for each ingestion and transformer job.

### DataTransformer DAG overlap with DT INGESTION DAG

This is an issue. These need to run sequentially and never overlap. This seems impossible to do with 2 seperate DAGs. This is complicated because I wrote the ingestion to be non blocking airflow. So, ingestion and MERGE can run seperately. A single batch thus runs as 2 DAG runs, the DAG first does ingestion and then it does MERGE. The batch state is checked in the job to see what needs to happen.

This seems too complex for airflow. I need to make the ingestion job run till its finished.

I will also provide a way to allow resource overrides on a stream by stream and transformer by transformer basis.

## 2025/08/05

I implemented a general job hinting system for ingestion and transformer jobs.
