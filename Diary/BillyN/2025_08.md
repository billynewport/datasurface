# August 2025

## 2025/08/01

I have made good progress on componentizing the kubernetes services yaml now in the Yellow PSP. I have 2 YellowDataPlatforms in the yellow starter project. One is LIVE and the other is MILESTONED. They both need a "stack" thats common to host the airflow and merge database. I'd like the ability to change the stack to be outside the YellowDataPlatform instances, 2 database or 2 databases, the namespace, nothing to do with them really. It's the PSP which is the common stack. However, ideally even the PSP I have now is not abstract enough to be a generic kubernetes environment to build the BlueDataPlatform which may be kubernetes based but completely different from Yellow, it might have an different job scheduler, it might run in minio for s3 compatible storage and a SQL database on top of S3. But, I don't want to have to redo all the kubernetes stuff when I need to do this.

Assembly and Component are basically a jinja template assembly framework and not kubernetes specific. I have the different Components for name spaces, postgres, airflow, logging and so on which can be reused. Maybe this is enough?

Working through the ripple on this, it's big. Always a lift...

Ok, it's working again. Like I said, big ripple for this but it's a big improvement in the area of making the kubernetes yaml stuff much more usable for different stacks. Lot of secret consistency issues fixed.

The YellowPlatformServiceProvider with YellowDataPlatform is now the Yellow stack. There is also an Assembly and Component classes which are used to assemble the components needed to run the Yellow stack. I have 2 assemblies, one with the postgres database shared between Airflow and the merge engine and another with them in seperate databases.

### Take a knee

So, where am I now? I have a working Yellow stack. It's easier to create other stacks also thanks to the assembly and component classes. I can demonstrate changing the model and the running Yellow stack will automatically update when the merge handler is called.

Short term goals:

* Allow postgres to specify a storage capacity.
* NFS container to hold the git clone cache for all the jobs
* Test Yellow with a larger model. 50 source databases, 200 datasets per source database and a 100 consumer Workspaces. This would be 50 ingestion streams. Not that big of a test when I think about it. It would be 4k tables in the merge database, staging+merge per dataset.
* Add extra supported ingestions. I just have SQL snapshot for now. I need maybe:
  * Ingest CSV from S3 (minio)
  * Incremental SQL ingestion
  * Ingest data from a JSON API.
* Scripted test to show a system evolving over time and making sure Yellow works as expected.
* Make Datasurface a MCP service.
* AI based operators watching Yellow's operational state and taking action.
* Cut a V1.0 release soon. Decide on which of the above are important enough to include. Maybe what I already have is enough for V1.0 except for the NFS and storage size tasks.

## 2025/08/03

Implemented StorageRequirement on SQLDatabase and it's subclasses, tests and added support for storage size in the Yellow stack for Postgres databases.

I'd like to get the NFS container support working for  the git cache in Yellow.

### Multi-architecture docker container images

I was only pushing arm64 images. Fixed this now with:

```bash
docker buildx build --platform linux/amd64,linux/arm64 -f Dockerfile.datasurface -t datasurface/datasurface:latest --push .
```

### Docker desktop limitations

I could not get docker desktop kubernetes to work with my NFS stuff today. I ended up testing it on my Dell running Ubuntu 24 and full k8s.

### Docker back off issues

The infrastructure_dag.py.j2 uses pull ways for its image policy. This needs to be changed to pull if not present.

## 2025/08/04

Gave up on NFS for the git clone cache shared PVs. It seems to be a kubernetes/NFS issue that hasn't been fixed. I switched to longhorn on my remote kubernetes cluster and it seems to just work.

Container image pulls are an issue.

With longhorn as the default storage provider, postgres wouldn't start because the data volume has a lost+found in it and it requires an empty folder or it wont start. I modified the component to put the database in the data subfolder and that fixed it.

Need to modify the psp to have a storage class name for postgres/ dags/logs etc.

I'm a little inconsistent with the secret stuff in infrastructure_dag.py.j2. Sometimes I use the secret name as a template variable and sometimes I use the context to get the secret name.

### Longhorn issues

It's working well now but longhorn caused some issues. First, I only gave my kubernetes cluster 32GB of disk space. Longhorn wants to reserve capacity for all its volumes and this exceeded 32GB. I added more disk space, enlarged the file system to 100GB and that fixed that. Then I found the default replica count on longhorn volumes was 3. I only have 1 node in the cluster so that caused issues also. I set the replica count to 1 and that fixed that.

### Job sizing parameterization

I need a way to specify how much capacity to limit a transformer or ingestion job to. I can have a system wide default. Normal ingestions are just python jobs running SQL within the database so they are not resource intensive. DataTransformers can be more resource intensive. Either way, we need a way to specify the resource limits for an ingestion or transformer job. Job settings are basically:

```yaml
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "1000m"
```

These 4 values need to be configurable per ingestion or transformer job.

Airflow web and scheduler need the same as well as Postgres. Postgres also needs a storage size which was already there.

handle model merge needs to write sizing requests to the database for each ingestion and transformer job.

### DataTransformer DAG overlap with DT INGESTION DAG

This is an issue. These need to run sequentially and never overlap. This seems impossible to do with 2 seperate DAGs. This is complicated because I wrote the ingestion to be non blocking airflow. So, ingestion and MERGE can run seperately. A single batch thus runs as 2 DAG runs, the DAG first does ingestion and then it does MERGE. The batch state is checked in the job to see what needs to happen.

This seems too complex for airflow. I need to make the ingestion job run till its finished.

I will also provide a way to allow resource overrides on a stream by stream and transformer by transformer basis.

## 2025/08/05

I implemented a general job hinting system for ingestion and transformer jobs. It's working now.

### Longhorn delaying container starts

I see jobs in airflow sometimes running in 10-15 seconds and other times taking 40-60 seconds. The difference is how long longhorn takes to mount the git clone cache volume. It seems related to longhorn's readiness probe mistakenly killing the share manager. I see the share manager pod restarting every minute of so and this delays the container start when the timing is right. Things work and are stable but, the inconsistency in container times is frustrating. The issue appears to be that the storage manager checks after 3 seconds for a PID file and if its not there then it kills the share manager and restarts it. The NFS server used in the share manager is a bit slow to start up and needs 7-10 seconds. This 3 second initialDelay time is hard coded in longhorn amazingly. So, I may be stuck with it. It's not a stability thing, the system works with no errors. The share manager doesn't fail and break pods with existing mounts. But, while it's restarting, new pods are delayed for 20-30 seconds. This isn't a Yellow issue in general, it's a longhorn issue. It'll be interesting to see on Azure etc, what the vloume mound delays are if any.

### Current testing environments

I'm currently testing datasurface on Ubuntu 24 with Kubernetes 1.33 and Longhorn 1.9.1 for my PVC storage manager (Postgres, Airflow, git clone cache). I'm running kubernetes on an x86 virtual machine with 32GB of RAM and 100GB of disk space and 8 cores.I don't expect longhorn to work on docker desktop on my mac. I can run git clone cache disabled if I need to run on my macbook.

### Integrate DataTransformer DAG with the output ingestion DAG as a task.

This is the next task. Fix ingestion/merge jobs in general to so ingestion (copy to staging) and merge in a single task and then do this for data transformers as a task rather than a seperate dependant DAG like now.

### YellowAzure thoughts

I'm thinking of creating the first cloud version. Azure has managed kubernetes AKS, cosmos DB for postgres and a managed airflow also. Secret management needs to change to use Azure key vault. It would be a good first cloud version and allow refactoring of Yellow to be more friendly to cloud implementations. There all look easy to do, AWS/GCP and Azure.

### Added image pull policy to the PSP

I added an image pull policy to the PSP. It's working now. Most important for the airflow kubernetes jobs. They were always pulling the image even if it was already present. Changed the default to IfNotPresent.

## 2025/08/06

Working on a large example ecosystem model. I'm using Microsofts WWI (World Wide Imports) example. This also lead to adding support for Geography types in the model.

### Spent the day trying to move up with Airflow 3.0.3

and it as a huge time suck. The kubernetes pod operator doesn't work as far as I can tell as at least I couldn't get it to work with claude 4 helping, it could not create the containers. Thats 8 hours of my time. I'm going to stick with 2.8.1 for now.

### Versioning assemblies

Even if I had managed to get airflow 3.0.3 working I made a mistake in that I didn't keep the original 2.8.1 version also. I did some refactoring just now to preserve 281 for next time. I'd add an airflow 303 folder, assembly and new infrastructure_dag.py.j2 and psp_airflow.yaml.j2 files. That way I can keep both. It's 2 variants of Yellow in a way, Yellow 281 and Yellow 303. It makes me wonder about the cloud vendor versions of yellow which will use hosted airflows and I'm expecting similar issues to today. Good experience, worst case. The AI claude doesn't do as well with airflow 3.0.3 as it does with 2.8.1 also...

### Tomorrow

* I guess I need to consolidate copy to staging and merge in to a single task and change the transformer DAG run do dt ingestion as a task within its DAG.
* Ingest from a YellowForensic style merge table. This is a key feature to limit fanout on a source database. A single DataPlatform instance can be designated as primary ingestor and then other DataPlatform instances can ingest from that platforms merge tables. This is key for scaling and for federation between private/cloud as well as cloud/cloud.
* Test WWI model as a next step up. 31 tables. I need a postgres with GIS enabled to use WWI, different docker image. New data simulator for WWI. It's still a single ingestion stream with 31 tables inside it.
* Next step up would be 100 WWI datastores. Now, we are at 100 ingestion streams, each with 31 tables inside. At 1 minute intervals, thats just over one ingestion per second. They currently take 40 seconds to run each. How many can my setup run concurrently is a question.
* Next step up would be 1000 WWI datastores. Now, we are at 1000 ingestion streams, each with 31 tables inside.

## 2025/08/07

Today, primary ingestion tagging. This allows a central/infra team to designate which DataPlatforms are allowed to ingest data from a Datasource. Other data platforms needing that data cannot ingest it directly from the data store. They need to get it from the primary ingestion platform. Typically, a primary ingestion platform will use forensic milestoning only. This makes ingestion much more efficient. This allows the ingestion load on a source datacontainer to be limited to a small number of data platforms.

Pondering whether I need to better seperate the model from infrastructure modelling. I starting this with the dsg platform assignment stuff, I think I need to do more of this. I don't want people using the system (producers/consumers) worrying about infrastructure complexity, thats the whole point of this. It's not their problem.

The PrimaryIngestionPlatform has a few uses.

* Limit fanout on a source datastore to a limited number of data platforms.
* Efficient ingestion regardless of source capture metadata, it's always a milestoned merge table in the PIP.
* A DataPlatform can choose a PIP it understands and ingest from that.
* Cloud scenarios. An onsite data platform can designed a datacontainer, a postgres, in the DMZ potentially as a PIP for stores needed on the cloud dataplatform. All cloud ingestions come from the data container in the DMZ. The link can go down and then when it comes back, the cloud can take over. Also when pulling data onsite from the cloud, all changes go through an onsite PIP. This limits outbound traffic from the cloud to onsite, saving money.

## 2025/08/08

With the PIP stuff in place, I'll modify YellowStarter for the Forensic yellow to be the PIP. This will cause the yellow live to ingest from the forensic yellow merge tables. What this means is the yellow live will reflect the actual live records present in forensic. This requires an initial ingestion which takes the live records as of the current batch. Then, it pulls the changes for subsequent batches and reflects that reality in the live yellow. The batch id's will match on both sides. The batch counter will be the same on both sides.

## 2025/08/09

Spent time improving the way view names are calculated and consolidated that code in YellowDatasetUtilities.

### Airflow prioritization

I already had the concept of Workload priorities in the ecosystem model. I simplified this today and the airflow DAGs now use it to prioritize DAG/tasks according to the model priority. Major feature enabled now.

I switched from looping over Workspace or storestoingest in the populateDAG or transformer methods to looping over the graph node looking for the right node types. This lets me use the node priority as a new field in the DAG/transformer context jsons in the database tables. The infrastructure dag template picks that out now for the task priority.

### PIP progress

Working towards implementing SQLMergeIngestion. This is tomorrows work though. The main thoughts are do I add an IUD flag to the live and forensic staging tables. For a full snapshot, I can just set IUD to "I".

## 2025/08/10

Working in Dataplatform.md. It's a little dated as this point.

## 2025/08/11

Just making sure where I am is working. Finding issues linting now with latest changes. Problems principally come down to DataPlatforms over linting. Yellow was using graph.workspaces a lot which is wrong sometimes. Whats assigned to a DataPlatform is a DSG, not a Workspace. There are DSGs in a Workspace which are not assigned to a DataPlatform. The Yellow forensic was liniting a DSG which was LIVE and vice versa. I went over the code looking for issues like this and corrected them.

createGraphg may be called too many times. Once during lintandhydrate and again after platform assignment which is a waste of time.

## 2025/08/13

Refactoring yellow ingest/merge. The ingest/merge job is now single cycle, i.e. ingest/merge run if possible in the same job. This helps giving the volume bind latencies I'm seeing with kubernetes/longhorn and expect to continue to see even on cloud setups. This also allows me to integrate the output ingestion job as a task in the datatransformer DAG. This reduces the DAG count but more importantly, the ingestion job now cannot run in parallel with the datatransformer job, they would always be sequential which avoids potential race conditions/corruption.

### Linting speed

Something to think about is right now in Yellow, the model is loaded for every job that runs. I can load the model quickly as linting is redundant as this was already done when the model was committed to git. I will likely need to add a way to load models without linting, that will be very fast. I can built tests to verify this. test_large_models.py verifies this. A model with 1000 Datastores, 2000 Workspaces and 40 tables per datastore (40k datasets) takes 4 seconds to load with linting. I don't think this is a problem at all in the overall scheme of things for now.

### Back to supporting ingest from a merge table

## 2025/08/15

I have merge remote live working now with tests. I used claude 4 sonnet and gpt-5 for this one and gpt-5 wasn't bad, I ended the day still using it. It's half the price of claude 4 so it's a good deal. Tomorrow, I will add the merge remote forensic tests and get that working. Then, I'll deploy a test to the full running system and verify it there also. It's 12:30am so I guess thats tomorrow, the remote forensic is working now with tests also.

## 2025/08/16

It's all working again on my single node kubernetes cluster. I will do a test over the weekend to test the priamry ingestion stuff. I also want to do a scale test just to see what happens on a single node setup. I'm documenting Yellow in terms of SDC (Slowly changing dimensions) also. Yellow technically supports SCD1 and SCD2. SCD1 is live records only, a mirror of the source table. SCD2 is fully milestoned so all observed versions of records for the primary key are kept with a batch in and out milestone. One batch out sentinel value marks live records. I added a lot of tests to make sure the remote merge SCD1 and SCD2 are working correctly. I need to work on additional ingestion types such as CDC streams and so on also. These are easier now after the remote SCD1 and SCD2 work. I used GPT5 in max mode for a lot of the remote merge work and it worked well, it's just slow. I also made changes so that job failures in airflow actually caused airflow job failures by using the correct exit codes.

Refactoring creating assemblies for Yellow out of Yellow PSP. I need to do this so I can make other assemblies for different similar stacks such as AWS, Azure and GCP. Also scenarios where the databases are provided outside the platform, i.e. preprovisioned. The postgres database we have been using was provisioned by us and running on longhorn for storage. Clearly, not the most performant database.

The external assembly stuff is running locally now so it's working.

### airflow template issues

Doesn't use afhostpair for name and port

## 2025/08/17

I created a new virtual machine on the Dell today (outside kubernetes) and then make a new assembly to allow a YellowDataPlatform to be created using an external database for the system. The merge database and the airflow database both run on the external database. This is a much higher performance setup than using longhorn PVCs for storage in the normal assembly.

This is a step towards running this on the cloud. A cloud provider will have a managed database, kubernetes and possibly a managed airflow also. The assembly breakouts in assembly.py are a good start down this path. The infra config needed to be broken out of the YellowPSP.

It's working. I hit kubernetes DNS issues. I'm using tailscale as a private VPN between my virtual machines including kubernetes. The kubernetes pods couldn't see the tailscale hostnames. I tried claude 4 sonnet in max mode but it couldn't fix it. I tried GPT5 and that figured it out.

It's documented at the end of the (HOWTO)[docs/yellow_dp/HOWTO_Setup_YellowDataPlatform_Environment.md] file. Nothing to do with datasurface, just my own kubernetes setup.

## 2025/08/18

Keeping the staging records for batches is a good idea. From an audit point of view, having the input data helps when checking whether there are merge issues later on. If you don't have the staging data then proving the merges are valid is a lot harder. They can always be pruned daily or weekly. Just looking at the postgres performance statistics this morning.

The code was dropping tables to change primary keys which was not good so that code was modified to modify the primary key constraints instead. This is a lot more efficient and doesn't require dropping and recreating the tables.

Ok, this morning round of testing is finished. It's now possible to override batch sizes for merge and ingest to staging. The default was raised from 1k to 50k records per chunk. Actually batchSize is the wrong name, should be chunkSize to avoid confusion with batches. Staging tables now retain the staging records for each batch. When a batch resets, the staging records for that batch are deleted only.