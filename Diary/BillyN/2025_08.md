# August 2025

## 2025/08/01

I have made good progress on componentizing the kubernetes services yaml now in the Yellow PSP. I have 2 YellowDataPlatforms in the yellow starter project. One is LIVE and the other is MILESTONED. They both need a "stack" thats common to host the airflow and merge database. I'd like the ability to change the stack to be outside the YellowDataPlatform instances, 2 database or 2 databases, the namespace, nothing to do with them really. It's the PSP which is the common stack. However, ideally even the PSP I have now is not abstract enough to be a generic kubernetes environment to build the BlueDataPlatform which may be kubernetes based but completely different from Yellow, it might have an different job scheduler, it might run in minio for s3 compatible storage and a SQL database on top of S3. But, I don't want to have to redo all the kubernetes stuff when I need to do this.

Assembly and Component are basically a jinja template assembly framework and not kubernetes specific. I have the different Components for name spaces, postgres, airflow, logging and so on which can be reused. Maybe this is enough?

Working through the ripple on this, it's big. Always a lift...

Ok, it's working again. Like I said, big ripple for this but it's a big improvement in the area of making the kubernetes yaml stuff much more usable for different stacks. Lot of secret consistency issues fixed.

The YellowPlatformServiceProvider with YellowDataPlatform is now the Yellow stack. There is also an Assembly and Component classes which are used to assemble the components needed to run the Yellow stack. I have 2 assemblies, one with the postgres database shared between Airflow and the merge engine and another with them in seperate databases.

### Take a knee

So, where am I now? I have a working Yellow stack. It's easier to create other stacks also thanks to the assembly and component classes. I can demonstrate changing the model and the running Yellow stack will automatically update when the merge handler is called.

Short term goals:

* Allow postgres to specify a storage capacity.
* NFS container to hold the git clone cache for all the jobs
* Test Yellow with a larger model. 50 source databases, 200 datasets per source database and a 100 consumer Workspaces. This would be 50 ingestion streams. Not that big of a test when I think about it. It would be 4k tables in the merge database, staging+merge per dataset.
* Add extra supported ingestions. I just have SQL snapshot for now. I need maybe:
  * Ingest CSV from S3 (minio)
  * Incremental SQL ingestion
  * Ingest data from a JSON API.
* Scripted test to show a system evolving over time and making sure Yellow works as expected.
* Make Datasurface a MCP service.
* AI based operators watching Yellow's operational state and taking action.
* Cut a V1.0 release soon. Decide on which of the above are important enough to include. Maybe what I already have is enough for V1.0 except for the NFS and storage size tasks.

## 2025/08/03

Implemented StorageRequirement on SQLDatabase and it's subclasses, tests and added support for storage size in the Yellow stack for Postgres databases.

I'd like to get the NFS container support working for  the git cache in Yellow.

### Multi-architecture docker container images

I was only pushing arm64 images. Fixed this now with:

```bash
docker buildx build --platform linux/amd64,linux/arm64 -f Dockerfile.datasurface -t datasurface/datasurface:latest --push .
```

### Docker desktop limitations

I could not get docker desktop kubernetes to work with my NFS stuff today. I ended up testing it on my Dell running Ubuntu 24 and full k8s.

### Docker back off issues

The infrastructure_dag.py.j2 uses pull ways for its image policy. This needs to be changed to pull if not present.

## 2025/08/04

Gave up on NFS for the git clone cache shared PVs. It seems to be a kubernetes/NFS issue that hasn't been fixed. I switched to longhorn on my remote kubernetes cluster and it seems to just work.

Container image pulls are an issue.

With longhorn as the default storage provider, postgres wouldn't start because the data volume has a lost+found in it and it requires an empty folder or it wont start. I modified the component to put the database in the data subfolder and that fixed it.

Need to modify the psp to have a storage class name for postgres/ dags/logs etc.

I'm a little inconsistent with the secret stuff in infrastructure_dag.py.j2. Sometimes I use the secret name as a template variable and sometimes I use the context to get the secret name.

### Longhorn issues

It's working well now but longhorn caused some issues. First, I only gave my kubernetes cluster 32GB of disk space. Longhorn wants to reserve capacity for all its volumes and this exceeded 32GB. I added more disk space, enlarged the file system to 100GB and that fixed that. Then I found the default replica count on longhorn volumes was 3. I only have 1 node in the cluster so that caused issues also. I set the replica count to 1 and that fixed that.

### Job sizing parameterization

I need a way to specify how much capacity to limit a transformer or ingestion job to. I can have a system wide default. Normal ingestions are just python jobs running SQL within the database so they are not resource intensive. DataTransformers can be more resource intensive. Either way, we need a way to specify the resource limits for an ingestion or transformer job. Job settings are basically:

```yaml
          resources:
            requests:
              memory: "2Gi"
              cpu: "500m"
            limits:
              memory: "4Gi"
              cpu: "1000m"
```

These 4 values need to be configurable per ingestion or transformer job.

Airflow web and scheduler need the same as well as Postgres. Postgres also needs a storage size which was already there.

handle model merge needs to write sizing requests to the database for each ingestion and transformer job.

### DataTransformer DAG overlap with DT INGESTION DAG

This is an issue. These need to run sequentially and never overlap. This seems impossible to do with 2 seperate DAGs. This is complicated because I wrote the ingestion to be non blocking airflow. So, ingestion and MERGE can run seperately. A single batch thus runs as 2 DAG runs, the DAG first does ingestion and then it does MERGE. The batch state is checked in the job to see what needs to happen.

This seems too complex for airflow. I need to make the ingestion job run till its finished.

I will also provide a way to allow resource overrides on a stream by stream and transformer by transformer basis.

## 2025/08/05

I implemented a general job hinting system for ingestion and transformer jobs. It's working now.

### Longhorn delaying container starts

I see jobs in airflow sometimes running in 10-15 seconds and other times taking 40-60 seconds. The difference is how long longhorn takes to mount the git clone cache volume. It seems related to longhorn's readiness probe mistakenly killing the share manager. I see the share manager pod restarting every minute of so and this delays the container start when the timing is right. Things work and are stable but, the inconsistency in container times is frustrating. The issue appears to be that the storage manager checks after 3 seconds for a PID file and if its not there then it kills the share manager and restarts it. The NFS server used in the share manager is a bit slow to start up and needs 7-10 seconds. This 3 second initialDelay time is hard coded in longhorn amazingly. So, I may be stuck with it. It's not a stability thing, the system works with no errors. The share manager doesn't fail and break pods with existing mounts. But, while it's restarting, new pods are delayed for 20-30 seconds. This isn't a Yellow issue in general, it's a longhorn issue. It'll be interesting to see on Azure etc, what the vloume mound delays are if any.

### Current testing environments

I'm currently testing datasurface on Ubuntu 24 with Kubernetes 1.33 and Longhorn 1.9.1 for my PVC storage manager (Postgres, Airflow, git clone cache). I'm running kubernetes on an x86 virtual machine with 32GB of RAM and 100GB of disk space and 8 cores.I don't expect longhorn to work on docker desktop on my mac. I can run git clone cache disabled if I need to run on my macbook.

### Integrate DataTransformer DAG with the output ingestion DAG as a task.

This is the next task. Fix ingestion/merge jobs in general to so ingestion (copy to staging) and merge in a single task and then do this for data transformers as a task rather than a seperate dependant DAG like now.

### YellowAzure thoughts

I'm thinking of creating the first cloud version. Azure has managed kubernetes AKS, cosmos DB for postgres and a managed airflow also. Secret management needs to change to use Azure key vault. It would be a good first cloud version and allow refactoring of Yellow to be more friendly to cloud implementations. There all look easy to do, AWS/GCP and Azure.

### Added image pull policy to the PSP

I added an image pull policy to the PSP. It's working now. Most important for the airflow kubernetes jobs. They were always pulling the image even if it was already present. Changed the default to IfNotPresent.

## 2025/08/06

Working on a large example ecosystem model. I'm using Microsofts WWI (World Wide Imports) example. This also lead to adding support for Geography types in the model.

### Spent the day trying to move up with Airflow 3.0.3

and it as a huge time suck. The kubernetes pod operator doesn't work as far as I can tell as at least I couldn't get it to work with claude 4 helping, it could not create the containers. Thats 8 hours of my time. I'm going to stick with 2.8.1 for now.

### Versioning assemblies

Even if I had managed to get airflow 3.0.3 working I made a mistake in that I didn't keep the original 2.8.1 version also. I did some refactoring just now to preserve 281 for next time. I'd add an airflow 303 folder, assembly and new infrastructure_dag.py.j2 and psp_airflow.yaml.j2 files. That way I can keep both. It's 2 variants of Yellow in a way, Yellow 281 and Yellow 303. It makes me wonder about the cloud vendor versions of yellow which will use hosted airflows and I'm expecting similar issues to today. Good experience, worst case. The AI claude doesn't do as well with airflow 3.0.3 as it does with 2.8.1 also...

### Tomorrow

* I guess I need to consolidate copy to staging and merge in to a single task and change the transformer DAG run do dt ingestion as a task within its DAG.
* Ingest from a YellowForensic style merge table. This is a key feature to limit fanout on a source database. A single DataPlatform instance can be designated as primary ingestor and then other DataPlatform instances can ingest from that platforms merge tables. This is key for scaling and for federation between private/cloud as well as cloud/cloud.
* Test WWI model as a next step up. 31 tables. I need a postgres with GIS enabled to use WWI, different docker image. New data simulator for WWI. It's still a single ingestion stream with 31 tables inside it.
* Next step up would be 100 WWI datastores. Now, we are at 100 ingestion streams, each with 31 tables inside. At 1 minute intervals, thats just over one ingestion per second. They currently take 40 seconds to run each. How many can my setup run concurrently is a question.
* Next step up would be 1000 WWI datastores. Now, we are at 1000 ingestion streams, each with 31 tables inside.

## 2025/08/07

Today, primary ingestion tagging. This allows a central/infra team to designate which DataPlatforms are allowed to ingest data from a Datasource. Other data platforms needing that data cannot ingest it directly from the data store. They need to get it from the primary ingestion platform. Typically, a primary ingestion platform will use forensic milestoning only. This makes ingestion much more efficient. This allows the ingestion load on a source datacontainer to be limited to a small number of data platforms.

Pondering whether I need to better seperate the model from infrastructure modelling. I starting this with the dsg platform assignment stuff, I think I need to do more of this. I don't want people using the system (producers/consumers) worrying about infrastructure complexity, thats the whole point of this. It's not their problem.

The PrimaryIngestionPlatform has a few uses.

* Limit fanout on a source datastore to a limited number of data platforms.
* Efficient ingestion regardless of source capture metadata, it's always a milestoned merge table in the PIP.
* A DataPlatform can choose a PIP it understands and ingest from that.
* Cloud scenarios. An onsite data platform can designed a datacontainer, a postgres, in the DMZ potentially as a PIP for stores needed on the cloud dataplatform. All cloud ingestions come from the data container in the DMZ. The link can go down and then when it comes back, the cloud can take over. Also when pulling data onsite from the cloud, all changes go through an onsite PIP. This limits outbound traffic from the cloud to onsite, saving money.

## 2025/08/08

With the PIP stuff in place, I'll modify YellowStarter for the Forensic yellow to be the PIP. This will cause the yellow live to ingest from the forensic yellow merge tables. What this means is the yellow live will reflect the actual live records present in forensic. This requires an initial ingestion which takes the live records as of the current batch. Then, it pulls the changes for subsequent batches and reflects that reality in the live yellow. The batch id's will match on both sides. The batch counter will be the same on both sides.

## 2025/08/09

Spent time improving the way view names are calculated and consolidated that code in YellowDatasetUtilities.

### Airflow prioritization

I already had the concept of Workload priorities in the ecosystem model. I simplified this today and the airflow DAGs now use it to prioritize DAG/tasks according to the model priority. Major feature enabled now.

I switched from looping over Workspace or storestoingest in the populateDAG or transformer methods to looping over the graph node looking for the right node types. This lets me use the node priority as a new field in the DAG/transformer context jsons in the database tables. The infrastructure dag template picks that out now for the task priority.

### PIP progress

Working towards implementing SQLMergeIngestion. This is tomorrows work though. The main thoughts are do I add an IUD flag to the live and forensic staging tables. For a full snapshot, I can just set IUD to "I".

## 2025/08/10

Working in Dataplatform.md. It's a little dated as this point.

## 2025/08/11

Just making sure where I am is working. Finding issues linting now with latest changes. Problems principally come down to DataPlatforms over linting. Yellow was using graph.workspaces a lot which is wrong sometimes. Whats assigned to a DataPlatform is a DSG, not a Workspace. There are DSGs in a Workspace which are not assigned to a DataPlatform. The Yellow forensic was liniting a DSG which was LIVE and vice versa. I went over the code looking for issues like this and corrected them.

createGraphg may be called too many times. Once during lintandhydrate and again after platform assignment which is a waste of time.

## 2025/08/13

Refactoring yellow ingest/merge. The ingest/merge job is now single cycle, i.e. ingest/merge run if possible in the same job. This helps giving the volume bind latencies I'm seeing with kubernetes/longhorn and expect to continue to see even on cloud setups. This also allows me to integrate the output ingestion job as a task in the datatransformer DAG. This reduces the DAG count but more importantly, the ingestion job now cannot run in parallel with the datatransformer job, they would always be sequential which avoids potential race conditions/corruption.

### Linting speed

Something to think about is right now in Yellow, the model is loaded for every job that runs. I can load the model quickly as linting is redundant as this was already done when the model was committed to git. I will likely need to add a way to load models without linting, that will be very fast. I can built tests to verify this. test_large_models.py verifies this. A model with 1000 Datastores, 2000 Workspaces and 40 tables per datastore (40k datasets) takes 4 seconds to load with linting. I don't think this is a problem at all in the overall scheme of things for now.

### Back to supporting ingest from a merge table

## 2025/08/15

I have merge remote live working now with tests. I used claude 4 sonnet and gpt-5 for this one and gpt-5 wasn't bad, I ended the day still using it. It's half the price of claude 4 so it's a good deal. Tomorrow, I will add the merge remote forensic tests and get that working. Then, I'll deploy a test to the full running system and verify it there also. It's 12:30am so I guess thats tomorrow, the remote forensic is working now with tests also.

## 2025/08/16

It's all working again on my single node kubernetes cluster. I will do a test over the weekend to test the priamry ingestion stuff. I also want to do a scale test just to see what happens on a single node setup. I'm documenting Yellow in terms of SDC (Slowly changing dimensions) also. Yellow technically supports SCD1 and SCD2. SCD1 is live records only, a mirror of the source table. SCD2 is fully milestoned so all observed versions of records for the primary key are kept with a batch in and out milestone. One batch out sentinel value marks live records. I added a lot of tests to make sure the remote merge SCD1 and SCD2 are working correctly. I need to work on additional ingestion types such as CDC streams and so on also. These are easier now after the remote SCD1 and SCD2 work. I used GPT5 in max mode for a lot of the remote merge work and it worked well, it's just slow. I also made changes so that job failures in airflow actually caused airflow job failures by using the correct exit codes.

Refactoring creating assemblies for Yellow out of Yellow PSP. I need to do this so I can make other assemblies for different similar stacks such as AWS, Azure and GCP. Also scenarios where the databases are provided outside the platform, i.e. preprovisioned. The postgres database we have been using was provisioned by us and running on longhorn for storage. Clearly, not the most performant database.

The external assembly stuff is running locally now so it's working.

### airflow template issues

Doesn't use afhostpair for name and port

## 2025/08/17

I created a new virtual machine on the Dell today (outside kubernetes) and then make a new assembly to allow a YellowDataPlatform to be created using an external database for the system. The merge database and the airflow database both run on the external database. This is a much higher performance setup than using longhorn PVCs for storage in the normal assembly.

This is a step towards running this on the cloud. A cloud provider will have a managed database, kubernetes and possibly a managed airflow also. The assembly breakouts in assembly.py are a good start down this path. The infra config needed to be broken out of the YellowPSP.

It's working. I hit kubernetes DNS issues. I'm using tailscale as a private VPN between my virtual machines including kubernetes. The kubernetes pods couldn't see the tailscale hostnames. I tried claude 4 sonnet in max mode but it couldn't fix it. I tried GPT5 and that figured it out.

It's documented at the end of the (HOWTO)[docs/yellow_dp/HOWTO_Setup_YellowDataPlatform_Environment.md] file. Nothing to do with datasurface, just my own kubernetes setup.

## 2025/08/18

Keeping the staging records for batches is a good idea. From an audit point of view, having the input data helps when checking whether there are merge issues later on. If you don't have the staging data then proving the merges are valid is a lot harder. They can always be pruned daily or weekly. Just looking at the postgres performance statistics this morning.

The code was dropping tables to change primary keys which was not good so that code was modified to modify the primary key constraints instead. This is a lot more efficient and doesn't require dropping and recreating the tables.

Ok, this morning round of testing is finished. It's now possible to override batch sizes for merge and ingest to staging. The default was raised from 1k to 50k records per chunk. Actually batchSize is the wrong name, should be chunkSize to avoid confusion with batches. Staging tables now retain the staging records for each batch. When a batch resets, the staging records for that batch are deleted only.

The datasimulator had an issue because I bounced the database and it didn't recover, it just looped saying the connection was closed. This was fixed.

### SQL Ingestion modes to add

#### CDC Style ingestion

The remote forensic ingestion is the bones of this. It's basically reading the changes from a CDC feed and writing them to the staging table with IUD flags. A concrete implementation of this would be a debezium style example.

#### Parameterized single field pulls

A snapshot followed by a delta which uses a timestamp column to pull changes since the last batch/snapshot. A column could be a timestamp column or a sequence number column which changes on insert and update commands. It won't capture deletes as the record is deleted unless it's marked with somekind of tombstone flag which can be reversed on a future reinsert of that key.

The column value can be stored as a nbullable value in the batchState. It can pull the whole table when null and then we update the batchState with the max value of the column. The next batch will use the previous batch job state. It will have the previous max value of the column. It can get the current max value of the column. We then pull all records >= the previous max to the current max. The current max goes to the batchstate for this batch. We may copy some duplicate records but thats necessary as some records may have been inserted/modified with the previous max value after we grabbed them before. For example, suppose the column was a second based timestamp, it can be easily seen in this case than we grabbed records at X.25 seconds and then more records were inserted/modified at more fractions until the clock rolled over the X+1 seconds. It ensures there are no gaps.

There will be a lot of these types of situations. A firm with a lot history of production systems may have many types of ingestion which must be supported. The easiest way to think of this is a pluggable system which provides the batchState and access to write the records to staging with IUD column attached. This is pretty open ended and allows ingestion to be extended to support any type of ingestion. This is the front half of the ingest/merge process. The back half is either a live or forensic merge from staging to the merge tables.

### Cloud port

Studying which cloud provider to do next. Azure is a candidate. It has a managed postgres offering which will work out of the box that offers up to 32TB of storage which is ok but not that much really. It has the usual high availability/replication so thats useful. It's managed airflow seems a problem. I think I'd be better off using a self managed airflow on Azure kubernetes. I'd have some

### Staging data pruning

I added a new hint parameter for how many batches to keep in staging. There is a default of 50 of so. This can be overridden on a per ingestion stream basis and set globally for a given YellowDataPlatform. If you use datasurface for regulatory purposes, I recommend seting this to -1 on each YellowDataPlatform. This is the kind of thing that could be enforced by a policy on the model, for example, a GovernanceZone policy.

## 2025/08/19

### Medallion architecture with DataSurface

Datasurface supports a medallion architecture. We can make 3 GovernanceZones, one of each of the medallions. Lets do the standard ones, gold, silver and bronze. Raw data is ingested to the bronze GZ. DataTransformers in the silver GZ can take bronze data and transform it into silver data. Thus, the silver GZ would have DataStores at the silver level of quality. Finally, the gold GZ can have datatransformers which take silver data and transform it into gold data. Thus, the gold GZ would have DataStores at the gold level of quality. Different data platforms can be used for each of the medallions. Consumers can create Workspaces which consume different medallions of data in the ecosystem. Policies can be used to restrict consumers from consuming data from the wrong medallion.

### Different forensic merge variants prototyped

I created a new merge base class which merges data from an IUD staging table in to the merge table. It's designed to be subclassed with an implementation class for various source targets, where to get the delta from. I implemented 2 of these. One grabs records from a CDC style table which can be described through parameters and the other reads from a table which is the sink of debezium style Kafka CDC streams, one per table.

### What next

I'm in a good place right now. Building out Yellow has helped me evolve the datasurface DataPlatform relationship. I have a production ready Yellow DataPlatform which granted just does snapshot SQL and PIP ingestion for now but it's extensible to what ever is needed quickly. The datasurface-core seems reasonably stable right now. I have Yellow and may now create Yellow variants for Azure, GCP or AWS. Most of the solutions being offered by cloud vendors are SDKs at best. Customer need to do a lot of work to productionize them and thats where DataSurface can help. Once a DataPlatform is created then standing up even complex applications is very straightforward. Moving that application to a different stack is equally straightforward.

## 2025/08/20

I am installing SQLServer2022 in a VM today. I will do a SQLServer2022 Yellow variant. I want to get an Azure Yellow variant working on Azure this week. I can save some money getting the SQLServer variant working locally versus doing it on Azure. I am also looking at how to implement tamper-proofing Yellow on an Azure environment. This is for HIPPA compliance.

I will modify Yellow to support different SQL plugins depending on the merge data container type.

## 2025/08/21

Started working on generalizing Yellow for different databases. Worked today on SQLServer suppport in Yellow. Mostly around removing postgres'ness from the code and making getting a database connection more generic. Next step is SQL modifications to support SQLServer for Yellow Merge, its SQL, T-Sql is not compatible with postgres SQL.

## 2025/08/23

Big check in today. I've done a lot of work generalizing Yellow to support different databases. Postgres and SQLServer to start. It working fully again with Postgres and all the tests are passing for both. But, I'm still trying to figure out how to get Airflow working with SQLServer. I may try to get it to work with postgres for Airflow and SQLServer for yellow. That might be a path. SQLServer seems a lot slower than postgres, especially for DDL operations. I've been optimizing the DDL operations which has helped but I think I can still do more.

### Timestamp issues and SQLAlchemy

SQLAlchemy isn't as good as I thought. They have done a poor job around temporal types like Timestamp. Timestamp is a binary row version in SQLServer and not a datetime type. They don't seem to have a portable type which maps to a datetime type in both databases. I added some dialect specific kludging to work around this but it's disappointing. Hopefully, I can find a better solution.

### Reconciling model schemas with staging and merge tables

I was doing this every job run but SQLServer is so slow that it's an issue. So, I stored the model dataset schema hashes in the BatchState. The code now checks if the current model hash is different from the last stored hash in the BatchState. It it is then we do the schema reconciliation. The current model hash is always stored in the BatchState for the current batch. This avoids the overhead most of the time as the schemas won't change that often in practice. I also cached engines, inspectors for SQLServer again.

### Airflow -> 2.11.0

I am looking at why airflow gave problems with SQLServer and it seems it's marginally supported first of all. The support which exists requires a couple of packages to be pip installed and require at least Airflow 2.10.0 and I was using 2.8.1. So, I upgraded to 2.11.0 which went very smoothly. I'll need a SQLServer specific docker image layered on top of the base airflow 2.11.0 image with the additional packages installed.

I should also probably try to upgrade to 3.0.x again. No point in staying on 2.x.

### HOWTO Setup YellowDataPlatform Environment

Making improvements in this to keep it current. Claude is able to do the right thing even when the HOWTO has some issues. I updated it to be more correct to make it easier for claude to do the right thing.

## 2025/08/24

Thinking about CQRS support. CQRS is an overlooked feature needed on any warehouse type of system. The reason it's so important is that there is no such thing as a perfect database which works for all use cases. There are always tradeoffs. Example, I can use postgres as my main storage. Microsoft offers up to 32Tb Postgres instances. What if you need more? What if you need more CPU than a single instance. Similarly, we could use snowflake/RedShift or BigQuery for our main storage. We have scalable CPU and storage now, we can likely get efficient ingestion/merge on these systems. But, what if our clients want milli-second responses for some queries? What if our clients need a different clustering strategy, what if different clients need different clustering/indexing strategies? These will all break a single storage or non CQRS system. A CQRS system allows us to have a primary storage system which is optimized for ingestion/merge and then have different engines for consumers depending on their needs.


### Postgres/Postgres CQRS

We have a postgres primary storage system with just the indexes for ingestion and merge. We can have a separate tier of postgres databases on top where consumer Workspaces are placed. We can have 5 such postgres databases. We can assign 20% of our Workspaces to each database. As we grow, we can add another postgres to this tier and migrate Workspaces from the existing 5 to the 6th one and so on. We can add consumer-specific indexes to their postgres databases without slowing down the primary storage system, which keeps only the ingestion/merge indexes.

### Columnar primary → row-store consumers

We can use a columnar warehouse like Snowflake/Redshift/BigQuery or an Iceberg-based lakehouse for our primary storage system and optimize it for ingestion/merge. We can have a separate tier of Postgres or SQL Server (row-store/OLTP) databases for our consumers that are optimized for their queries. We can scale both primary and consumer tiers independently.

### PIP work is the basis

We already implemented PIP for allowing a DataPlatform to ingest from a different DataPlatform with either a LIVE or SCD2 source image. LIVE allows us to capture inserts/updates in the target DataPlatform, SCD2 allows us to mirror the source DataPlatform. We can use the same code to implement replication from the primary storage system to the consumer tiers. I call this task export. It's very easy to implement export services from the primary to the consumer tier using the remote merge code. This would work like this.

* A PSP can have a set of consumer tier DataContainers configured. This can be done in the model. It changes slowly, this has storage, it's not stateless. Bringing another consumer DataContainer online can take hours while it's hydrated with the necessary data.
* Workspaces can be assigned to one or more consumer tier DataContainers. This is again done in the model.
* We need to export the union of all Workspace DatasetSinks assigned to a consumer DataContainer to that DataContainer. We need Workspace level views on the consumer DataContainer also.

Export here means a remote merge from the primary storage system to the consumer DataContainer. We already implement this for PIP.

Exports are asynchronous and idempotent; batch IDs are propagated to consumer tiers to preserve lineage.

So, we are very close to implementing CQRS, the gap analysis is small:

* PSP have consumer DataContainers.
* Workspaces are assigned to a set of Consumer DataContainers for the DataPlatforms assigned to the Workspace.

### CQRS adds latency

Yes it does, but look at the benefits. Running a workload on a cloud database that it's not optimized for can get very expensive. Running it on a database designed for the workload will be cheaper. This extra latency is usually measured in a minute or two. Not a factor for the majority of use cases. Your upstream sources may demonstrate bigger data availability than this.

### Security

Nothing changes here. The consumers access their Workspace data through views defined on the consumer DataContainers, and these views continue to enforce Workspace policies to prevent cross-workspace data leakage.

### Scalability

Adding a DataContainer is a much lower load on the primary storage system than adding consumer workloads to the primary storage system. You can move the consumer workload to the most cost effective location which may even be on site...

### Batch id propagation and so on

The consumer DataContainers would be exact replicas of the primary storage system subset assigned to them because export is basically the SCD2 ingest/merge. This also means we can add a new consumer DataContainer and it will be hydrated first with a seed with the current state and then it will run ahead to catch up to the current batch.

### Balancing the Workspaces across the consumer DataContainer fleet

This can be done by adding the "target" DataContainer to the Workspaces to migrate. Then wait for the hydration to complete and then remove the original consumer DataContainer from the Workspace assignment. We can run a garbage collection job to remove all datasets not needed anymore from the consumer data containers. My own direct experience with this is a fleet of over 100 Sybase IQ databases. Each database was a cluster of 8 machines with 24 cores and 384GB of RAM each. Cluster storage was a shared 20TB disk. There were maybe 30k datasets being exported to each database. There were over 2000 Workspaces assigned to this fleet. There is a lot of commonality in datasets across the fleet, mostly reference data which is used by most Workspaces. High commonality of reference datasets suggests deduplicating export work and prioritizing shared datasets to minimize redundant writes.

### Pruning Consumer DataContainers

It may be necessary to prune consumer DataContainers. This means pruning historical data for datasets according to retention. We might decide to keep only the last 60 days of data for each dataset, for example. This may not make a difference early on, but in 3 or 4 years it will for fast‑changing datasets. The primary retains full history; consumer pruning does not affect the source of truth. Targeted backfill can rehydrate pruned ranges into consumer containers from the primary when needed. Retention should be governed by policy (for example, at the GovernanceZone or Workspace level).

### Workspace indexing

We will use hints on Workspaces to provide indexing hints for the ops team to use. These can influence which Workspaces are placed on the same DataContainers. We want to place consumer Workspaces with similar workloads together. This is most efficient.

### High availability

Consumer Workspaces should be placed on at least 2 DataContainers. This can also be done on the cloud using replication. The backup can be turned off until used and then consumers switch to it. When the primary container starts up then once it catches up, consumers can be assigned to it. Some kind of consumer facing proxy is needed to route SQL requests to the correct DataContainer.

### Export job scheduling

This is a tricky one. I have seen databases get overloaded by too many concurrent DDL operations or even write transactions, especially columnar databases that you all know and love. Limiting concurrency is a good idea and may even trigger more scale out before the existing compute is saturated. It’s surprising to people that this can be a bigger limit than CPU/memory on the query clusters. So, we need a way to limit the concurrency of export jobs per DataContainer. This also needs prioritization. We can't have low priority exports starving out high priority exports for consumers. This, of course, also means that high priority exports can and likely WILL starve out low priority exports for low priority consumers. Assigning the right kinds of consumers together on a DataContainer can prevent these issues.

Export scheduling is asynchronous and should implement starvation protection (for example, aging) so low‑priority jobs eventually run. Separate DDL from DML: cap concurrent DDL per DataContainer (often 0–1) and prefer maintenance windows for heavy index operations.

Actionable guidance:

- Per‑DataContainer concurrency caps with priorities: use a priority queue with weighted‑fair sharing, and apply aging to avoid starvation.
- DDL/DML lanes: strict limits (or windows) for DDL; back off on lock contention and long commit latencies.
- Adaptive throttling: token‑bucket per container with dynamic refill based on CPU, commit latency, and warehouse queue depth.
- Priority alignment: map export priorities to existing Workspace “Workload priorities” so scheduling is consistent across Airflow and exports.

### Performance monitoring

Besides consumer query monitoring, we need to monitor the export jobs and the current lag across all datasets on a container or even Workspace basis. There are 2 latencies here. One is source to primary latency and the other is primary to consumer latency. Measuring this in terms of batch delays is one way and we can even assign timestamps (batch end time) to these to get a time delta. Track both batch distance and wall-clock lag; also monitor commit latency, DDL queue length, active export jobs, and retry/backoff rates.

### Moving PSP and its objects to a new GitControlled object

I'm considering moving the PSP and its objects to a new GitControlled object. It seems like a good thing to carve these out, possibly on a PSP basis. I imagine a local one, an Azure one and an AWS one, is the same team managing all of them? Maybe. They still could but it would be nice to be able to seperate them in to different git repositories from a control plane point of view.

### Need more component oriented merging

I'm about to start implementing watermark SQL merge support. This will be a Job but I need versions of it for live and forensic merge. I'm playing with making those methods static on the other jobs so they are callable from other jobs. Lets see.

## 2025/08/25

Finishing watermark ingestion. A lot of refactoring around making this easier and the same with tests.

SQL Server database_operations is using bracket quoting for columns now rather than quotes which depend on settings to work correctly. Brackets for column names are always safe.

Investigating other databases. I'm looking at Oracle and DB2 which I can run in docker locally. Snowflake, Redshift and Azure Synapse are cloud only but easy to stand up for running a suite of tests against when needed. Postgres, SQLServer, Oracle and DB2 could be run always as they are just docker images.

## 2025/08/26

Oracle and DB2 installed in VMs. Snowflake work started.

Oracle working now. Lot of work though because it's very picky about how column names are quoted and case sensitivity. So, right now thats Postgres, SQLServer and Oracle working. DB2 is next.

I think the work today with Oracle was necessary and should make more database support easier.

## 2025/08/27

I'll get DB2 working next. Oracle took a few hours yesterday because of the column name quoting issues and case sensitivity. Also, SQLAlchemy is again letting me down. Dates are not consistently handled, tests which worked with postgres and SQLServer were failing with Oracle because it mixes datetime with dates.

It might be worth repeating how Datasuface abstracts the database away from the model. First, each DataContainer has a naming mapper. This is code which handling name length, name case, how to quoting names, mangling the name when it's too long. Database specific types are handled in sqlalchemyutils.py. There are method there which return a SQLAlchemy table for a DDLTable (model side schema). This handles column type conversion. There is also a method to do the reverse, create a DDLTable from a SQLAlchemy table.

The Yellow platform builds on this and implements the various ingestion and merge algorithms. These use the database_operations and an associated factory. The factory picks the right database_operations for the database type. A Yellow Job would have 2 of these, one for the source, one for the merge side. You can also think of this in terms of the job reads from the source side and writes to the merge side. Yellow jobs also need 2 naming mappers, one for the source datacontainer and one for the merge datacontainer.

The various high level SQL operations needed for ingestion and merge are implemented in the database operations class. This gives a lot of common code and allows fast database ports to be done. It's also interesting in terms of how standard SQL is. Not very is the answer.

Database connection differences are handled in the db_utils.py file. This creates a SQLAlchemy engine for a given Database container.

Adding DB2 support means, using an appropriate naming mapper on the DB2Database data container. Making sure the Table to DDLTable conversion is done correctly. Adding DB2 support to db2_utils.py. Finally, create a DB2 database operations which does the same thing as the others. I'm finding AI's are good here. I have 3 other versions of this and the task is to make a DB2 equivalent version. I have the tests (@test_MergeRemote.py) which has a standard set of tests I can use to check every database.

Tests for a specific database can be skipped using an annotation. Typically, you need to define an env var called "ENABLE_ORACLE" or "ENABLE_SQLSERVER" and set it to true to run the tests. The default is just run postgres. SQLServer is very slow during tests compared with the others.

To enable DB2 testing, the tests use the pip_test_model folder eco.py. I add a set of Datastores, and DataPlatforms using a DB2 merge datacontainer.

**Key Issues Resolved:**
1. **MERGE Statement Compatibility**: DB2's MERGE statement is stricter about duplicate keys than other databases. Fixed by replacing MERGE with UPDATE + subquery in `get_remote_forensic_update_closed_sql()`.

2. **NULL Constraint Violations**: DB2 doesn't allow NULL values in unique constraint columns. Fixed by ensuring hash columns (`ds_surf_key_hash`, `ds_surf_all_hash`) and other critical columns are defined as NOT NULL.

3. **Hash Column Width**: Made hash column widths database-dependent to accommodate DB2's specific requirements.

**Test Results (Final):**

That took about an hour. The main issue is DDL operations are very slow. I saw this also with SQLServer. Postgres spoiled me, I was doing DDL operations to make sure table schemas matched the model when the merge jobs ran. I did some optimizations for SQLServer. Ok, implementing SQLServer, Oracle and DB2 was useful. I fixed a lot of portability issues with the code. Every database show a different set of issues and I think the code got better for it.

### Snowflake work starting

Implementing merge support for snowflake today. First challenge was YellowDataPlatform worked with HostPortSQLDatabase which SnowFlakeDatabase is not. The current code was extracting the post and port and using them generate the templates. I need some rework to allow hostport and snowflake style connection info.

I'm using a trial account on snowflake. I have not created my own database or warehouse yet. I'm using the SNOWFLAKE_LEARNING_DB/PUBLIC for now with whatever warehouse they provide out of the box.

DDL operations are not so bad on snowflake, compared with DB2. But, on the test Snowflake instance I have, normal SQL operations are very slow compared with the others, 200-400ms per SQL statement. Snowflake's console shows the last queries and how long they took on the cloud side so it's not network latency or something.

I think on snowflake, I'll need to use stored procedures or something, performance through pyodbc is a big issue it seems. I tried a medium warehouse instance and no difference.

## 2025/08/28

I have SQLServer, Oracle, Snowflake and DB2 working locally with my project tests. I'm now looking at doing a system test with each. So, stand up a full system with Airflow working with Postgres and the merge database being SQLServer/Oracle and DB2. My snowflake implementation is not working well, it's very slow. A SQL Server test takes 6 seconds, Snowflake is over 2 minutes for example.

I built and added to the project a datasurface airflow docker image. This is standard airflow 2.11.0 with the various database drivers added. I'll be testing with this from now on, the Airflow assembly is using this image now. I also changed the naming of Airflow in the code to Airflow 2X rather than Airflow281 which is confusing when we're using 2.11.0 right now.

With the new composite airflow image, I can now test with each database as merge database. I'm going to do this next.
