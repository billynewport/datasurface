# September 2025

## 2025/09/01

Working on getting it working on AWS today using EKS. Pointed my local kubectl to the EKS cluster and doing the bring using local shells.

Having trouble with far gate. Switching to EC2. Problems are mainly with efs-sc. So far, experience with AWS is poor. It's very slow. Deleting clusters and so on takes over 20 minutes. I don't know if this is normal.

Frustrating day. AWS is crazy slow at provisioning or changing resources. I spent the whole day trying to start an EKS cluster. It would come up and then I'd discover something wrong with it and then I needed to do it all over again. Very slow loop. I'm currently making an EKS cluster with m5.4xlarge nodes. I tried smaller nodes but the airflow pods need more than the small ones I was using and I will still need to run the datasurface jobs so the node group is up to 5 nodes, 2 desired. The last issue of the day was I hit a vcore quota after waiting nearly an hour for it to tell me. I've asked for a quota increase to 48 vcores but the day is gone now. It'll be Wed before I can continue. I'm using claude 4 sonnet in cursor to help with this but it's struggling with the yaml files.

### Security implementation

I'm thinking through an initial security implementation. I'll implement for postgres, local kubernetes and AWS to start. This is tomorrows work while I wait for the vcore quota to be approved. I need to make a new job called postgres_security_sync. This will be launched periodically on airflow and have access to the merge database. I'll need to have a db_security_operations class with different implementations for each database type at least depending on the database.

I would also like a transformer to load the eco model and write the major entities to a Datastore. The transformer will load the model, and write the ecosystem to its dataset, datastores, datasets, Workspaces, DataTransformers, Teams, GovernanceZones. This would be useful as its much more queryable than the Ecomodel in memory. The security module could itself be a transformer to create the transformer users and required read permissions and outputs for example. Another transformer would by the security sync job itself. We would need a way for transformers to run with an AWS IAM role so they can update secrets in AWS Secrets Manager for example.

I need to define a datasurface method to create the output Datastore for people to use in their models. Maybe, the whole DataTransformer is returned by the method to include in the model Workspace. Maybe the whole Workspace.

Use dataclasses and sqlalchemy to define the model.

## 2025/09/03

Adding in support for builtin system artifacts. There are Workspaces, DataTransformers and Datastores which are automatically added to Ecosystems by DataSurface. These includes Datastores which contain the major entities of the Ecosystem model. These Datastores are ingested to the system and are available to use
by devops and other DataTransformers. The ability to run SQL queries to find out about the model is a useful feature in general.

This also created work to allows different kinds of Code Artifacts to be supported in Yellow. I'm debating whether security should be implemented by a DataTransformer on the builtin datastores.

This also helps test self referencing DataTransformers. This is a DataTransformer whose Workspace contains DatasetSinks referencing the output DataStore of the DataTransformer.

## 2025/09/04

The model externalization is now working and a test of it is passing. 

### Returning to AWS

AWS just approved the vcore quote increase to 48 vcores so I'm back on that now.

Made progress, a few bugs fixed but I'm now stuck on secret management. I want to use AWS secret manager. This is a pain to use for the airflow kubernetes yaml stuff as EKS doesn't seem to understand secrets or provide a way to access them easily from yaml files.

I'm switching to using the airflow-aws secrets backend which simplifies things a lot. The infrastructure_dag.py.j2 is already using the secrets manager with boto so I don't need extra_creds and so on. The main thing airflow's yaml needs are the credentials for the airflow database. The airflow-service-account needs "*" access to datasurface/* secrets in AWS. All pods receive credentials by injecting environment variables into the pods. This limits the user code in these pods to just what they need for the job.

### Gemini 2.5 Flash for the win today

I normally use Claude 4 sonnet in cursor to help with everything but this time it suffered on the secrets with airflow etc. Google gemini 2.5 Flash one shotted the answer. Old k8s yaml template in and new AWS yaml template out. Maybe, I'll try gemini 2.5 tomorrow, it depends on how good it is with tools, thats where I see the benefit of claude 4 sonnet. I'll try gemini tomorrow and see if it can pick up using the AWS HOWTO.

## 2025/09/05

I started the day with Claude 4 but gave up after a few minutes when it could not delete unused AWS VPCs. I'll try Gemini 2.5 Pro max mode today. It's doing better so far. It deleted the VPCs quickly after finding the dependencies stopping it. I teardown the EKS clusters when I'm not actively using them like overnight as if I left my 2 node running it would be about $1.2k a month. Recreating it everyday takes about 20 minutes though. Today I hit the 5 VPC quota limit so I used Gemini to delete the unused VPC. I have a single VPC which my tailscale and Aurora Postgres database are located. Looks like the EKS clusters get their own VPC everytime.

With the AWS secrets stuff from yesterday, I think today I can get AWS working with Yellow. This is a good step as I plan on doing Azure next and I find the more I do, the easier it is to do the next one. So, if today is successful, I will have Yellow working with AWS and local Kubernetes as well as Postgres/SQL Server/Oracle/DB2. Technically it works with Snowflake but it was so slow, I wouldn't call it working. I think I can try their hybrid tables when I get back to working on Snowflake. Ducklake is looking interesting also as a columnar store that I can use locally or on cloud. I looked at duckdb a couple of weeks ago but didn't think that would be a good fit for this project because it only allowed a single writer or readers at the same time. A writer means no readers. If you are running continuous ingestion then this limitation is a deal breaker. Ducklake does not have this limitation.

AWS permissions are proving to be a pain right now, I went with a limited permissions approach but am wasting a lot of time adding permissions as I go. As I have likely mentioned before, creating and deleting AWS EKS clusters takes way too long. 20 mins to create and at least as long to delete. I am seeing failing because of s3 bucket names being too long or a permission is missing. They could really use some kind of lint check up front to catch these issues given how long it takes to fail and reset right now. For a stack as mature as AWS, I really don't udnerstand how they have these issues at this stage other than it's intentional. I am using an approach now where when I delete a broken cluster, I made a new one with a V1/v2/v3/v4 name to avoid the delay but the quota limits will still get me depending on how long deleting a cluster takes.

Making a little more progress today. EFS permissions problem is documented. Now, trying to get airflow to start and pickup secrets from the AWS secrets manager and the containers are hanging on start.

### Cursor update changes pylance

A cursor update switched me from pylance to basedpyright. It has picked up issues in the code based which I'm working through.



