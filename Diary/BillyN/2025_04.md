# April 2025

## 2025/04/01

Initial location: New Zealand.

Spent today working on finishing the json serialization for DataTypes. Created tests to make sure that all the types are serialized correctly. This makes the model server usable when I need it.

Largely using cursor in Agent/auto mode and it's working well. It mostly wrote the model server with some minor fixes. I'm noticing it's more aware of context and this is improving the tabbing. The cursor rules I added are also helping with generation.

## 2025/04/03

Working yesterday on security. Security and DataPlatform are independent components. We'll need an extensible framework for SecurityModules. We can provide a default implementation which works with the ZeroDataPlatform and postgres but would serve as a model for writing other SecurityModules.

I'm just about to get on a flight to Sydney from Queenstown. The next 2 weeks will be in Sydney for hopefully, I can get ZeroDataPlatform up and working. I need a better name than ZeroDataPlatform. This Zero doesn't work with Security, ZeroSecurityModule doesn't work for some reason...

Maybe, just rename ZeroDataPlatform to PostgresDataPlatform or SimpleDataPlatform.

The kafka CDC ingestion class needs to support mappings. It needs to allow a data producer to specify how the datasets in their datastore are exported to the Kafka topics and what the expected format is. Really, it's the other way around though. The DataPlatform will support the mappings defined in the ingestion classes. The producer must publish their data to the topics in a way that the supported mappings can be used.

For Kafka, debezium would be a standard mapping. Insert and update messages on a topic per dataset is also an option.

## 2025/04/05

Location: Sydney.

Thinking about adding MCP support to DataSurface's API. FastAPI has an MCP extension which allows you to add MCP support to a FastAPI app. I need to use pydantic to better document the API for it to work. My JSON objects are different than the DSL objects so I need a set of classes representing the response objects and then use them to generate the dicts for the API. I really don't like having seperate classes for JSON and the DSL objects but it is what it is.

## 2025/04/07

Pydantic is a dead end. The problem is we are going to have very large DSL objects. I have seen datastores with 500k datasets and a hundred columns per dataset. Imagine someone creating a table every hour to partition the data. Thats 24 * 365 tables per year or 9125 tables. Imagine a data schema with a single fact table and 10 dimensions. Now, each hour partition is 11 tables. It's easy to see how this explodes. You could say "thats a stupid design" and you'd be right but the team owning it is gone and it's not getting rewritten so you need to ingest it.

Currently, I'm using JSONable which simply adds a to_json method to the object. A large nested graph will to_json to a lot of nested dict objects. This is expensive to generate CPU wise and memory wise. It's not even json yet. Now, it needs to be converted to json and the json may stay in memory also. Thats the memory for the original graph, the dicts and now the json too. Imagine thousands of jobs running on a cluster all asking for these calls concurently. You can burn a lot of CPU and memory with this approach.

Basically, what I want to do is to serialize to the network as I convert the DSL objects to dicts. I need to use a Generator to do this. I only need the parts of the DSL that are being serialized right now, keeping it around during serialization is just a waste of memory. In my case, the dict is a means to an end, not the end itself.

If I used pydantic to do the json then it would be hugely inefficient and pydantic is not very good at complex class hierarchies either. This is an optimization that I will do later. What I have now is good enough to get the job done for now.

I still need to document all the API schemas using pydantic for fastapi mcp to work. But, I'm using cursor to generate the schemas and there is no point until I need to document it to generate them. I'm finding when using cursor AIs to generate large blocks of code, it's better to wait as cursor will not keep them up to date as I progress with the project automatically.

### SimpleDataPlatform

Pivoting back to this. What do I need to do to get it working?

* SimpleDataPlatform needs to support linting and validation of the datasurface model during the build process.
* MERGE time reconciliation means generating terraform to create the airflow DAGs.
* Node time reconcilation means creating the schemas in the postgres database.

I will implement merge time as a seperate from the git action. It will run on demand for now.

## 2025/04/11

Adding in Workspace priority. I don't understand why I didn't do this already but it's started now. I need to also add code to back propogate the priority to the nodes in the pipeline graph.

## 2025/04/12

Removing some redundancy in the PlatformPipelineGraph, I was tracking both exports and consumers when I can derive the exports from the consumers. This simplifies the code a little. The AI says it makes the code a little more DRY, Don't Repeat Yourself.

## 2025/04/14

Making sure priority is propagated correctly through the graph. Tried to use cursor AI to fix some bugs but even with gemini-2.5-pro MAX, it was unable
to fix the bugs. I used an AI generated priority propagation algorithm and it didn't work and it couldn't fix it. I hand coded a solution that works.

## 2024/04/20

Working through SimpleDataPlatform slowly. I will need to provide a persistent folder which is stable across calls to the DataPlatform to generate the resources for the pipeline graph. The SimpleDataPlatform will use terraform to manage the various kafka connectors to ingest the data from datasets to postgres staging tables. There could be hundreds or even thousands of connectors. Putting them all in a single terraform file is not practical for a variety of reasons. So, I will make a single terraform file per datastore. Potentially, even there are issues with that as I have seems datastores with over 100k datasets before. Terraform would likely blow up with that many connectors. So, I may need to have a terraform file per datastore slice where a slice is a 100 datasets or something.

This is good as it's allowing me to work through changing/standardizing the interface to the DataPlatform for these concepts. The DataPlatform will likely be wrapped in a docker container. One per DataPlatform. The container will be given local file system paths for credentials and the actual models it should use. Every time we invoke the
python code in that container, we will provide the various file system paths that it needs for that call.

## Merge time handler

The merge time handler needs to:

* Load the merged model from git.
* Load the model from the cloned filesystem.
* Workthrough all Workspaces and assign DataPlatforms based on needs to the Workspaces.
* Launch a docker container for each DataPlatform.
  * Provide the docker container with all credentials on a mount point.
  * Provide the docker container with a copy of the model on a mount point.
  * Call the processMergeEvent method of the DataPlatform with the appropriate arguments.

I think the merge handler will need to know previous Workspace to DataPlatform assignments. Switching DataPlatforms for a Workspace is an expensive event. It's something that would be done in a very deliberate manner in phases.

In my previous experience, migrating to a different DataPlatform was a major event. We could not afford mistakes, any mistake would be a "career limiting event" or worst case, even criminal charges. Fines around regulatory reporting mistakes/delays can be very large and persistent issues can result in trading licenses being revoked. Switching data platforms for a reporting system may require a migration of the existing data to the DataContainers for the new DataPlatform. Potentially having to reverse such a migration after several days or even hours would be very difficult. Rerunning a few days of processing on the old platform if a problem was discovered would be very time consuming delaying the publication of new reports which regulators may be waiting on. These kinds of mistakes can erode confidence that the bank/company has control of their systems and result in major issues.

Now, a lot of people may use DataSurface for devops, internal reports or similar use cases where the stakes of issues are really just an inconvience but in a regulated environment, the stakes are much, much higher.

This also highlights the risks of using cloud infrastructure for such systems. Cloud vendors will not ask their customers to approve or test when the cloud vendor is making changes to the platform. Cloud database vendors will also not ask. If a cloud vendor makes a version change which results in a subtle bug changing reporting results then the regulator will not come after the cloud vendor, they will come after the bank/company using the cloud vendor. This cannot be emphasized enough, it's your problem, not the vendor...

As a result, it would typically take 12-18 months to migrate all Workspaces to a new DataPlatform (2.5k Workspaces). We would move low priority Workspaces first, we could take snapshots of a subset of the production system. We then started maybe 100 batches back from the current live batch and remerged them using the new DataPlatform so we could compare the results against the production system. This was in additional to the normal junit tests for the DataPlatform.This was effectively a production parallel test. Certain teams with high risk reports running on the system would forbid us to make such a change until a time of their choosing when they were ready or had done additional testing. There were timing questions, we cannot change the DataPlatform until after a quarterly report is finished for example.When dealing with getting team approvals then its easy to take this much time to migrate. Unless the testing can be automated then this is the situation. The problem is the complexity of all the edge cases in data collection, processing and so on. All it takes is one mistake and the system breaks.

Where to store this state is a question. It can't be in git. If it was deterministic then it could be stored in git. If it's deterministic then a central team can updated it with a pull request to manage such migrations. Thats the answer. I was thinking of using an LLM to match Workspace requirements to the DataPlatforms available. This could be done when the central team is updating the metadata guiding the merge handler.

## 2025/04/21

I've started working on the merge handler. I'll be describing the design in the [MergeHandler.md](../docs/MergeHandler.md) file.

## 2025/04/24

Flying over the Pacific from Sydney to San Francisco.

I've developing the concept of BrokerRenderEngine. This sits between the model and the DataPlatforms. It is responsible for providing the compute, storage and CredentialStore infrastructure within which the DataPlatforms are executed. I'm fleshing it out by developing a Docker Swarm RenderEngine to start. This means the CredentialStore types supported depend on the RenderEngine so I removed the CredentialStores from DataPlatform. Instead, the RenderEngine will provide itself to the pipeline graph in a new method called renderPipeline. Previously, the lintGraph method was intended to both lint and execute the graph rendering. This thinking has changed.

The Credentials specified in the model need to be linted against the CredentialStore as well as the various DataContainers in terms of does the data container support that type of credential. For example, does a particular SQL database only support UserPassword credentials and therefore using a Certificate based one should fail lint.

The MergeHandler will basically load the Ecosystem and then use the renderEngine property of the Ecosystem to render the model via the various DataPlatform instances. This makes sense as the team who manage the overall Ecosystem would be the natural ones to specify the renderEngine to use. This assumes, an Ecosystem is managed by a single renderEngine which is false. I can see Ecosystems spanning local and cloud based renderEngines. This kind of leads to the renderEngines being owned by the Infrastructure Vendors rather than directly by the Ecosystem. This is also tricky because DataPlatforms themselves may also be local or cloud specific, I can easily see a DataPlatform working on AWS only for example. I need to figure out the federation aspects of an Ecosystem later. Lets get one federation member working and then flesh out a federation larger than one.

### Credentials

Revisiting Credential now with a view to the docker swarm work. In Swarm, every secret is a local file. Interpreting the file can give access keys, user/password pairs and certificates. I have FileSecretCredential and UserPassword and Certificate which kind of doesn't make sense now. It seems like UserPassword/Certificate need to be something that can use a credential to get it. An enum on Credential maybe describing what it's supposed to be.

When linting credentials on a store, wondering if linting a credential means the credential has been defined in the runtime? Does the credential exist in the storage and is the right type (merge time only)

It also seems there is no easy way to have an API like getAsCertificate on CredentialStore. Most python libraries want keys stored in files not in strings. There will also be non-python spark jobs for DataTransformers so APIs need to inject keys/secrets into the execution environments for those jobs.

It seems the trick for submitting spark jobs to a swarm based cluster would be to use submit time args with the keys.

### Docker container compose file for Merge Handler and SimpleDataPlatform

I'd like to add build steps and files so it can construct these 2 containers.

### Mapping to docker image names for DataPlatforms

I'm thinking DataPlatforms should be wrapped in their own docker container image. The image has a linux image, a python version, the requirements modules all staged. Such a container can then be run to execute a python command when needed by the MERGE handler. I think running them with airflow is likely best here. Airflow would then hopefully track each run attempt along with logs of what happened for operation staff. The MergeHandler could simply be one airflow job which itself launches other airflow jobs. These jobs can use either the PythonOperator for development or dockeroperator for production.

### No standards for credentials really

The whole credential injection space is a minefield. I could pass credentials to a spark job using command line args but those are visible in spark history server and anything that looks at jobs. I could use environment variables instead but I cannnot use certificates with that approach. I could use secrets to push the certificates in to the container file system but on a shared worker cluster then every job can see every certificate which is bad also.

I can get away with just supporting user/passwords for postgres, minio (access keys, same thing), and kafka but using mTLS etc is crazy difficult although bundles of public keys for verifying CAs don't seem risky at all compared with private keys.

All private keys could be mounted in the local filesystem and the private key password as an env variable, this is what I'll do for now. The toplevel container gets the password secret but injects it using a ENV variable to any containers/jobs which need it.

### Code Execution Environments

Reevaluating this stuff. The DataTransformers can specify a CodeExecutionEnvironment. This is a requirements document, "to
run my code, I need this support". Thus, it needs to be a SparkExecEnv for example or a flink one or a MR job on hadoop. The
render engine being used decides whether it supports such an env for DataTransformers to use. It also defines those envs
and provisions them.

I removed the Kubernetes Code Exec env because it makes no sense with this view point. I can't run a job on kubernetes but if the render engine was based on kubernetes and provisioned a spark cluster on it then it can run a spark job.

## 2025/04/25

Benchmarking how long it takes to lint a large ecosystem today. The bottleneck is the UserDSLObject which records the file and line number where the DSL objects were created. This is used to show the user where the object with issues was created. This is a problem. I can help with this by thinking of linting in two ways. When the users model changes are done locally or as a result of a commit in the repo then these messages are needed. When the model is loaded from a live repo, the model is always valid and thus we could disable the capture of the source location.

'''bash
cd src
python -m cProfile -o test_lint_performance.prof -m unittest tests/test_lint_performance.py
python -m pstats test_lint_performance.prof
'''

I'm realizing that even DataType are defined as UserDSLObject's. I'm wondering whether to leave them or switch them to InternalLintableObject's because of the performance hit. I worked with Gemini 2.5 pro today to find a quicker way to find the source location of the DSL objects but all its suggestions were slower. If I made DDLColumn and DataType not UserDSLObject then this would eliminate the vast majority of the UserDSLObject's and speed up linting.

I made DataType, DDLColumn and Documentable not capture source location and it doubled the speed of linting. These are the vast majority of the calls happening before. Now, it's just DDLTable, Datasets and bigger scoped/fewer objects. I think this is a good trade off for now.
