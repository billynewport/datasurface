# April 2025

## 2025/04/01

Initial location: New Zealand.

Spent today working on finishing the json serialization for DataTypes. Created tests to make sure that all the types are serialized correctly. This makes the model server usable when I need it.

Largely using cursor in Agent/auto mode and it's working well. It mostly wrote the model server with some minor fixes. I'm noticing it's more aware of context and this is improving the tabbing. The cursor rules I added are also helping with generation.

## 2025/04/03

Working yesterday on security. Security and DataPlatform are independent components. We'll need an extensible framework for SecurityModules. We can provide a default implementation which works with the ZeroDataPlatform and postgres but would serve as a model for writing other SecurityModules.

I'm just about to get on a flight to Sydney from Queenstown. The next 2 weeks will be in Sydney for hopefully, I can get ZeroDataPlatform up and working. I need a better name than ZeroDataPlatform. This Zero doesn't work with Security, ZeroSecurityModule doesn't work for some reason...

Maybe, just rename ZeroDataPlatform to PostgresDataPlatform or SimpleDataPlatform.

The kafka CDC ingestion class needs to support mappings. It needs to allow a data producer to specify how the datasets in their datastore are exported to the Kafka topics and what the expected format is. Really, it's the other way around though. The DataPlatform will support the mappings defined in the ingestion classes. The producer must publish their data to the topics in a way that the supported mappings can be used.

For Kafka, debezium would be a standard mapping. Insert and update messages on a topic per dataset is also an option.

## 2025/04/05

Thinking about adding MCP support to DataSurface's API. FastAPI has an MCP extension which allows you to add MCP support to a FastAPI app. I need to use pydantic to better document the API for it to work. My JSON objects are different than the DSL objects so I need a set of classes representing the response objects and then use them to generate the dicts for the API. I really don't like having seperate classes for JSON and the DSL objects but it is what it is.

## 2025/04/07

Pydantic is a dead end. The problem is we are doing to have very large DSL objects. I have seen datastores with 500k datasets. Imagine someone creating a table every hour to partition the data. Thats 24 * 365 tables per year or 9125 tables. Imagine a data schema with a single fact table and 10 dimensions. Now, each hour partition is 11 tables. It's easy to see how this explodes. You could say "thats a stupid design" and you'd be right but the team owning it is gone and it's not getting rewritten so you need to ingest it.

Currently, I'm using JSONable which simply adds a to_json method to the object. A large nested graph will to_json to a lot of nested dict objects. This is expensive to generate CPU wise and memory wise. It's not even json yet. Now, it needs to be converted to json and the json may stay in memory also. Thats the memory for the original graph, the dicts and now the json too. Imagine thousands of jobs running on a cluster all asking for these calls concurently. You can burn a lot of CPU and memory with this approach.

Basically, what I want to do is to serialize to the network as I convert the DSL objects to dicts. I need to use a Generator to do this. I only need the parts of the DSL that are being serialized right now, keeping it around during serialization is just a waste of memory. In my case, the dict is a means to an end, not the end itself.

If I used pydantic to do the json then it would be hugely inefficient and pydantic is not very good at complex class hierarchies either.

I still need to document all the API schemas using pydantic for fastapi mcp to work. But, I'm using cursor to generate the schemas and there is no point until I need to document it to generate them. I'm finding when using cursor AIs to generate large blocks of code, it's better to wait as cursor will not keep them up to date as I progress with the project automatically.

