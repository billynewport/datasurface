# December 2024

## 2024/12/09

The issues I was having around using a LegacyDataPlatform are because originally, I'd thought of DataPlatform as something configured globally and then selected for use by the ecosystem broker when the Workspaces are used to create the various pipeline graphs for each DataPlatform. The idea here was there would be a US AWS DataPlatform possibily using specify S3 buckets and Glue instances. There would also be others. The cardinality between DataPlatform instance and Workspaces would be 1:N. Many Workspaces per DataPlatform instance. However, with legacy data pipelines, this cardinality changes. It becomes 1:1. The problem was then that currently, DataPlatforms instances are declared at the ecosystem level. The Workspaces are decoupled from the DataPlatforms using a PlatformChooser. I used the Fixed platform chooser to create a legacy dataplatform instance configured with the team which owns the pipeline and other metadata for governance such as the DataContainers used by that legacy DataPlatform. This creates a problem in that this legacy platform is basically directly coupled to the Workspace which it actually is. It's possible that the legacy platform is used by multiple Workspaces, indeed this is likely. So, it's not quite 1:1, it's still 1:N but likely small N.

But, the impact of the original design is that ALL these legacy platforms would need to be created at the Ecosystem level and approved by the ecosystem team before those Workspaces were created and then referenced the legacy DataPlatform likely using a lookup platform method on Ecosystem with a key. This keeps the definition of the legacy platform centralized and it's referenced by the Workspace. The friction this causes might be an issue. But, at the same time, the Workspace team would create a lone of the repo, create the legacy platform in the Ecosystem, do a pull request, the eco team approves it and then the Workspace team can reference it when creating the Workspace. This is not bad. This allows the eco team to police the creation of duplicate platforms for example (in an ideal world, of course). It also exposes the Workspace team to seeing the other platforms and hopefully reusing an existing one, possibly adding some data containers used by this specific Workspace to it. This isn't that bad.

So, maybe the original design is still good. Working through this flow shows it's not that bad.

### References to DataPlatforms

Starting at classes which reference DataPlatforms. DefaultDataPlatform as well as FixedDataPlatform are currently the only 2 objects which require a reference to a DataPlatform when they are constructed. The convention right now is to obtain the reference by making a call to the getDataPlatformOrThrow method on the ecosystem with the name of the required instance as a parameter. If the name is unknown then an exception will be thrown.

I am considering if provide a direct DataPlatform reference is the right way or not. An alternative would be some kind of wrapped key string, called DataPlatformKey(name: str), for example. This is still type safe and avoids requiring the ecosystem reference during construction. The key would be resolved during the lint phase and if not found, an error can be exposed then. This avoids needing to break up the construction of the DSL for an ecosystem so that dataplatforms can be defined before being referenced in forced later add commands.

## 2024/12/10

Refactoring the DataPlatform stuff. The IacRendered has been renamed to DataPlatformGraphHandler. The intention here is that we will
need to know all the DataContainers used by a DataPlatform. This is only possible after the intention graphs for the DataPlatform
have been generated. Each graph results in a DataPlatform#createGraphHandler(graph) call which returns a DataPlatformGraphHandler. The
DataPlatformGraphHandler there fore has the information needed to figure out which DataContainers are used by that data platform
instance for that graph. This would include containers for producers, workspaces, internal containers and so on.

## 2024/12/11

### Summary of DatasetGroup/DataPlatform mapping approach

Teams cooperate and contribute towards a global ecosystem model in a github repo. All changes made by teams are contributed using pull requests in to the repository and verified by action handler that the changes:

* Pass basic validation rules
* Do not make the rest of the model inconsistent.
* All governance policies are still valid.

Now, we have the model in the primary branch in github. When a commit is detected on this branch then a process watching for these will run. The process will take the model and generate the set of DataPlatforms which will satisfy the consumer Workspaces defined in the model. Each DataPlatform will have 1 or more intention graphs. These intention graphs are the DAG working backwards from the Workspace/DatasetGroups towards the data producers. The DataPlatform can then make that DAG physical using any process it wishes. One approach would be to generate a terraform document for the pipelines and commit that to a different github branch. This branch can then be checked out periodically and terraform can be used to make the changes. It might make sense for a production system to do daily changes, a development system might do it on demand. It depends.

Each consumer DatasetGroup is serviced by exactly one DataPlatform instance. Instance means that it's possible for an AWS DataPlatform to be created which includes just US locations. Another instance might be a global one and have every location. Another might be just China. The instance is a particular configuration of a DataPlatform. Maybe even things like the master account for the AWS bills is configured on the instance. Workspaces then specify which dept they belong to or those instances cannot be chosen.
