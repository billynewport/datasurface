# December 2024

## 2024/12/09

The issues I was having around using a LegacyDataPlatform are because originally, I'd thought of DataPlatform as something configured globally and then selected for use by the ecosystem broker when the Workspaces are used to create the various pipeline graphs for each DataPlatform. The idea here was there would be a US AWS DataPlatform possibily using specify S3 buckets and Glue instances. There would also be others. The cardinality between DataPlatform instance and Workspaces would be 1:N. Many Workspaces per DataPlatform instance. However, with legacy data pipelines, this cardinality changes. It becomes 1:1. The problem was then that currently, DataPlatforms instances are declared at the ecosystem level. The Workspaces are decoupled from the DataPlatforms using a PlatformChooser. I used the Fixed platform chooser to create a legacy dataplatform instance configured with the team which owns the pipeline and other metadata for governance such as the DataContainers used by that legacy DataPlatform. This creates a problem in that this legacy platform is basically directly coupled to the Workspace which it actually is. It's possible that the legacy platform is used by multiple Workspaces, indeed this is likely. So, it's not quite 1:1, it's still 1:N but likely small N.

But, the impact of the original design is that ALL these legacy platforms would need to be created at the Ecosystem level and approved by the ecosystem team before those Workspaces were created and then referenced the legacy DataPlatform likely using a lookup platform method on Ecosystem with a key. This keeps the definition of the legacy platform centralized and it's referenced by the Workspace. The friction this causes might be an issue. But, at the same time, the Workspace team would create a lone of the repo, create the legacy platform in the Ecosystem, do a pull request, the eco team approves it and then the Workspace team can reference it when creating the Workspace. This is not bad. This allows the eco team to police the creation of duplicate platforms for example (in an ideal world, of course). It also exposes the Workspace team to seeing the other platforms and hopefully reusing an existing one, possibly adding some data containers used by this specific Workspace to it. This isn't that bad.

So, maybe the original design is still good. Working through this flow shows it's not that bad.

### References to DataPlatforms

Starting at classes which reference DataPlatforms. DefaultDataPlatform as well as FixedDataPlatform are currently the only 2 objects which require a reference to a DataPlatform when they are constructed. The convention right now is to obtain the reference by making a call to the getDataPlatformOrThrow method on the ecosystem with the name of the required instance as a parameter. If the name is unknown then an exception will be thrown.

I am considering if provide a direct DataPlatform reference is the right way or not. An alternative would be some kind of wrapped key string, called DataPlatformKey(name: str), for example. This is still type safe and avoids requiring the ecosystem reference during construction. The key would be resolved during the lint phase and if not found, an error can be exposed then. This avoids needing to break up the construction of the DSL for an ecosystem so that dataplatforms can be defined before being referenced in forced later add commands.

## 2024/12/10

Refactoring the DataPlatform stuff. The IacRendered has been renamed to DataPlatformGraphHandler. The intention here is that we will
need to know all the DataContainers used by a DataPlatform. This is only possible after the intention graphs for the DataPlatform
have been generated. Each graph results in a DataPlatform#createGraphHandler(graph) call which returns a DataPlatformGraphHandler. The
DataPlatformGraphHandler there fore has the information needed to figure out which DataContainers are used by that data platform
instance for that graph. This would include containers for producers, workspaces, internal containers and so on.

## 2024/12/11

### Summary of DatasetGroup/DataPlatform mapping approach

Teams cooperate and contribute towards a global ecosystem model in a github repo. All changes made by teams are contributed using pull requests in to the repository and verified by action handler that the changes:

* Pass basic validation rules
* Do not make the rest of the model inconsistent.
* All governance policies are still valid.

Now, we have the model in the primary branch in github. When a commit is detected on this branch then a process watching for these will run. The process will take the model and generate the set of DataPlatforms which will satisfy the consumer Workspaces defined in the model. Each DataPlatform will have 1 or more intention graphs. These intention graphs are the DAG working backwards from the Workspace/DatasetGroups towards the data producers. The DataPlatform can then make that DAG physical using any process it wishes. One approach would be to generate a terraform document for the pipelines and commit that to a different github branch. This branch can then be checked out periodically and terraform can be used to make the changes. It might make sense for a production system to do daily changes, a development system might do it on demand. It depends.

Each consumer DatasetGroup is serviced by exactly one DataPlatform instance. Instance means that it's possible for an AWS DataPlatform to be created which includes just US locations. Another instance might be a global one and have every location. Another might be just China. The instance is a particular configuration of a DataPlatform. Maybe even things like the master account for the AWS bills is configured on the instance. Workspaces then specify which dept they belong to or those instances cannot be chosen.

## 2024/12/14

Decided to following python package naming conventions which say that package names should be lowercase. This was a pretty big checkin and hit every file basically. One feature in python which I really dislike is it cannot manage cyclic references or forward references well. I merged a lot of files in md to governance.py. This is very different than Java for me which I have most experience in.

### Pylance not working as I'd like

vscode is acting up also and I find it makes it hard to see all the errors in the project when you change the names of stuff etc. This is more pylance than vscode but there is a lot of latency when I make changes before errors vanish. I have to wait a few seconds before I can see the errors. This is very annoying, and this is on an M2 Max with 96GB of RAM, what is going on? It looks like it was overindexing files, including files from the env/venv folders. I didn't change anything so I think this is a change in behaviour from a release or something. i excluded env/ venv from analysis in the settings file. It now finds about 740 files which I think includes extensions so that seems ok. I'll see if the speed is better now.

Finally figured out which setting makes pylance do the whole workspace for errors continously. I was needing to open each file to see any issues when I make breaking changes. Now, that this is working, I'm noticing some issues. First DataContainers had a single location associated with them. This is wrong. Many containers are replicated to multiple locations so it should be a set. This is a big ripple through all the DataContainer subclasses like databases and object stores. Working through this now. It is also causing an issue with AWS DMS DataPlatforms but that likely needs reworking anyway and I think I'll delete it for now and come back to it when I work on this later. Cloud is not the priority for now given the cost of keeping environments going. I'll focus on local data platforms first.

For some reason, pip wouldn't work, some kind of damage possibly when I moved everything from timemachine to the M2 Max laptop. I recreated the venv and it seems to be working now.

``` shell
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

After this, pylance is finding about 73 source files which is correct. Hopefully, my error latencies will be much faster now. I guess this is an area where the M4 Max's would be better with basically 50% single core speed improvements. At the same time, it's not like the M2 Max is slow. I'm sitting here with an Apple II plus beside me on the other desk...

## 2024/12/15

### Github issues with renaming files

Just tried updating vscode on the MacBookAir. I updates vscode and the extensions. I then opened it and did a github sync on the datasurface project. I did need to delete the env folder and create a .venv virtual environment as describe above. The problem I'm seeing now is that I did rename files like Governance.py to governance.py and this is reflected locally and things build locally. However, on github, the filename remains Governance.py and as a result nothing compiles there as the imports all use lower case governance also. Wondering how to fix this.
