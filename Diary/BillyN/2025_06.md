# June 2025

## 2025/06/01

I created a new test to introspect DSL classes and check that all attributes are covered by the __eq__ method. This is important because if an attribute is not tested then that attribute can be modified by an unauthorized repository and the git action handler will not catch it. This new test checks that all attributes are covered by the __eq__ method. I did find a few places where the __eq__ method was not complete. I fixed them.

## 2025/06/02

More work on improving testing for the DSL objects and Ecosystem and Team now use the new constructor style. Happy with the testing progress. Need to focus on having a working DataPlatform for rest of week.

## 2025/06/04

I need to revise my credential thinking. As part of getting to MVP for this, this all needs to be fleshed out right now. The DataPlatform owns the Credential store (this will ultimately be the PlatformProvider but for now, the DataPlatform can step in). The Credentials specified in the DSL are basically keys to content man+aged by the CredentialStore which depends on the DataPlatform. For example, I currently have FileSecretCredential which makes no sense because it's possible in a kubernetes environment that credential might be in a file or a environment variable. What matters is the type of credential not how it's stored, thats a CredentialStore/DataPlatform concern. When the DAG is rendered, the question is do all the Credentials in the DSL exist in the CredentialStore, this can only be figured out at runtime by checking the runtime platform hosting the DataPlatform. The MERGE Handler for the DataPlatform needs to be able to check this.

### Plan to implement

I'm in the beginning of implementing KubernetesPGStarterDataPlatform. This is the first DataPlatform so I'm expecting to have to make a lot of changes to the DSL and Ecosystem. It needs to support 3 phases. A lint check which makes sure that the intention graph assigned to it is supported by the implementation. This involves checking capturemetadata objects assigned to it are supported, potentially datatypes, column/table names are legal or created a mapping supported by the underlying data containers used by the platform. The plan is for the DataPlatform to store staging data and merge data in a postgres database initially. This means a type mapping and conversion process from the input sources must exist. Right now, we're only supporting kafka topic ingestion. This simplifies as the original source data systems are abstracted away by the existing data container to kafka publishing process. This leaves the question of the schema used for the kafka messages. I can assume that debezium is being used to do the kafka publishing for now. I'm also using kafka connect so it has a lot of source adapters and sink adapters. Primarily I'm using the source adapters to capture and publish source systems to topics. Once published to kafka then the sink connect can be used to write the messages to the staging tables in the postgres database used by the data platform. This runs continuously (or intermit batches or what ever mechanism kafka connect supports, it's not our problem once the sink connect is setup). We want to run a MERGE job which periodically takes unprocessed records from the staging tables and merges them into the MERGE tables. Each dataset being processed by the pipeline has its own staging and merge tables. This brings up the name mapping problem. Dataset names (combined with the name of the datastore containing it) needs to be used to create these staging and merge tables. Same for dataset columns mapping to columns within these staging tables. Expected name mapping issues are length, allowed characters, case sensitivity and uniqueness across all such tables within a database. Each store could be mapped to a schema and datasets are tables within the schema or we could have a single schema (named after the dataplatform) containing tables.

The merge algorithm is imagined as 2 implementations. The first is a simple one which takes messages as primarily insert/update events and maintains the latest version of the record in the merge table. The second is more complex which uses milestoning. It adds 2 columns to the merge record, dp_in and dp_out. The dp_in column is the timestamp when the record was first inserted in the merge table. The dp_out column is null (indicated live) or the timestamp when the record was replaced by a newer version or was deleted. The set of live records in the merge table is the set of records where dp_out is null.

There is also a job metrics table where merge jobs can record metrics about their execution. Number of rows inserted/updated/deleted, time taken, failure counts and anything else. I don't want to use prometheus as it's for reporting rather than a source of truth for the metrics. We can have a seperate airflow job which runs periodically and pushes these metrics to prometheus or a similar collector.

If merge jobs fail then the DAG job is marked as failed, the metrics table is updated with hints and the output of the merge job should by exposed by airflow for the operator to see. Jobs failing should fire events to notify the operator.

The DataPlatform will use a slack channel to publish events in human readable form.

The DataPlatform has an Airflow instance, a kafka connect cluster and a postgres database available to it. The airflow instance has jobs for each merge task. There is a merge task for every batch-stream from an ingested source. If the Capturemetadata indicates single data set then there is a batch-stream for every dataset. If it's multi-dataset then there is a batch-stream for every data store ingested which handles all datasets as a single transaction. There must be an airflow task for every batch-stream. The merge job does not run continously in this design. It's an airflow task which starts and finishes. We rely on airflow to guarantee single threaded execution of the merge tasks. I'm ignoring scaling issues with Airflow like number of tasks in a DAG. The merge tasks run with a default frequency of 5 minutes between runs. I will use AI later to decide on an optimial per batch-stream interval to balance resources used. The airflow tasks will run the merge logic as a kubernetes pod operator using a standard docker container image containing datasurface. The image will be given all credentials as environment variables for now. The DAG description needs to check all needed credentials for the merge job are available as kubernetes secrets and exposed using environment variables.

The GraphHandler returned by the DataPlatform handles linting during model validation in the action handler. It also handles rendering a new infrastructure when the model is revised with changes. The rendering means generating the following:

* Airflow DAG for the ingestion graph provided.
* System airflow DAG containing job for the renderer, the metrics collector and the table removal job.
* Terraform code to describe all the kafka connectors.

Each merge job will need to validate that the schema of the staging and merge tables are correct and that the tables exist. Once this is done then the merge job executes the appropriate merge algorithm. When a new data store is needed by a consumer then the rendering runs to add the necessary kafka connectors and airflow jobs. The airflow jobs then make sure the tables exist and are correct. Removing tables that are not used is a batch job which runs offline. This is an additional airflow job which runs periodically and removes tables that are not used by the current intention graph.

The current intention graph is determined by using git to extract the current branch containing the model. This is provided to the docker container image containing the python container as a mounted volume in a known location. The docker container can then use DataSurface to load the model and then execute the linting and rendering. The airflow jobs need to provide as parameters for merge jobs, the name of the data platform, data store and dataset name if needed (single dataset).

### Bootstrap

The Data platform is described in the model and contains the following parameters:

* Name of the data platform
* Kubernetes namespace to use for the data platform.
* Name to use for kafka connect cluster and hostnames. The kafka connect cluster is a kubernetes pod which runs the kafka connect server.
* Hostname for the postgres database.
* UsernamePassword Credential for the postgres database. The postgres database is a kubernetes pod which runs the postgres server.
* Credential for the kafka connect cluster.
* Name for the airflow instance. The airflow instance is a kubernetes pod which runs the airflow server.
* Credential for the airflow instance.
* The name of the docker container image hosting datasurface code. This is used for every kubernetes pod operator in airflow with different command line arguments depending on the task at hand.
* Credential for accessing git repository containing the model.
* Credential for slack and channel name for events.

My intention is for the bootstrap method to create a kubernetes yaml file containing definitions for the airflow/postgres and kafka connect clusters. The container image for kafka connect is assumed to have all connectors preinstalled. The user is expected to have created kubernetes secrets in advance to reference in this yaml file. All secrets must be made by the user, not the system. The system will only reference them. The DataPlatform creates its own pods for itself, it doesnt support running on existing components like postgres etc.

## 2025/06/06

Credential work today. CredentialStore is clearer now. Credential is a key really and the CredentialStore is responsible to lint it as supported and to return the credentials at runtime from itself. Different credential stores will use different mechanisms. We're supporting environment variables only for kubernetes right now. I added calls to check every credential used in the ingestion pipeline graph for a DataPlatform is supported by the DataPlatforms credentialstore.
