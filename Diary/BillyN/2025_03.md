# March 2025

## 2025/03/04

The schema reconciler. This is a standard service used by DataPlatforms to make sure the tables and views in a data base match the schemas in the DataSurface model. This would typically be called before an operation to update data in a table. Therefore, it is a service which takes a DataPlatform instance, a credential and the Workspace/Datasetgroup containing the datasets to reconcile. The intention graph has a set of graphs keys by the DataPlatform instance. It also has a list of DSG's per DataContainer. I'm going to try a simply python job using sqlachemy and SQL to start with. This will work and be tested on Postgres and hopefully will work on cloud databases like Snowflake/Redshift also. I'll try to avoid Spark for now.

Worked with claude to start getting a postgres instance running in github actions and locally. Adding a conftest.py file to initialize a fixture for the tests which need postgres accessto run.

Tests using the database should have a db_engine parameter and ones which want a clean database should have a test_db parameter.

## 2025/03/11

About to get on a plane to New Zealand tonight. Learning more about cursor rules and how to use it. I think it'll make a difference to how effective the AI tools are.

## 2025/03/12

### Schema management and making it flexible enough

Pipeline developers usually don't appreciate this enough as they target a single tech stack. We need to support multiple technologies over time. We need a way to map datasets to a DataContainer and take into account differences between DataContainers. PostGres and Iceberg tables for example will have differences in identifier naming, datatypes supported and so on. Sometimes even differences between Postgres or iceberg versions, new datatypes added for example. DataSurface needs to work with multiple technologies and versions of those concurrently.

Working on the code to create and maintain the schemas of objects in a DataContainer. The DataContainer will always place its constraints on a data platform. Naming limitations, length of identifiers, reserved words, reserved characters, case sensitivity. DataTypes also need from the model type to the DataContainer type even if its supported. I had some of this prototyped before but now I want to flesh it out and make it work with the first postgres container.

This also requires lint time checks, when the model is modified. If a specific DataContainer is chosen by or for a consumer then the datasets required by that consumer must be compatible with the selected DataContainer or at least mappable through a conversion. This really needs a tool, like an extension for vscode which keeps a local copy of the current latest model and constantly compares it to the current model or does it on demand and previews the action failures before attempting a push to the main branch.

### VsCode DataSurface extension

I've started exploring how I'd write this extension. Working with cursor, it looks straightforward. It's basically a wrapper around the existing code used during a merge operation in the action handlers. Vscode makes it easy to flag warnings and errors against the python code creating the DSL objects with the issues.

## 2025/03/20

Thinking about DataPlatforms and schemas today. Really, a DataPlatform needs to be given an ingestion, transformer or export graph node along with a DataContainer instance. It then needs to provision the schema artifacts etc needed to support that graph node on that DataContainer. For example, an ingestion node might require a staging table and a MERGE able. An export node would need the read only tables/views for the Workspaces. A transformer node would need everything an export node needs plus a set of tables to receive the transformer output for later ingestion.

A DataPlatform could check the schemas are correct before an operation such as ingest/merge, transform or export. It could also check once when a new version is committed to the model. Just store in metadata, the git version the schema is compatible with and only update the schemas when the git version changes. Doing them all at ocne when the git version changes is likely a problem as there will be jobs running and updating the schemas while jobs are running is usually a bad idea.

## 2025/03/27

I'm currently travelling in New Zealand so working in the ZeroDataPlatform is hard given I'm driving and moving location almost daily. I'll be in Sydney Austrailia on April 3rd for a couple of weeks so that should allow me to focus a little on this and make some progress. It's not much of an excuse but it is what it is. I have built a simpler/monolithic version of this previously and unfortunately, that brings a level of knowledge of whats needed to make DataSurface work. Sometimes, ignorance is bliss. I'd like to get this working with a ZeroDataPlatform and an s3/Spark version also. If I can get it to there, then I can get help as the bones will be there. I spent probably 3 years with a team of 20 doing a simpler/monolithic version of this last time so the amount of work shouldn't be underestimated. However, I learned a lot from that experience and it's 2024 rather than 2014 so I'm hoping I can get this working with much less code than last time.

Seperation of concerns. DataSurface will validate the live model when the action handler runs when pull requests are merged in. It's likely better to have an async listener watching for git merge events. When a new merge is approved then the watcher can pull the model and then start materializing the graph. This is two steps. There is the merge approval action handler which validates that the changes are ok and then there is the provisioning/reconciling step which will happen asynchronously in the background.

In parallel with this, it's important to remember that the DataPlatform infrastructures are running in parallel with this. Changing their state/configuration requires a level of coordination. Any changes from a merge approval will be either a forward compatible change or a delete something change. Forward compatible changes can be made when a dataplatform element responisble for a pipeline graph node is executed. An ingestion node can have the incoming schema updated, for example. A transformer will use a newer version of the code implementing the transformer. It's better for the code implementing those steps to check if the current infrastructure/environment matches the live model and if not then they should reconcile those changes before resuming the job for those steps. This means there are 2 levels of reconciliation. That which is done by the async merge listener and that which is done by the DataPlatform infrastructure when a pipeline step is executed.

I'll call these MERGE time reconciliation and node time reconciliation.

### MERGE time reconciliation

This happens when an async listener stops a new version being commited on the live branch for the model. This is responsible for initial provisioning of the DataPlatform infrastructure. The MERGE listener needs to look at the DAG subset for each DataPlatform and ask the DataPlatform to reconcile the graph against what has been previously provisioned in a previous MERGE time reconciliation. A DataPlatform can use state to figure this out. For example, this state could be stored in terraform and the DataPlatform just generates a new terraform file for the current graph and then asks terraform to apply it. Terraform would then check the current state against the new state and apply the changes. This is complicated by the fact that there are running jobs for the previous provisioned state. Jobs may need to be stopped and restarted or possibly just deleted. The DataPlatform will need a strategy for this. It could apply changes for new nodes with each MERGE but changes for existing nodes in the graph may be delayed until the next node time reconciliation when those jobs are executed. This is likely to be the most common approach. New stuff can be done instantly but existing stuff is delayed until the next node time reconciliation.

### Node time reconciliation

These typically happen at the beginning of the code for implementing a graph node. The code will check if the current state of the DataPlatform job matches the live model. This can be as simple as keeping the git hash of the branch which the infrastructure was provisioned to and if it's different from that then the code will pull the current model and reconcile the infrastructure before the job runs again, possibly even skipping the job and deleting itself if the changes means it's no longer needed. Reconciling schemas on tables/catalogs/views/indexes etc would be part of this work.

I would like both of these to be implemented in the DataPlatform in python. I'd like scripts which run to be able to invoke services calling these routines. There is a lot of helper code which can make coding these tasks easier and it's likely to be a lot of common code across platforms.

## 2025/03/28

So, I need to write the MERGE trigger python service which can cascade around the DataPlatforms so they can take appropriate actions. I'm also removing the non ZeroDataPlatforms because I have no short term plans to test them and support them as I fully implement the ZeroDataPlatform and the DataSurface driver. Many of the tests were using the cloud DataPlatforms so I modified those to use the LegacyDataPlatform.

## 2025/03/29

The MERGE driver will have the latest model checked out. Loading an Ecosystem from a folder and checking out specific versions is pretty easy for DataPlatforms to do. A DataPlatform can choose to only refresh infrastructure once a day or twice a day or on every change. It's a policy. For production systems, there may be windows when no changes are allowed for example. So, for some DataPlatforms, it's a noop. Those DataPlatforms would be running their own external loop independently of the MERGE driver. Funnily enough, the DataPlatform which needs to be the most current are ones that support the developers or team QA environments. Developers are always changing the model and pushing new versions and a lot of money/time can be wasted if the DataPlatforms those teams are using are not kept up to date quickly.

## 2025/03/30

Working with gemini-2.5-pro-03-25 today with cursor and it's a solid improvement over the previous models. I was using agent mode and it's able to analyze the project with documentation/diary and the code to create a 1 page white paper on the project. It wasn't bad, still needed some prompting to get to what I wanted but it was very good.

Code wise, a lot of objects in the DSL are subclassing from ANSI_SQL_NamedObject which tries to make sure the object name is SQL compatible. I'm trying to make differences between how objects are named between DataPlatforms are ideally just length. This impacts user experience because the user runs SQL queries against Workspaces to get data. They need to know that if the producer used a specific name then whats the equivalent name in the database/container that they are querying and they may not be the same. Ideally, they should be as similar as possible, ideally identical. Length limits and reserved works are the main issues with identical. I'm currently handling length restrictions using a hashing approach, restrict the identifier to X characters and then replace the last 2 characters with a hex hash of the full identifier. This is not a great user experience but it's the best I can do for now. Maybe, later an AI could be used to generate the identifiers with a length restriction rather than using hashing. An AI with the appropriate prompt could generate an identifier like CustOrder as an alias for CustomerOrder for example. It would need a post generation check to make sure the names make sense, are unique and are valid SQL identifiers. If this test fails then we could interate with the issue added to the prompt but ultimately if we can't pass the tests after a couple of tries then we'd revert to hashing or similar approaches.

Overall, I think enforcing some basic checks on naming of these objects is good hygiene. But, a physical mapping will still be needed.

### DataPlatform state

The mental discussion on physical name mapping has triggered a debate on should the state of a DataPlatform be stored in a git repository. Some aspects of the state are candidates. The provisioning state for example. The merge version, which aspects of the current model are being provisioned. The merge time reconciliation state. This would have things like Workspace assignments, whats provisioned for certain graph nodes. Node time reconciliation could be stored but this would run the risk of collisions with the MERGE time changes. Collisions will be common. There will be thousands of nodes at least. My own experience had graphs with 12 million datasets being ingested sending data through thousands of transformers and 3k Workspaces. The scale of the job state shouldn't be underestimated. DataPlatforms will conflate nodes to reduce the number of jobs as many job schedulers are simply not designed for this scale. Git certainaly isn't designed to absorb changes at this level of concurrency either. So, node time reconciliation state cannot be stored in a git repo.

State that is needed by node reconcilations also cannot be stored in git. Imagine thousands of concurrent jobs all doing git clones to grab the current version of the model. This won't work either. I also have not looked at how long it takes to inflate such a large model from the time of git clone to the time the DSL is ready to use. I would like this time to be seconds. I plan on using Modular as the python runtime as it's significantly faster at running python code than the standard python runtime. I'll need to look at the performance of Modular vs the standard runtime.

So, I'm largely where the DataPlatform state won't be stored in git at all. Anything needed by node time reconciliations needs to be stored somewhere fast, like a normal database. The Dataplatform may have a cached version of the last realized model version and a target version. When the target version is fully realized then we roll up the pointers and the realized model version becomes the target model and the target model becomes the lastest merged model version. A data platform will likely either use a git clone and a python server with services available to the node reconcilors or it will need to push the state in to some kind of database and then query it out. It's a lot of work for the database version and you still need services for the various jobs to pull their state out so git clone plus services looks the way to go and has the advantage that it could be implemeneted once and provided to the DataPlatform implementors.

### "Language server" style model server

This standard model API can be a lot of code. This was about 200k lines of java code using JAX-RS for a REST API with HBase underneath in my old platform. Datasurface is kind of splitting concerns when compared with the old system. The runtime state is being stored externally by DataPlatforms. DataSurface itself is storing the model state in a git repository. This lets DataSurface leverage git actions, pull requests, authentication, authorization, auditing, versioning and so on which is a major win. But, the model expressed as a python DSL is also trapped there. If a DataPlatform is written in python and has a cached model then it's going to want to just use the model in memory rather than an API. Non python DataPlatforms will either use a language server approach (like vscode) or a REST API to a service hosting the model version required.

I really like the idea of a language server. This could be packaged as a standard docker container which loads a specific version of the model from git when it starts up. It would then allow a client to execute python code against the model and get json back as the results. The coupling is tight but then everything is tightly coupled to the model DSL regardless. I can package the standard model server as a docker container and DataPlatforms can choose the scaling subsystem to host it. A DataPlatform can use K8, Docker Swarm or similar to host the model server when the model versions transistion. This would mean the language servers would need to stop serving the old model and move to the new one either by restarting or pausing/resuming each server instance. There will be a lot of instances running for a large DataPlatform, 30-40 model server instances on large boxes. The models should be backwards compatible so jobs where are running should be able to use either one. However, a job which starts using a dataset with schema version X, will not like the schema switching to version Y with some extra attributes mid job. We can treat this as a job reads all schemas at the beginning and caches them. Even within the startup, the version can change. The job should likely grab from the model server a merge version and then request that specific model version until the job is finished. If no such model version is available then the job should fail, then pick a current model version and continue.

Worked with Agent mode on auto today to prototype a language server built using FastAPI and wrote test cases for it. I got there in the end with the assistance of the agent mode but it was a bit of a struggle. I think it's simpler than the grpc one I'd done before so I'll replace that grpc one with this new FastAPI version.
