# February 2025

## 2025/02/01

Worked on making DSL objects more traceable when reporting validation errors. It was just reporting the name of the class and the issue but this doesn't tell the user where to look to find the problem. The code todays records the file and line number which constructed the DSL object, for example DataStore or Dataset etc. It does this by walking back the stack frame when the object is constructed looking for a stack frame which is not part of the datasurface codebase. The ValidationTree code was modified also to force all objects associated with a validation tree to be a LintableObject which is the construct every DSL class now inherits from.

ValidationTree.printTree was modified also to print out className@file:linenumber rather than just className. The next step is once again to finish a local DataPlatform to get a working system end to end.

There are also objects which are constructed from the model after it's been defined. These classes like pipeline graphs and the like inherit from a LintableObject subclass called InternalLintableObject. This simply sets the filename and line number to "Internal:0" and avoids walking the stack for these objects as there is no point. Users don't create these objects in their source, they are created by the system.

## 2025/02/02

Redid the class hierarchy for LintableObjects. Base class is now a ValidatableObject. This is subclasses by UserDSLObject rather than LintableObject. There is also an InternalLintableObject for objects created by the system. This is better but also required moving the ABC base class of quite a few DSL classes to the ValidatableObject class. ABC can't be defined more than once it seems.

## 2025/02/03

Starting on adding in streaming Datastore support.

## 2025/02/04

Making CredentialStore a first class DSL object with it acting as a factory for Credentials. Doing this as I think more about a Kafka streaming IngestionMetadata. I should have done this earlier.

I'm defining the kafka metadata for ingestion, i.e. a producer whose data is exposed on a kafka topic. I'm making a KafkaIngestion object to describe this. This is leading to how to define DataContainers such as a KafkaServer. DataContainer represent services which store data. These need to be managed from a policy point of view. A Kafka server is a DataContainer as it buffered data. It's no different than a SQL Database from this point of view, both of them store data. I don't want to define these inline when they are referenced by IngestionMetadata or on Workspaces. I'm thinking of trying defining them at the TeamScope. This makes sense to me because a team can create these and the stores/workspaces which use them at their level without needing them defined at the GovernanceZone or Ecosystem scopes.

The dataset to kafka topic mapping is an outstanding issue also. Debezium has a very specific mapping which is useful. Maybe, define mapping metadata with a debezium mapping and a flexible way to describe other mappings.

I put DataContainers on the Team scope, CredentialStores are next, still staring at the best place to put them. Making an example kafka ingestion model to work through how the DSL looks. Planning on putting more examples in that folder.

### Python DSL thoughts

Thinking about this also. Whats nice about things like yaml is that there is no coding, no refactoring, they are "linear". A Python DSL can be constructed a million different ways and the only way to see it is to execute the main factory method and then examine it in memory. It is also hard to give to an LLM to have a chat with about. LLMs want a text representation, having a python in memory data structure cannot be used directly, it has to be turned in to text (yml or json) and then it can work with that.

## 2025/02/05

Experimenting using pydantic to make the DSL easier to serialize to json or yaml. Discovered the python convention for __eq__ is to call the other object 'other'. I wasn't doing this so when I changed base classes to use standard classes, I had a lot of pylance errors because what I was calling the other parameter wasn't consistent with the standard classes.