# February 2025

## 2025/02/01

Worked on making DSL objects more traceable when reporting validation errors. It was just reporting the name of the class and the issue but this doesn't tell the user where to look to find the problem. The code todays records the file and line number which constructed the DSL object, for example DataStore or Dataset etc. It does this by walking back the stack frame when the object is constructed looking for a stack frame which is not part of the datasurface codebase. The ValidationTree code was modified also to force all objects associated with a validation tree to be a LintableObject which is the construct every DSL class now inherits from.

ValidationTree.printTree was modified also to print out className@file:linenumber rather than just className. The next step is once again to finish a local DataPlatform to get a working system end to end.

There are also objects which are constructed from the model after it's been defined. These classes like pipeline graphs and the like inherit from a LintableObject subclass called InternalLintableObject. This simply sets the filename and line number to "Internal:0" and avoids walking the stack for these objects as there is no point. Users don't create these objects in their source, they are created by the system.

## 2025/02/02

Redid the class hierarchy for LintableObjects. Base class is now a ValidatableObject. This is subclasses by UserDSLObject rather than LintableObject. There is also an InternalLintableObject for objects created by the system. This is better but also required moving the ABC base class of quite a few DSL classes to the ValidatableObject class. ABC can't be defined more than once it seems.

## 2025/02/03

Starting on adding in streaming Datastore support.

## 2025/02/04

Making CredentialStore a first class DSL object with it acting as a factory for Credentials. Doing this as I think more about a Kafka streaming IngestionMetadata. I should have done this earlier.

I'm defining the kafka metadata for ingestion, i.e. a producer whose data is exposed on a kafka topic. I'm making a KafkaIngestion object to describe this. This is leading to how to define DataContainers such as a KafkaServer. DataContainer represent services which store data. These need to be managed from a policy point of view. A Kafka server is a DataContainer as it buffered data. It's no different than a SQL Database from this point of view, both of them store data. I don't want to define these inline when they are referenced by IngestionMetadata or on Workspaces. I'm thinking of trying defining them at the TeamScope. This makes sense to me because a team can create these and the stores/workspaces which use them at their level without needing them defined at the GovernanceZone or Ecosystem scopes.

The dataset to kafka topic mapping is an outstanding issue also. Debezium has a very specific mapping which is useful. Maybe, define mapping metadata with a debezium mapping and a flexible way to describe other mappings.

I put DataContainers on the Team scope, CredentialStores are next, still staring at the best place to put them. Making an example kafka ingestion model to work through how the DSL looks. Planning on putting more examples in that folder.

### Python DSL thoughts

Thinking about this also. Whats nice about things like yaml is that there is no coding, no refactoring, they are "linear". A Python DSL can be constructed a million different ways and the only way to see it is to execute the main factory method and then examine it in memory. It is also hard to give to an LLM to have a chat with about. LLMs want a text representation, having a python in memory data structure cannot be used directly, it has to be turned in to text (yml or json) and then it can work with that.

## 2025/02/05

Experimenting using pydantic to make the DSL easier to serialize to json or yaml. Discovered the python convention for __eq__ is to call the other object 'other'. I wasn't doing this so when I changed base classes to use standard classes, I had a lot of pylance errors because what I was calling the other parameter wasn't consistent with the standard classes.

### Pydantic and complex nested graphs

It seems pydantic doesn't handle this well. If a Person object has a list of address objects and some of these address objects are in the list twice (home and work address are the same for example) or an employee has an employer object and the employer is shared between employees in memory then pydantic doesn't keep track of the employer objects it already created and reuses them. I'm investigating how much custom logic I'd need to write to handle these across the entire DSL. If I serialize a DSL graph then when I deserialize it, I want the resulting data structure to be identical to the original. I don't want to have to write custom code to handle this.

### Ripple of trying to use pydantic

Just tried a round of making all UserDSLObjects pydantic objects. This required adding attributes to all the classes for any instance variables which are DSL specified. I spotted some other DSL objects which weren't inheriting from UserDSLObject so after discarding the changes, I made those updates which showed up some python constructor trickiness. Looks like I need to call super constructor classes with specific types rather than simply using super() as when such a class is used in another class, the super() call doesn't know what type to use. Then again, I'm still a python newbie and learning how to do this in a sophisticated way still.

## 2025/02/06

Slowly realizing the problem with pydantic. Trying again today and realized that the main issue is inheritance. My DSL objects uses a class hierarchy. Each class constructor calls the base class init method in its constructor. The problem here is that if you call BaseModel.init then it assumes every instance variable has been set and this is simply not true. I was trying to move the BaseModel.init method to the last call of the constructor method but this doesn't work when the class has a subclass which also calls the base constructor. It seems pydantic is really not designed for inheritance at all. It's designed for a flat class structure. The whole validate your classes when it wants to even though I know they are correctly formed is breaking me here. I just want to convert my classes to json/yml and back again without the validation.

I'm going to try pyYAML today and see where I get.

## 2025/02/09

Surprisingly, doing yaml isn't easy. I extensively use python classes/class inheritiance/enums and so on. I appear to be alone in this given the lack of support even for Enum's in pyyaml. This should'nt be this hard. I just want to dump a class based graph data structure with internal references and read it back again. This doesn't seem that unusual a use case to me.

Yesterday, I tried using copilot to write the code and that was 2 hours of a disaster. The problem with it for me was that while I can coax it over a chat to slowly get to where I want it, when I then ask it for something new, it rewrites the existing code back at V1 and we're at the start again. I think for programmers doing simple web pages or basic programming, it works but for anything complex, well, it won't end well.

## 2025/02/12

Fleshing out the starter example. Working on the DSL, where to place CredentialStores and fleshing out the ZeroDataPlatform. Intention is to flow data from a Kafka data store to an S3 compatible local object store and then query using trino. This is a simple example but it's a start.

Adding DataPlatforms to the Ecosystem. Adding CredentialStores to DataPlatform. I really don't like the way locations are looked up on ecosystem, I'll switch to using a LocationKey object and validate it at lint time. The Zero DataPlatform will be designed to work in containers so I'm expecting credentials to be provided using the normal secrets mechanism defined by docker. The credentials will be plaintext files provided by the container infrastructure.
