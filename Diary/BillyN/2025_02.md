# February 2025

## 2025/02/01

Worked on making DSL objects more traceable when reporting validation errors. It was just reporting the name of the class and the issue but this doesn't tell the user where to look to find the problem. The code todays records the file and line number which constructed the DSL object, for example DataStore or Dataset etc. It does this by walking back the stack frame when the object is constructed looking for a stack frame which is not part of the datasurface codebase. The ValidationTree code was modified also to force all objects associated with a validation tree to be a LintableObject which is the construct every DSL class now inherits from.

ValidationTree.printTree was modified also to print out className@file:linenumber rather than just className. The next step is once again to finish a local DataPlatform to get a working system end to end.

There are also objects which are constructed from the model after it's been defined. These classes like pipeline graphs and the like inherit from a LintableObject subclass called InternalLintableObject. This simply sets the filename and line number to "Internal:0" and avoids walking the stack for these objects as there is no point. Users don't create these objects in their source, they are created by the system.

## 2025/02/02

Redid the class hierarchy for LintableObjects. Base class is now a ValidatableObject. This is subclasses by UserDSLObject rather than LintableObject. There is also an InternalLintableObject for objects created by the system. This is better but also required moving the ABC base class of quite a few DSL classes to the ValidatableObject class. ABC can't be defined more than once it seems.

## 2025/02/03

Starting on adding in streaming Datastore support.

## 2025/02/04

Making CredentialStore a first class DSL object with it acting as a factory for Credentials. Doing this as I think more about a Kafka streaming IngestionMetadata. I should have done this earlier.

I'm defining the kafka metadata for ingestion, i.e. a producer whose data is exposed on a kafka topic. I'm making a KafkaIngestion object to describe this. This is leading to how to define DataContainers such as a KafkaServer. DataContainer represent services which store data. These need to be managed from a policy point of view. A Kafka server is a DataContainer as it buffered data. It's no different than a SQL Database from this point of view, both of them store data. I don't want to define these inline when they are referenced by IngestionMetadata or on Workspaces. I'm thinking of trying defining them at the TeamScope. This makes sense to me because a team can create these and the stores/workspaces which use them at their level without needing them defined at the GovernanceZone or Ecosystem scopes.

The dataset to kafka topic mapping is an outstanding issue also. Debezium has a very specific mapping which is useful. Maybe, define mapping metadata with a debezium mapping and a flexible way to describe other mappings.

I put DataContainers on the Team scope, CredentialStores are next, still staring at the best place to put them. Making an example kafka ingestion model to work through how the DSL looks. Planning on putting more examples in that folder.

### Python DSL thoughts

Thinking about this also. Whats nice about things like yaml is that there is no coding, no refactoring, they are "linear". A Python DSL can be constructed a million different ways and the only way to see it is to execute the main factory method and then examine it in memory. It is also hard to give to an LLM to have a chat with about. LLMs want a text representation, having a python in memory data structure cannot be used directly, it has to be turned in to text (yml or json) and then it can work with that.

## 2025/02/05

Experimenting using pydantic to make the DSL easier to serialize to json or yaml. Discovered the python convention for __eq__ is to call the other object 'other'. I wasn't doing this so when I changed base classes to use standard classes, I had a lot of pylance errors because what I was calling the other parameter wasn't consistent with the standard classes.

### Pydantic and complex nested graphs

It seems pydantic doesn't handle this well. If a Person object has a list of address objects and some of these address objects are in the list twice (home and work address are the same for example) or an employee has an employer object and the employer is shared between employees in memory then pydantic doesn't keep track of the employer objects it already created and reuses them. I'm investigating how much custom logic I'd need to write to handle these across the entire DSL. If I serialize a DSL graph then when I deserialize it, I want the resulting data structure to be identical to the original. I don't want to have to write custom code to handle this.

### Ripple of trying to use pydantic

Just tried a round of making all UserDSLObjects pydantic objects. This required adding attributes to all the classes for any instance variables which are DSL specified. I spotted some other DSL objects which weren't inheriting from UserDSLObject so after discarding the changes, I made those updates which showed up some python constructor trickiness. Looks like I need to call super constructor classes with specific types rather than simply using super() as when such a class is used in another class, the super() call doesn't know what type to use. Then again, I'm still a python newbie and learning how to do this in a sophisticated way still.

## 2025/02/06

Slowly realizing the problem with pydantic. Trying again today and realized that the main issue is inheritance. My DSL objects uses a class hierarchy. Each class constructor calls the base class init method in its constructor. The problem here is that if you call BaseModel.init then it assumes every instance variable has been set and this is simply not true. I was trying to move the BaseModel.init method to the last call of the constructor method but this doesn't work when the class has a subclass which also calls the base constructor. It seems pydantic is really not designed for inheritance at all. It's designed for a flat class structure. The whole validate your classes when it wants to even though I know they are correctly formed is breaking me here. I just want to convert my classes to json/yml and back again without the validation.

I'm going to try pyYAML today and see where I get.

## 2025/02/09

Surprisingly, doing yaml isn't easy. I extensively use python classes/class inheritiance/enums and so on. I appear to be alone in this given the lack of support even for Enum's in pyyaml. This should'nt be this hard. I just want to dump a class based graph data structure with internal references and read it back again. This doesn't seem that unusual a use case to me.

Yesterday, I tried using copilot to write the code and that was 2 hours of a disaster. The problem with it for me was that while I can coax it over a chat to slowly get to where I want it, when I then ask it for something new, it rewrites the existing code back at V1 and we're at the start again. I think for programmers doing simple web pages or basic programming, it works but for anything complex, well, it won't end well.

## 2025/02/12

Fleshing out the starter example. Working on the DSL, where to place CredentialStores and fleshing out the ZeroDataPlatform. Intention is to flow data from a Kafka data store to an S3 compatible local object store and then query using trino. This is a simple example but it's a start.

Adding DataPlatforms to the Ecosystem. Adding CredentialStores to DataPlatform. I really don't like the way locations are looked up on ecosystem, I'll switch to using a LocationKey object and validate it at lint time. The Zero DataPlatform will be designed to work in containers so I'm expecting credentials to be provided using the normal secrets mechanism defined by docker. The credentials will be plaintext files provided by the container infrastructure.

## 2025/02/17

Thinking about what I want the zero dataplatform to be. I am a fan of the "don't distribute if not needed" mantra. I know for a fact that the vast majority of data warehouse needs do not need a scalable architecture. They are small, have low change rates and can easily be accomodated in a single box database like PostGres or similar. The issue with this is when you have a lot of datasets. How many datasets can you put in a single database instance? More than you think is the likely answer. Each dataset would need a staging table and a MERGE table. I have personally seen Microsoft SQL Server databases with 500k database tables. Crazy, but it existed. My production experience with a large data warehouse plant had 12 million datasets. This would be 24 million datasets and would overwhelm any single instance database probably, it would likely overwhelm most of the data catalog products for non sql systems also. So, such a solution would end up using say 50k datasets per database instance. We'd need 20 instances for a million datasets. Thats not that bad. Spark jobs can run natively against the datasets stored in these database instances. It depends on how many jobs you want to run concurrently but the other angle is do you need spark at all for this work? Would simple python applications using jdbc be better? Really what you'd need is a framework to declare the 100 datasets (as an example) that the application needs and then to take the application queries and federate them on to the up to 100 database instances. The number of database instances would change dynamically. It will be necessary to balance datasets across the database instances occasiionally. WE might be running out of disk space,memory or cpu capacity. So, we'd add 50 databases and move datasets to the 50 new ones and then once they are up to date, transistion future client requests to the new database instances for those datasets. Clients would need to ask the data platform for the location of a dataset and the dataplatform would need to promise not to move it/delete it while it's being used. Simple lease style appraoches have worked for me in the past here. Once a client asks for a location then it gets a lease for 12 hours. The idea being that the client must finish within 12 hours which seems realizable.

This is more complex than simply putting everything in an s3 store with iceberg tables for example. But, you can push down more complex queries to a postgres cluster and get better performance. It's not like there is an advantage to fewer instances with things like Spark anyway. Spark doesn't care whether there is 1 s3 instance or 1000. It will still read the data in parallel. The advantage of fewer instances is less complexity and less operational cost (possibly).

I think as a starter DataPlatform, simple is likely best. Maybe, we do 2. One using postgres for the staging and MERGE tables and one using iceberg. The iceberg one would be more complex to implement but it would be more scalable. The postgres one would be simpler to implement but less scalable. Lets see how much code can be shared between the two. Postgres 15 supports MERGE SQL statements which is also supported on Spark/Iceberg. Iceberg it's self doesn't really support anything, it's the SQL engine on top which matters.

I'd like to be able to run this code on both postgres and snowflake from a python application. They both support MERGE style SQL commands. The stack might then just be:

* Gitlab to store the DSL, run the validation actions from pull requests
* Airflow to schedule python jobs
* Postgres for storing staging and MERGE tables
* Kafka Connect to copy messages from kafka topics to postgres staging tables for processing by python jobs.

Consumer Workspaces could then connect directly to views layered on the MERGE tables and this would be a basic running platform. Consumers could scale out by running their Workspaces on top of READ only replicas of the MERGE tables. Eventually of course, we'd get ingest limited by the postgres core instance and even that could be scaled out by spreading datasets over multiple postgres instances. We could also then do more sophisticated consumer postgres management. Lets assign Workspaces/DatasetGroups to one of N managed postgres consumer facing instances. Now, only the union of the DSRs of the DSG's assigned to a postgres consumer instance need to be replicated to it. We can redistributed consumers across more and more postgres instances as consumer load grows. This decouples the consumer query engine from the MERGE engine. I believe this is an important tenet of this type of system. You will be switching MERGE technologies over time, you want to be able to do this without disrupting the consumer query engines.

We could switch the MERGE tech stack to a minio/Spark based stack also. This would scale better potentially. Our ingest performance is now limited by minio scaling and python job scaling. We could run the python jobs against trino instances on minio. Trino would become the bottleneck. This simple stack would be a good starting point to deploy a data platform to allow DataSurface to be deployed. It would scale up to a pretty big infrastructure and then we could look at a different DataPlatform or indeed use both Dataplatforms.

My local .venv got messed up somehow. pip stopped working. When I run pip, I get a module missing for pip. No idea why. I used the following commands in the vscode terminal to recreate it:

```bash
rm -rf .venv
python3 -m venv .venv
pip install --upgrade pip
. .venv/bin/activate
pip install -r requirements.txt
# Verify the installation
pip list
```

Well, that didn't work. vscode didn't recognize the new .venv. I had to go to the command palette and select the new .venv. vscode created a new .venv and it needed the latest python 3.11 version, not 3.13. 3.13 caused problems installing pyscop. Now, it works again.

It's broke again. Looks like a vscode update bug type thing.

## 2025/02/18

I switched to cursor instead of vscode. The code level of cursor doesn't have the current bug in vscode which breaks venv support. The AI assistant seems better than copilot also. Discussed yaml support with cursor. It's not straight forward. It gave the best suggestion so far which was to use the pyyaml library and add dict conversion methods to all the classes. My usage of python inheritance and objects referenced multiple times puts me out of common use for yaml. Of course it can be done, it's just how hard is it going to be working around the tools, thats the question.

## 2025/02/19

Installed bitnami postgres 17.3 on the m2max and the dell today with docker compose. I need kafka connect also on my m2max, it's already on the dell. Looking at the existing spark dmsingestor code to convert it to use postgres instead of spark and minio for simplicity. Just use kafka connect to copy data from kafka to postgres staging tables and then run a python job to read from the staging tables and write to the MERGE tables. Same as before without spark/minio. Ok, kafka-connect is installed and running on the m2max. That gives me a starting point, infra wise.

## 2025/02/20

Looking at the zero container. Providing secrets isn't very standardized across cloud vendors. Some containers (docker secrets) inject secrets on mounted volumes in plain text within the container with a RAM disk. The cloud vendors seem to use APIs (non portable). So, I'm looking for something that works like docker secrets but hides cloud platform differences.I'd like the zero platform to be usable locally and on cloud providers. It looks like the best bet here is to do kubernetes which has managed versions on the different cloud providers. I'll use docker kubernetes on my laptop, I've had a lot of trouble in the past with kubernetes locally on the dell so I'll try kubernetes locally on the m2max. Had a slight problem with docker kubernetes, some kind of certificate issue.  I reset the kubernetes config in desktop and that fixed it.

Downloaded helm using brew in the end. Easier. upgrading all brew packages while I'm at it, lets hope this doesnt break anything. But you know what, I'm not going to use kubernetes initially anyway, time heals all wounds and I have forgotten what a nghtmare this was last time I tried this, 3 weeks of wasted time. I'll stick with docker compose for now. I just need kafka connect and postgres to get this running.

## 2025/02/25

Next step, make code to create tables using SQLAlchemy. I'm using SQLAlchemy to leverage it's support for multiple databases and their SQL dialects. The code needs to create a table if missing and make sure the columns defined within it match the columns in the dataset. It needs to create views as well as tables.

## 2025/02/26

Goal today is to get the code to create tables and views. I'm going to start with postgres. I've got a script to create a table if it doesn't exist and match the columns to the dataset. I'm going to add views next. I'm thinking of how this will work. I think schema reconciliation is a major common feature across DataPlatforms. It makes sense to have it as a provided capability in the main module. A DataPlatform can always get the current git branch, mount it in a docker volume and then use the primary datasurface docker container to invoke the code to make this happen. The DataPlatform will also need to provide credentials for access to the data containers which it should have already.

The DataSurface docker container will package everything DataSurface needs to run. IT has teh code, the dependencies, the python interpreter. The model can be provides as a mounted volume. Credentials can also be provided as mounted volumes. The DataPlatform can then easily use "docker run" to spin it up and take advantage of the standard services. The container will provide top level shell scripts for the various services. This lets me switch to modular to run the python code for more speed if that makes sense plus all the usual benefits
