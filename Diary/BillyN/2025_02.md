# February 2025

## 2025/02/01

Worked on making DSL objects more traceable when reporting validation errors. It was just reporting the name of the class and the issue but this doesn't tell the user where to look to find the problem. The code todays records the file and line number which constructed the DSL object, for example DataStore or Dataset etc. It does this by walking back the stack frame when the object is constructed looking for a stack frame which is not part of the datasurface codebase. The ValidationTree code was modified also to force all objects associated with a validation tree to be a LintableObject which is the construct every DSL class now inherits from.

ValidationTree.printTree was modified also to print out className@file:linenumber rather than just className. The next step is once again to finish a local DataPlatform to get a working system end to end.

There are also objects which are constructed from the model after it's been defined. These classes like pipeline graphs and the like inherit from a LintableObject subclass called InternalLintableObject. This simply sets the filename and line number to "Internal:0" and avoids walking the stack for these objects as there is no point. Users don't create these objects in their source, they are created by the system.

## 2025/02/02

Redid the class hierarchy for LintableObjects. Base class is now a ValidatableObject. This is subclasses by UserDSLObject rather than LintableObject. There is also an InternalLintableObject for objects created by the system. This is better but also required moving the ABC base class of quite a few DSL classes to the ValidatableObject class. ABC can't be defined more than once it seems.

## 2025/02/03

Starting on adding in streaming Datastore support.

## 2025/02/04

Making CredentialStore a first class DSL object with it acting as a factory for Credentials. Doing this as I think more about a Kafka streaming IngestionMetadata. I should have done this earlier.

I'm defining the kafka metadata for ingestion, i.e. a producer whose data is exposed on a kafka topic. I'm making a KafkaIngestion object to describe this. This is leading to how to define DataContainers such as a KafkaServer. DataContainer represent services which store data. These need to be managed from a policy point of view. A Kafka server is a DataContainer as it buffered data. It's no different than a SQL Database from this point of view, both of them store data. I don't want to define these inline when they are referenced by IngestionMetadata or on Workspaces. I'm thinking of trying defining them at the TeamScope. This makes sense to me because a team can create these and the stores/workspaces which use them at their level without needing them defined at the GovernanceZone or Ecosystem scopes.

The dataset to kafka topic mapping is an outstanding issue also. Debezium has a very specific mapping which is useful. Maybe, define mapping metadata with a debezium mapping and a flexible way to describe other mappings.

I put DataContainers on the Team scope, CredentialStores are next, still staring at the best place to put them. Making an example kafka ingestion model to work through how the DSL looks. Planning on putting more examples in that folder.

### Python DSL thoughts

Thinking about this also. Whats nice about things like yaml is that there is no coding, no refactoring, they are "linear". A Python DSL can be constructed a million different ways and the only way to see it is to execute the main factory method and then examine it in memory. It is also hard to give to an LLM to have a chat with about. LLMs want a text representation, having a python in memory data structure cannot be used directly, it has to be turned in to text (yml or json) and then it can work with that.

## 2025/02/05

Experimenting using pydantic to make the DSL easier to serialize to json or yaml. Discovered the python convention for __eq__ is to call the other object 'other'. I wasn't doing this so when I changed base classes to use standard classes, I had a lot of pylance errors because what I was calling the other parameter wasn't consistent with the standard classes.

### Pydantic and complex nested graphs

It seems pydantic doesn't handle this well. If a Person object has a list of address objects and some of these address objects are in the list twice (home and work address are the same for example) or an employee has an employer object and the employer is shared between employees in memory then pydantic doesn't keep track of the employer objects it already created and reuses them. I'm investigating how much custom logic I'd need to write to handle these across the entire DSL. If I serialize a DSL graph then when I deserialize it, I want the resulting data structure to be identical to the original. I don't want to have to write custom code to handle this.

### Ripple of trying to use pydantic

Just tried a round of making all UserDSLObjects pydantic objects. This required adding attributes to all the classes for any instance variables which are DSL specified. I spotted some other DSL objects which weren't inheriting from UserDSLObject so after discarding the changes, I made those updates which showed up some python constructor trickiness. Looks like I need to call super constructor classes with specific types rather than simply using super() as when such a class is used in another class, the super() call doesn't know what type to use. Then again, I'm still a python newbie and learning how to do this in a sophisticated way still.

## 2025/02/06

Slowly realizing the problem with pydantic. Trying again today and realized that the main issue is inheritance. My DSL objects uses a class hierarchy. Each class constructor calls the base class init method in its constructor. The problem here is that if you call BaseModel.init then it assumes every instance variable has been set and this is simply not true. I was trying to move the BaseModel.init method to the last call of the constructor method but this doesn't work when the class has a subclass which also calls the base constructor. It seems pydantic is really not designed for inheritance at all. It's designed for a flat class structure. The whole validate your classes when it wants to even though I know they are correctly formed is breaking me here. I just want to convert my classes to json/yml and back again without the validation.

I'm going to try pyYAML today and see where I get.

## 2025/02/09

Surprisingly, doing yaml isn't easy. I extensively use python classes/class inheritiance/enums and so on. I appear to be alone in this given the lack of support even for Enum's in pyyaml. This should'nt be this hard. I just want to dump a class based graph data structure with internal references and read it back again. This doesn't seem that unusual a use case to me.

Yesterday, I tried using copilot to write the code and that was 2 hours of a disaster. The problem with it for me was that while I can coax it over a chat to slowly get to where I want it, when I then ask it for something new, it rewrites the existing code back at V1 and we're at the start again. I think for programmers doing simple web pages or basic programming, it works but for anything complex, well, it won't end well.

## 2025/02/12

Fleshing out the starter example. Working on the DSL, where to place CredentialStores and fleshing out the ZeroDataPlatform. Intention is to flow data from a Kafka data store to an S3 compatible local object store and then query using trino. This is a simple example but it's a start.

Adding DataPlatforms to the Ecosystem. Adding CredentialStores to DataPlatform. I really don't like the way locations are looked up on ecosystem, I'll switch to using a LocationKey object and validate it at lint time. The Zero DataPlatform will be designed to work in containers so I'm expecting credentials to be provided using the normal secrets mechanism defined by docker. The credentials will be plaintext files provided by the container infrastructure.

## 2025/02/17

Thinking about what I want the zero dataplatform to be. I am a fan of the "don't distribute if not needed" mantra. I know for a fact that the vast majority of data warehouse needs do not need a scalable architecture. They are small, have low change rates and can easily be accomodated in a single box database like PostGres or similar. The issue with this is when you have a lot of datasets. How many datasets can you put in a single database instance? More than you think is the likely answer. Each dataset would need a staging table and a MERGE table. I have personally seen Microsoft SQL Server databases with 500k database tables. Crazy, but it existed. My production experience with a large data warehouse plant had 12 million datasets. This would be 24 million datasets and would overwhelm any single instance database probably, it would likely overwhelm most of the data catalog products for non sql systems also. So, such a solution would end up using say 50k datasets per database instance. We'd need 20 instances for a million datasets. Thats not that bad. Spark jobs can run natively against the datasets stored in these database instances. It depends on how many jobs you want to run concurrently but the other angle is do you need spark at all for this work? Would simple python applications using jdbc be better? Really what you'd need is a framework to declare the 100 datasets (as an example) that the application needs and then to take the application queries and federate them on to the up to 100 database instances. The number of database instances would change dynamically. It will be necessary to balance datasets across the database instances occasiionally. WE might be running out of disk space,memory or cpu capacity. So, we'd add 50 databases and move datasets to the 50 new ones and then once they are up to date, transistion future client requests to the new database instances for those datasets. Clients would need to ask the data platform for the location of a dataset and the dataplatform would need to promise not to move it/delete it while it's being used. Simple lease style appraoches have worked for me in the past here. Once a client asks for a location then it gets a lease for 12 hours. The idea being that the client must finish within 12 hours which seems realizable.

This is more complex than simply putting everything in an s3 store with iceberg tables for example. But, you can push down more complex queries to a postgres cluster and get better performance. It's not like there is an advantage to fewer instances with things like Spark anyway. Spark doesn't care whether there is 1 s3 instance or 1000. It will still read the data in parallel. The advantage of fewer instances is less complexity and less operational cost (possibly).

I think as a starter DataPlatform, simple is likely best. Maybe, we do 2. One using postgres for the staging and MERGE tables and one using iceberg. The iceberg one would be more complex to implement but it would be more scalable. The postgres one would be simpler to implement but less scalable. Lets see how much code can be shared between the two. Postgres 15 supports MERGE SQL statements which is also supported on Spark/Iceberg. Iceberg it's self doesn't really support anything, it's the SQL engine on top which matters.

I'd like to be able to run this code on both postgres and snowflake from a python application. They both support MERGE style SQL commands. The stack might then just be:

* Gitlab to store the DSL, run the validation actions from pull requests
* Airflow to schedule python jobs
* Postgres for storing staging and MERGE tables
* Kafka Connect to copy messages from kafka topics to postgres staging tables for processing by python jobs.

Consumer Workspaces could then connect directly to views layered on the MERGE tables and this would be a basic running platform. Consumers could scale out by running their Workspaces on top of READ only replicas of the MERGE tables. Eventually of course, we'd get ingest limited by the postgres core instance and even that could be scaled out by spreading datasets over multiple postgres instances. We could also then do more sophisticated consumer postgres management. Lets assign Workspaces/DatasetGroups to one of N managed postgres consumer facing instances. Now, only the union of the DSRs of the DSG's assigned to a postgres consumer instance need to be replicated to it. We can redistributed consumers across more and more postgres instances as consumer load grows. This decouples the consumer query engine from the MERGE engine. I believe this is an important tenet of this type of system. You will be switching MERGE technologies over time, you want to be able to do this without disrupting the consumer query engines.

We could switch the MERGE tech stack to a minio/Spark based stack also. This would scale better potentially. Our ingest performance is now limited by minio scaling and python job scaling. We could run the python jobs against trino instances on minio. Trino would become the bottleneck. This simple stack would be a good starting point to deploy a data platform to allow DataSurface to be deployed. It would scale up to a pretty big infrastructure and then we could look at a different DataPlatform or indeed use both Dataplatforms. 