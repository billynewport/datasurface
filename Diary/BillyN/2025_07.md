# July 2025

## 2025/07/02

I'm working with cursor in auto mode to verify the starter platform is correctly generating the kubernetes and airflow artifacts. It's working suprisingly well. The kubernetes artifacts are working on my local docker desktop kubernetes cluster. I built a docker container image for all the datasurface code which we need for airflow tasks as they are primarily kubernetes pod operators which need a container image. I'm building the image with this command:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

I have created a repository on docker hub called datasurface and the primary container image is there and called datasurface/datasurface:latest.

Right now, the bootstrap infrastructure is working and I wrote a [how to guide](../docs/HowToKubPGStarterSetup.md) for setting up the starter platform. The airflow DAG seems ok and the next step is to render the terraform files for the kafka connectors and the ingestion DAG.

Reworking the SQLIngestion stuff. I'm trying to simplify for the starter platform. The easiest ingestion involves as few moving parts as possible. A simple batch "select * from every table" to staging every batch is easy and useful. There are a lot of databases in companies which simply exist and have no fancy CDC software. A SQL full pull is the answer given how hard it is to reversion these datastores on more modern databases or add CDC to them.

## 2025/07/03

Working on the snapshot merge job. Frustration with SQLAlchemy type checking drove me back to 2.0 but it's still not great. I miss Java at times like this.

## 2025/07/04

Command to build the datasurface container image:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

Command to push the image to docker hub:

```bash
docker push datasurface/datasurface:latest
```

Looking at docker scout. I don't like all the vulnerablilities so I spent an hour today cleaning up the image, much better now and everything works on python 3.13 so thats good.

## 2025/07/07

Working on running the merge job in tests. Fixing some issues not passing DataContainers to the SQLIngestion classes. Making a simple ecosystem model for testing which
just has a single datastore and dataset with a single Workspace.

## 2025/07/08

My experience building the Goldman Sachs data lake is weighing on me now. I broke up the batch processing pipeline when I did this before. I had a job for start batch, ingest data, validate data, merge data, commint batch and then export jobs to push deltas to client data warehouses which were different that the merge storage. My merge storage was parquet files on HDFS and the consumers mostly used a sybase iq OLAP database. This introduced these problems:

* There are a lot of jobs. Imagine 8k datastores with 12 million tables and hundreds of Sybase IQ databases. WE ran in to job limitation in both the job scheduler and the YARN cluster job scheduler. We ended up having to collapse jobs in to bigger jobs to get the number of jobs under control at scale.
* Job scheduling latency was a major issue, mostly with YARN. It would sometimes takes minutes from when a job was scheduled and when it started running.
* We changed export jobs to use a custom job scheduler which limited the concurrency of export jobs on a specific Sybase IQ system as we were overloading IQ because of too many concurrent jobs especially when they did meta data changes.
* AWS Glue and Airflow do not scale well. Hundreds of jobs is about their limit as far as I can tell. I haven't used them yet but I read glue has a limit of a 100 jobs in their own documentation.

So, I know I'm going to need macro and micro schedulers, 2 levels with the inner level possibly a multi-threaded event handler for specific actions. The job will go through several stages. A batch starts, the data gets copied to staging from the source. The data is then merged in to the merge table and finally it gets commited. There is a metadata table recording batch state for each batch. Started, ingesting, merging, committed and failed. Multi-dataset ingestions can collapse these but some of these operations are very time consuming. Imagine the first batch for a billion row dataset, for example, you have to do it in sub batches and commit each one seperately with a restart point during ingestion. The batch would stay in ingesting for hours or even days.

Single dataset ingestion means the producer wants each dataset/table to be ingested independently off the other datasets at the source, there is no consistency between datasets. Multi-dataset ingestion means the producers wants all datasets to be ingested together within a single transaction, consistency between datasets is important. Single dataset ingestion explodes the number of jobs. Imagine a datastore with 500 datasets or more. There are trade offs here in terms of collapsing job counts and individual ingestion stream latency. Developers often focus on individual job or batch latency but do not look at the impact of this at scale. If we give everybody their own daemon running continously, this seems most optimal. Now, imagine the memory resources to do that for 12 million datasets, 24 hours a day versus running ingestion in a batch which it only uses memory when the batch is running, not all the time. One solution architectures don't work at scale. At the same time, a job per batch isn't always better either, it depends.

Thus the job DAG in airflow for a dataset might just be one job per ingestion stream. When it runs, it checks what needs to happen, continue ingesting to some limit? Stop ingesting, start merging? Start a new batch and resume ingesting. This is likely my first implementation. The job can be triggered either by an external trigger or timer or by the previous job finishing and then it schedules itself immeadiately. Given there are many such jobs, one per ingestion stream, they will run round robin until the batch commits and then wait for an external trigger like a timer.

The job is executed using the datasurface container image using a kubernetes pod operator. This can invoke the job python command. The job returns a string which is "KEEP_WORKING" if the job needs to go back in the queue and run again later or "DONE" indicating the current ingestion/merge has completed and now the job should wait for an external trigger to start the next batch.

### DataPlatform needs an opportunity to check the Workspace is good

CAme across a bug testing this stuff today. My test ecosystem Workspace has no data container which is causing problems as there are a few places where no data container caused chunks of code to be skipped. The Workspace has a lint method but it also needs to be linted by the chosen DataPlatform. There also needs to be a way for the DataPlatform to assign a data container to the Workspace when this is appropriate. Maybe, I need to make the Workspace datacontainer non optional and provide a datacontainer called DataPlatformAssigned for use. In the case of the starter platform the Workspaces will use the merge store as the data container for now, there is no export, consumers just query the merge database directly through workspace specific views for all datasetsinks needed.

DataPlatform needs a way to lint a Workspace when the chooser picks the DataPlatform for the Workspace. Workspaces are currently linted before the DataPlatform is chosen. Maybe, I extend the Workspace lint method to also allow the chosen DataPlatform to lint the Workspace. This seems early though. I'd prefer this linting to happen when the graph is generated.

Thinking outloud here. The other issue is once a pipeline is rendered and operational, switching to another Dataplatform needs guard rails. So, there needs to be infrastructure metadata stored outside git, a runtime infra database maybe, where the current DataPlatform for each Workspace is stored. If the DataPlatform is changed the the existing pipeline needs to be stopped, the new pipeline needs to be started and the metadata for the old pipeline needs to be migrated to the metadata for the new pipeline. For example, ingestion offsets would need to be preserved. Existing data needs to be preserved and possibly exported to the new dataPlatform's containers if needed. This should be automated but there needs to be a guard rail. Pause the existing pipeline on a long weekend for example and then do the migration and then we are on the new pipeline by the next business day. Or more likely, we keep the old pipeline running and start the new pipeline in parallel after migrating. Maybe the consumer can verify a provided test Workspace using the new pipeline before switching over and then the old pipeline can be stopped. This also allows a rollback where the old pipeline is kept in case there are issues with the new pipeline not uncovered during testing, it happens...

TODO for tomorrow. Allow DataPlatform to lint a Workspace in Workspace lint method. Once the graph is generated then a second linting pass can be done which already exists in DataPlatform. This can check all types/schemas/ingestion types/data containers/ are supported by the DataPlatform. This would happen during git actions so it will be visible to the user when pushing changes.

The infrastructure time handling of dataplatform changes requires a framework. To detect Workspace DataPlatform reassignments and to have a workflow to manage through that. The infra rendering code needs a historical list of DataPlatforms assigned to each Workspace and keep those working until there are decommissioned. Maybe, a Workspace instead has a list of DataPlatforms and DataPlatforms can only be added to the list. This is a new twist on the model. It's not a single DataPlatform, it's a list of DataPlatforms. At the end of the day, storing which dataplatform should be used likely belongs in github given it's permanence. There could be a map of DataPlatforms for each Workspace/DSG. The chooser can still be there to show candidates as new dataplatforms become available but a list of provisioned dataplatforms likely isn't a bad idea. Each could be marked with a Documentation and a ProductionStatus enum.

This list of DataPlatforms for a Workspace needs to be in the Ecosystem object. This is because the consumer adds the Workspace and then the central team needs to review the request and assign a platform. This can be automated also but it's still approved by a different team, the infra team, that the consumer.

## 2025/07/09

Working on github actions. The docker action needed updates to use the correct container name and I needed to add a docker hub PAT on the github side. The datasurface/datasurface container image is now being published.