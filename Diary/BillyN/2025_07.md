# July 2025

## 2025/07/02

I'm working with cursor in auto mode to verify the starter platform is correctly generating the kubernetes and airflow artifacts. It's working suprisingly well. The kubernetes artifacts are working on my local docker desktop kubernetes cluster. I built a docker container image for all the datasurface code which we need for airflow tasks as they are primarily kubernetes pod operators which need a container image. I'm building the image with this command:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

I have created a repository on docker hub called datasurface and the primary container image is there and called datasurface/datasurface:latest.

Right now, the bootstrap infrastructure is working and I wrote a [how to guide](../docs/HowToKubPGStarterSetup.md) for setting up the starter platform. The airflow DAG seems ok and the next step is to render the terraform files for the kafka connectors and the ingestion DAG.

Reworking the SQLIngestion stuff. I'm trying to simplify for the starter platform. The easiest ingestion involves as few moving parts as possible. A simple batch "select * from every table" to staging every batch is easy and useful. There are a lot of databases in companies which simply exist and have no fancy CDC software. A SQL full pull is the answer given how hard it is to reversion these datastores on more modern databases or add CDC to them.

## 2025/07/03

Working on the snapshot merge job. Frustration with SQLAlchemy type checking drove me back to 2.0 but it's still not great. I miss Java at times like this.

## 2025/07/04

Command to build the datasurface container image:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

Command to push the image to docker hub:

```bash
docker push datasurface/datasurface:latest
```

Looking at docker scout. I don't like all the vulnerablilities so I spent an hour today cleaning up the image, much better now and everything works on python 3.13 so thats good.

## 2025/07/07

Working on running the merge job in tests. Fixing some issues not passing DataContainers to the SQLIngestion classes. Making a simple ecosystem model for testing which
just has a single datastore and dataset with a single Workspace.

## 2025/07/08

My experience building the Goldman Sachs data lake is weighing on me now. I broke up the batch processing pipeline when I did this before. I had a job for start batch, ingest data, validate data, merge data, commint batch and then export jobs to push deltas to client data warehouses which were different that the merge storage. My merge storage was parquet files on HDFS and the consumers mostly used a sybase iq OLAP database. This introduced these problems:

* There are a lot of jobs. Imagine 8k datastores with 12 million tables and hundreds of Sybase IQ databases. WE ran in to job limitation in both the job scheduler and the YARN cluster job scheduler. We ended up having to collapse jobs in to bigger jobs to get the number of jobs under control at scale.
* Job scheduling latency was a major issue, mostly with YARN. It would sometimes takes minutes from when a job was scheduled and when it started running.
* We changed export jobs to use a custom job scheduler which limited the concurrency of export jobs on a specific Sybase IQ system as we were overloading IQ because of too many concurrent jobs especially when they did meta data changes.
* AWS Glue and Airflow do not scale well. Hundreds of jobs is about their limit as far as I can tell. I haven't used them yet but I read glue has a limit of a 100 jobs in their own documentation.

So, I know I'm going to need macro and micro schedulers, 2 levels with the inner level possibly a multi-threaded event handler for specific actions. The job will go through several stages. A batch starts, the data gets copied to staging from the source. The data is then merged in to the merge table and finally it gets commited. There is a metadata table recording batch state for each batch. Started, ingesting, merging, committed and failed. Multi-dataset ingestions can collapse these but some of these operations are very time consuming. Imagine the first batch for a billion row dataset, for example, you have to do it in sub batches and commit each one seperately with a restart point during ingestion. The batch would stay in ingesting for hours or even days.

Single dataset ingestion means the producer wants each dataset/table to be ingested independently off the other datasets at the source, there is no consistency between datasets. Multi-dataset ingestion means the producers wants all datasets to be ingested together within a single transaction, consistency between datasets is important. Single dataset ingestion explodes the number of jobs. Imagine a datastore with 500 datasets or more. There are trade offs here in terms of collapsing job counts and individual ingestion stream latency. Developers often focus on individual job or batch latency but do not look at the impact of this at scale. If we give everybody their own daemon running continously, this seems most optimal. Now, imagine the memory resources to do that for 12 million datasets, 24 hours a day versus running ingestion in a batch which it only uses memory when the batch is running, not all the time. One solution architectures don't work at scale. At the same time, a job per batch isn't always better either, it depends.

Thus the job DAG in airflow for a dataset might just be one job per ingestion stream. When it runs, it checks what needs to happen, continue ingesting to some limit? Stop ingesting, start merging? Start a new batch and resume ingesting. This is likely my first implementation. The job can be triggered either by an external trigger or timer or by the previous job finishing and then it schedules itself immeadiately. Given there are many such jobs, one per ingestion stream, they will run round robin until the batch commits and then wait for an external trigger like a timer.

The job is executed using the datasurface container image using a kubernetes pod operator. This can invoke the job python command. The job returns a string which is "KEEP_WORKING" if the job needs to go back in the queue and run again later or "DONE" indicating the current ingestion/merge has completed and now the job should wait for an external trigger to start the next batch.