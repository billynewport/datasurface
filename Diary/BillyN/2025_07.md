# July 2025

## 2025/07/02

I'm working with cursor in auto mode to verify the starter platform is correctly generating the kubernetes and airflow artifacts. It's working suprisingly well. The kubernetes artifacts are working on my local docker desktop kubernetes cluster. I built a docker container image for all the datasurface code which we need for airflow tasks as they are primarily kubernetes pod operators which need a container image. I'm building the image with this command:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

I have created a repository on docker hub called datasurface and the primary container image is there and called datasurface/datasurface:latest.

Right now, the bootstrap infrastructure is working and I wrote a [how to guide](../docs/HowToKubPGStarterSetup.md) for setting up the starter platform. The airflow DAG seems ok and the next step is to render the terraform files for the kafka connectors and the ingestion DAG.

Reworking the SQLIngestion stuff. I'm trying to simplify for the starter platform. The easiest ingestion involves as few moving parts as possible. A simple batch "select * from every table" to staging every batch is easy and useful. There are a lot of databases in companies which simply exist and have no fancy CDC software. A SQL full pull is the answer given how hard it is to reversion these datastores on more modern databases or add CDC to them.

## 2025/07/03

Working on the snapshot merge job. Frustration with SQLAlchemy type checking drove me back to 2.0 but it's still not great. I miss Java at times like this.

## 2025/07/04

Command to build the datasurface container image:

```bash
docker build -f Dockerfile.datasurface -t datasurface/datasurface:latest .
```

Command to push the image to docker hub:

```bash
docker push datasurface/datasurface:latest
```

Looking at docker scout. I don't like all the vulnerablilities so I spent an hour today cleaning up the image, much better now and everything works on python 3.13 so thats good.

## 2025/07/07

Working on running the merge job in tests. Fixing some issues not passing DataContainers to the SQLIngestion classes. Making a simple ecosystem model for testing which
just has a single datastore and dataset with a single Workspace.

## 2025/07/08

My experience building the Goldman Sachs data lake is weighing on me now. I broke up the batch processing pipeline when I did this before. I had a job for start batch, ingest data, validate data, merge data, commit batch and then export jobs to push deltas to client data warehouses which were different that the merge storage. My merge storage was parquet files on HDFS and the consumers mostly used a sybase iq OLAP database. This introduced these problems:

* There are a lot of jobs. Imagine 8k datastores with 12 million tables and hundreds of Sybase IQ databases. We ran in to job limitations in both the job scheduler and the YARN cluster job scheduler. We ended up having to collapse jobs in to bigger jobs to get the number of jobs under control at scale.
* Job scheduling latency was a major issue, mostly with YARN. It would sometimes takes minutes from when a job was scheduled and when it started running.
* We changed export jobs to use a custom job scheduler which limited the concurrency of export jobs on a specific Sybase IQ system as we were overloading IQ because of too many concurrent jobs especially when they did meta data changes.
* AWS Glue and Airflow do not scale well. Hundreds of jobs is about their limit as far as I can tell. I haven't used them yet but I read glue has a limit of a 100 jobs in their own documentation.

So, I know I'm going to need macro and micro schedulers, 2 levels with the inner level possibly a multi-threaded event handler for specific actions. The job will go through several stages. A batch starts, the data gets copied to staging from the source. The data is then merged in to the merge table and finally it gets commited. There is a metadata table recording batch state for each batch. Started, ingesting, merging, committed and failed. Multi-dataset ingestions can collapse these but some of these operations are very time consuming. Imagine the first batch for a billion row dataset, for example, you have to do it in sub batches and commit each one seperately with a restart point during ingestion. The batch would stay in ingesting for hours or even days.

Single dataset ingestion means the producer wants each dataset/table to be ingested independently off the other datasets at the source, there is no consistency between datasets. Multi-dataset ingestion means the producers wants all datasets to be ingested together within a single transaction, consistency between datasets is important. Single dataset ingestion explodes the number of jobs. Imagine a datastore with 500 datasets or more. There are trade offs here in terms of collapsing job counts and individual ingestion stream latency. Developers often focus on individual job or batch latency but do not look at the impact of this at scale. If we give everybody their own daemon running continously, this seems most optimal. Now, imagine the memory resources to do that for 12 million datasets, 24 hours a day versus running ingestion in a batch which it only uses memory when the batch is running, not all the time. One solution architectures don't work at scale. At the same time, a job per batch isn't always better either, it depends.

Thus the job DAG in airflow for a dataset might just be one job per ingestion stream. When it runs, it checks what needs to happen, continue ingesting to some limit? Stop ingesting, start merging? Start a new batch and resume ingesting. This is likely my first implementation. The job can be triggered either by an external trigger or timer or by the previous job finishing and then it schedules itself immeadiately. Given there are many such jobs, one per ingestion stream, they will run round robin until the batch commits and then wait for an external trigger like a timer.

The job is executed using the datasurface container image using a kubernetes pod operator. This can invoke the job python command. The job returns a string which is "KEEP_WORKING" if the job needs to go back in the queue and run again later or "DONE" indicating the current ingestion/merge has completed and now the job should wait for an external trigger to start the next batch.

### DataPlatform needs an opportunity to check the Workspace is good

CAme across a bug testing this stuff today. My test ecosystem Workspace has no data container which is causing problems as there are a few places where no data container caused chunks of code to be skipped. The Workspace has a lint method but it also needs to be linted by the chosen DataPlatform. There also needs to be a way for the DataPlatform to assign a data container to the Workspace when this is appropriate. Maybe, I need to make the Workspace datacontainer non optional and provide a datacontainer called DataPlatformAssigned for use. In the case of the starter platform the Workspaces will use the merge store as the data container for now, there is no export, consumers just query the merge database directly through workspace specific views for all datasetsinks needed.

DataPlatform needs a way to lint a Workspace when the chooser picks the DataPlatform for the Workspace. Workspaces are currently linted before the DataPlatform is chosen. Maybe, I extend the Workspace lint method to also allow the chosen DataPlatform to lint the Workspace. This seems early though. I'd prefer this linting to happen when the graph is generated.

Thinking outloud here. The other issue is once a pipeline is rendered and operational, switching to another Dataplatform needs guard rails. So, there needs to be infrastructure metadata stored outside git, a runtime infra database maybe, where the current DataPlatform for each Workspace is stored. If the DataPlatform is changed the the existing pipeline needs to be stopped, the new pipeline needs to be started and the metadata for the old pipeline needs to be migrated to the metadata for the new pipeline. For example, ingestion offsets would need to be preserved. Existing data needs to be preserved and possibly exported to the new dataPlatform's containers if needed. This should be automated but there needs to be a guard rail. Pause the existing pipeline on a long weekend for example and then do the migration and then we are on the new pipeline by the next business day. Or more likely, we keep the old pipeline running and start the new pipeline in parallel after migrating. Maybe the consumer can verify a provided test Workspace using the new pipeline before switching over and then the old pipeline can be stopped. This also allows a rollback where the old pipeline is kept in case there are issues with the new pipeline not uncovered during testing, it happens...

TODO for tomorrow. Allow DataPlatform to lint a Workspace in Workspace lint method. Once the graph is generated then a second linting pass can be done which already exists in DataPlatform. This can check all types/schemas/ingestion types/data containers/ are supported by the DataPlatform. This would happen during git actions so it will be visible to the user when pushing changes.

The infrastructure time handling of dataplatform changes requires a framework. To detect Workspace DataPlatform reassignments and to have a workflow to manage through that. The infra rendering code needs a historical list of DataPlatforms assigned to each Workspace and keep those working until there are decommissioned. Maybe, a Workspace instead has a list of DataPlatforms and DataPlatforms can only be added to the list. This is a new twist on the model. It's not a single DataPlatform, it's a list of DataPlatforms. At the end of the day, storing which dataplatform should be used likely belongs in github given it's permanence. There could be a map of DataPlatforms for each Workspace/DSG. The chooser can still be there to show candidates as new dataplatforms become available but a list of provisioned dataplatforms likely isn't a bad idea. Each could be marked with a Documentation and a ProductionStatus enum.

This list of DataPlatforms for a Workspace needs to be in the Ecosystem object. This is because the consumer adds the Workspace and then the central team needs to review the request and assign a platform. This can be automated also but it's still approved by a different team, the infra team, that the consumer.

## 2025/07/09

Working on github actions. The docker action needed updates to use the correct container name and I needed to add a docker hub PAT on the github side as a repository secret. The datasurface/datasurface container image is now being published. It all seems to be working now. The procedure to do a pypi or test pypi is to push the code after updating the version in setup.py. Then create a release with that version on github and the actions will run. The docker container and the pypi modules are being pushed correctly after todays fixes.

Filling out jobs.py iteratively. Command line error checking is there now with checks for various issues with the parameters.

## 2025/07/10

Switched to a postgres calculated hash for the hash columns during ingestion. This will be faster than calculating it in python.

## 2025/07/11

Working on the batch processing job. Almost there.

## 2025/07/12

Worrying about the job again today. A potential issue is that the schema for a store changes mid batch. This will cause issues. It makes sense to store a hash of the store schema when the batch starts and
then check it hasn't changed when committing the batch. Another way would be to store the list of datasets when the batch starts and then only use those during the batch, not the set of datasets in the store from the model as these could change if the job restarts. Just tracking the datasets is enough. Datasurface doesn't allow datasets to be deleted or dataset schemas to change in a non backwards compatible way. Worst case, we get extra columns but even here we can get failures. Maybe, the extra columns have not been added yet in the staging/merge tables for the dataset. Any kind of issue like this during the job should be detected and just cause the batch to be restarted. The merge for all datasets happens in a single transaction so thats ok and no clean up needed. The ingestion happens seperately but we can just delete the staging tables for the datasets as part of batch reset/batch start. Truncating the tables is usually faster than deleting rows.

Starting writing the snapshot merge job tests. Doing this on a different macbook, always healthy to switch. Upgraded it to postgres 16 as we need 16 for MERGE support.

## 2025/07/13

Upgraded PostgreSQL from version 14 to 17 (latest) using Homebrew. The process was:

1. **Stop the old version:**

   ```bash
   brew services stop postgresql@14
   ```

2. **Install PostgreSQL 17:**

   ```bash
   brew install postgresql@17
   ```

3. **Update PATH in shell config:**

   ```bash
   echo 'export PATH="/opt/homebrew/opt/postgresql@17/bin:$PATH"' >> ~/.zshrc
   source ~/.zshrc
   ```

4. **Start PostgreSQL 17:**

   ```bash
   brew services start postgresql@17
   ```

5. **Verify the upgrade:**

   ```bash
   psql --version  # Should show 17.5
   psql -d postgres -c "SELECT version();"
   ```

6. **Remove the old version:**

   ```bash
   brew uninstall postgresql@14
   brew cleanup --prune-prefix
   ```

PostgreSQL 17 is now running and provides full MERGE statement support which is needed for the job logic. The upgrade creates a fresh database cluster, so any existing databases would need to be recreated, but for testing purposes this is perfect since I'm creating test databases anyway.

Now I can continue with the job logic testing with confidence that all PostgreSQL 17 features including MERGE are available.

* Also encountered a PostgreSQL connection error: `FATAL: role "postgres" does not exist` after upgrading to PostgreSQL 17. The upgrade created a fresh cluster without the default 'postgres' user.
* Fixed this by running:

  ```bash
  psql -d postgres -c "CREATE USER postgres WITH PASSWORD 'postgres' SUPERUSER;"
  ```

* This allowed the tests and job logic to connect as expected using the 'postgres' user.

### Fix DAG

With the job now working in a basic form, I'd like to generate the ingest DAG to test running the jobs in airflow. The idea is to use airflow dynamic jobs. We would create an airflow DAG with a single job per ingestion stream. Thats one job per multi-dataset ingestion datastore or one job per dataset per datastore for single-ingestion streams. That job is trigger by an external trigger or cron timer. It will then run and return one of 3 values, KEEP_WORKING, DONE or ERROR. A value of DONE means wait for the next trigger (timer or external). KEEP_WORKING means that the job partially completed and is yielding its slot for other jobs to run. However, we want this job scheduled ASAP. A value of error means the job failed and should wait for devops to investigate/restart it.

## 2025/07/14

Updated the kubpgstarter platform to support SQL snapshot ingestion DAGs. The key changes were:

1. **Added SQL Snapshot Ingestion Linting**: Created `lintSQLSnapshotIngestion()` method to validate SQL snapshot ingestion stores, checking for required credentials and data containers. Both single and multi-dataset ingestion are supported.

2. **Updated Graph Linting**: Modified `lintGraph()` to handle both Kafka and SQL snapshot ingestion types instead of being Kafka-only.

3. **Updated Terraform Generation**: Modified `createTerraformForAllIngestedNodes()` to only generate Kafka infrastructure for Kafka ingestion stores. SQL snapshot ingestion doesn't need Kafka infrastructure since the SnapshotMergeJob handles ingestion directly from source databases.

4. **Enhanced DAG Template**: Updated the `ingestion_dag.py.j2` template to conditionally include credentials based on ingestion type:
   * Kafka ingestion: Includes Kafka Connect credentials
   * SQL snapshot ingestion: Includes source database credentials

5. **Updated DAG Generation**: Modified `createAirflowDAG()` to pass the correct credential information to the template based on the ingestion type of each store.

The platform now properly supports both Kafka and SQL snapshot ingestion workflows, generating appropriate Airflow DAGs for each ingestion stream with the correct credential configuration.

The system now works by generating a DAG for each ingestion stream. There didn't seem to be a nice way of having thousands of merge jobs within a single DAG for the data platform and have job by job restart control. Splitting it in to a DAG named after <platformname,ingestionstreamname> is a good solution.

I don't like the name of this data platform, it's too long. I think I'll just call it a color. It's as balance of trying to make the name descriptive of what it does or use a 'brand' name. I'll call it Yellow for now, YellowPlatform.

### Mapping Workspace/DatasetGroup to DataPlatform

Initially, the platformChooser on the DSG indicates which DataPlatform to use for the Workspace. This isn't flexible enough. We need to allow at least 2 DataPlatforms to be assigned to a Workspace. This is because migrating from one DataPlatform to another requires the new DataPlatform to be hydrated with the data and for the team to verify they are happy with the new DataContainer holding their data. Only then can the old DataPlatform be decommissioned. This mapping is managed centrally by the devops team. I think storing it in github also makes sense, it rarely changes. I would like code to be able to read and write this mapping. Storing the mapping as a python object seems harder than storing it as a json file which can be easily read/written by a tool. The Ecosystem repository should be the master for this, the consumer Teams can change the platformMD for the DSG but it's not their call to pick a suitable DataPlatform. It could be fully automatic. The merge handler could call platformMD.chooseDataPlatform and if the DataPlatform is not in the list, it could add it to the json file with a status of PROVISIONING. Each mapping needs a status, PROVISIONING, PROVISIONED, DECOMMISSIONING, DECOMMISSIONED. A Datasurface model in a repo should currently have a eco.py file in the root which allows the model to be loaded. The mapping files should also be in the root of the repo and called 'dsg_platform_mapping.json'. It will be loaded with the Ecosystem model. A devops person or a tool can edit that file to change the mapping. Extreme care will need to be taken to changes to this, the scope for utterly screwing everything up is large. I would argue than sageguards on the provisioner will be needed to limit changes. If a mapping is currently provisioned and a new model version shows it missing then it should fail and throw exceptions to devops.

While doing the above, I found a bug in the tests I wrote to check that the DSL objects all have complete equality methods and so on. The code using an AST to parse the DSL source code, looking for all instance variables in the '__init__' method. It detected variables created using assignment but not using annotated assignment. After this was fixed, 12 other DSL objects failed the tests which I'll fix now.

I fixed the tests for eq completeness and all the downstream eq methods now being flagged. One wierd issue is that AllowDisallowPolicy had an incomplete eq method which was specifically being tested that it ignored the missing variables which I don't understand. This would mean that unauthorized users can changed the allow and disallow sets of an AllowDisallowPolicy which makes no sense to me. I disabled the test for now and make the eq method of the policy complete.

I've implemented the dsg-dataplatform assignment mapping on the ecosystem. I updated existing create ecosystem methods to programmatically add the mapping based on the dsg platformMD. The YellowDataPlatform is using the json artifact which loads correctly.

I added datacontainers for the common sqlalchemy databases so i can ingest data from any of those easily in to the yellow merge store. IBM DB2 looks like an issue as it's not supported on arm64. I'll need to find a way to support it but also, this is ibms problem, not mine. Hosting the container on x86 would fix this also, it's an arm64 issue.

## 2025/07/15

Looking into implementing a milestoned merge job also for Yellow. Looking at how I'd imagined using SchemaProjector. This was meant to be a way to provide users with the actual schema of the dataset that DataSurface exposes to consumers. This will be different than the schema provided by a data owner. This is because the datacontainer will likely be different on both sides. Database type differences, column naming, table/view naming all will be different. The DataPlatform may also add extra columns to schemas indicating various platform features. I have currently placed SchemaProject as an abstract method on DataContainer but am realizing now that it's the wrong place. It needs to be on the DataPlatform. The method should take the Workspace, DatasetGroup and Dataset and return a Schema. The SchemaProjector will then be used to project the schema to the DataContainer being used by the Workspace.

I refactored the YellowDataPlatform to use the SchemaProjector the correct way. All the platform specific schema management is now in its SchemaProjector as used as the authorative schema in the jobs.

The merge code in the Snapshotmerge is not portable across databases so only postgres is supported for now.

I added a command line utility to YellowDataPlatform. This will traverse the graph for a specific data platform and create the views for any Workspace/DSG datasets. These views point at the merge tables and exclude the synthetic columns. This script always updates the views when needed. These command line utilities should be triggered by any merge handler from github actions. The command returns 0 if all views were created/updated successfully. It should loop after a time when it returns non 0 values. It can fail for various reasons, the merge tables may not exist or not contain columns needed for the view. The merge/ingestion jobs are where staging and merge tables schemas are maintained so these jobs might  need to run to update schemas before the views can be altered or created.

So, lots of progress over the last few days. Next step is to provide a milestoned ingestion capability and tests.

Added in support for the assigned DataPlatforms to verify that the Workspace/DSG's requirements are supported by the DataPlatform. For example, a user which needs forensic milestoning cannot use a DataPlatform which only supports live only.

Implemented basic batch milestoning for snapshot ingestion with test cases. Refactoring a bunch of the test code in common between the live only and forensic tests.

## PostgreSQL Version Compatibility Fix

Fixed a critical compatibility issue in the forensic merge job. The original code used PostgreSQL 17 MERGE syntax with `WHEN NOT MATCHED BY TARGET` and `WHEN NOT MATCHED BY SOURCE` clauses, but I'm running PostgreSQL 16.9 which doesn't support these clauses.

**Solution:** Refactored `SnapshotMergeJobForensic.mergeStagingToMerge()` to use PostgreSQL 16 compatible syntax by replacing the single MERGE statement with 4 separate SQL operations:

1. **Close changed records**: UPDATE for records where hash differs 
2. **Close deleted records**: UPDATE for records not in staging (simulates WHEN NOT MATCHED BY SOURCE)
3. **Insert new records**: INSERT for new records (simulates WHEN NOT MATCHED BY TARGET)  
4. **Insert new versions**: INSERT new versions for changed records

This maintains identical forensic behavior with proper history tracking while being compatible with PostgreSQL 16. Performance is excellent (1-2ms per operation) and all tests pass successfully. The 4-step approach is functionally equivalent to PostgreSQL 17's advanced MERGE syntax but works on older PostgreSQL versions.

## 2025/07/16

I've made solid progress on YellowDataPlatform over the last 2 weeks. The ingest jobs are working for sql snapshot which is all I need for now at least. I want to get to MVP over the next week or so. I want to have my model in git. I want the MVP to be:

* A single data producer which has a database holding simple customer data. A customer table with a few columns with name/address/tel no and so on. An address table with a simple US addresses linked to the customer table. A billing address and a multiple shipping addresses. I want this described in the ecosystem model.
* A single consumer who has a Workspace with 2 DSGs. One DSG is using live_only with a 1 minute refresh rate and the other is using forensic with a 10 minute refresh rate. Each DSG has Datasetsinks for all tables in the producer database.

Nothing needs to be provisioned until DataPlatforms are assigned to the DSGs. There will be 2 DataPlatforms, a live yellow dataplatform and a forensic yellow dataplatform.

Once the assignments are commited to the main branch of the git repo then the MERGE Handler job needs to be triggered. This will create the ingestion DAG for both DataPlatforms. We should then see the DAGs for both ingestion streams, there should be a DAG for the live stream from the producer and another for the forensic stream. The consumer will query the data from views created for the Workspace/DSGs using the view reconciler command line tool in YellowDataPlatform.

I'd like a change simulator running on the producer database which creates and updates customers, addresses constantly so there is data changing to be merged.

I've made a plan to get to MVP. These are always interesting sprints as they help to highlight issues. For example, right now I have a crontrigger on the ingestion metadata for a Datastore. This is the wrong place for it possibly. My live only and forensic DSGs would use the same trigger which is maybe wrong. However, really I need to optimize the overall DAG across DataPlatforms. Especially for SQL snapshot ingestions, the load of frequent 'select *' full table queries on the database is a problem. Doing this for each DataPlatform makes it worse. CDC using debezium or a delta based approach would be better. My previous experience always used a single ingestion step regardless of consumer loads which producers obviously liked. The other reality is that how many DataPlatform instances are there really going to be. A handful I think. Each DataPlatform should be able to support many consumers, ideally one dataplatform can support ALL consumers whose requirements are similar. Datasurface operations teams will try to minimize the number of DataPlatforms they need to support.

### DataPlatformManagedDataContainer

I've added a new DataContainer type called DataPlatformManagedDataContainer. This is used on Workspaces to specify a DataContainer that is provided by the DataPlatform
assigned to the Workspace. Some DataPlatforms may only support this type of container if they do not support pushing data to different explicit data containers for
Workspaces, for example, a consumer wants the data pushed to an existing database where they want the data along with other data they have in that database.

I've updated the YellowDataPlatform to use this new DataContainer type. The consumer will have a Workspace with a DataPlatformManagedDataContainer as the data container.
The DataPlatform will then create the merge tables and views for the Workspace/DSG datasets.

The consumer will then query the data from views created for the Workspace/DSGs using the view reconciler command line tool in YellowDataPlatform.

### MVP Progress

Plugging along with the MVP plan. I've created a git repository at billynewport/mvpmodel which contains the mvp eco.py and the dsg assignment json file.

## 2025/07/17

Made a lot of progress yesterday on MVP, long day. The live and forensic jobs now work properly in airflow and kubernetes. I can see data moving through the pipeline when the simulator is running to generate changes. Today, we will create the workspace views and check the infrastructure DAGs work correctly. I don't like that the repo is cloned within the jobs. This seems inefficient to me. I'd prefer to clone them once when the merge handler runs and then the job pods can mount that folder as a volume. I can't imagine hundreds or thousands of jobs running git clone is a good thing when the repo barely changes. The only issue here is concurrency issues, a clone happening during a job run and causing issues. Maybe, the merge handler clones in to a temp folder and then we rename it to a permanent folder based on the timestamp. Something like this: git clone to a temp folder. When the clone finishes then rename the temp folder to a folder called "model-{timestamp}". The job pods when they run can select the model folder with the highest timestamp. We can then prune the older model-folders as a job which runs monthly for example. Another approach would be to use a sym link pointing at the latest model but that still allows the folder to change mid job when the model is being loaded.

The model-{timestamp_secs} approach is the best. The job can just pick the latest model when it starts, that won't change mid job. This seems best. The model folder should be post-fixed with the current UTC timestamp in seconds. The seconds portion should have a fixed number of digits so that a simple sort will pick the latest model. UTC is important because when day light savings changes happen, wierd stuff will happen, models going back in time etc. 10 digit padding for the timestamp is enough.

```python
import time
timestamp = int(time.time())  # Current UTC seconds since epoch
folder_name = f"model-{timestamp:010d}"  # Zero-padded to 10 digits
```

Fixed the git clone configuration in the data ingestion jobs. The hardcoded repository name `billynewport/mvpmodel` has been replaced with configurable parameters extracted from the Ecosystem's `owningRepo`. Added command line parameters `--git-repo-owner`, `--git-repo-name`, and `--git-repo-branch` to the jobs. The DAG templates now pass these parameters from the GitHubRepository object, properly parsing the `repositoryName` field (format: "owner/repo") and using the `branchName` field. This ensures the jobs clone the exact repository and branch specified in the ecosystem model rather than hardcoded values. The git clone command now uses `git clone --branch {branch} {url}` to clone the specific branch.

## Future Git Repository Architecture

The current implementation has each job pod performing its own git clone, which is inefficient and creates potential concurrency issues. The planned architecture will work as follows:

**Merge Handler Responsibility:**
- When ecosystem model changes are committed to the main branch, the MERGE Handler will perform a single git clone operation
- The cloned repository will be stored in a persistent location on the Airflow server
- This eliminates hundreds/thousands of redundant git clone operations

**Job Pod Execution:**
- Individual job pods will NO LONGER perform git clone operations
- Instead, the pre-cloned model folder will be mounted as a volume in each job pod
- Jobs will simply read from the `--git-repo-path` directory that contains the already-cloned ecosystem model
- This provides much better performance and eliminates git clone concurrency issues

**Benefits:**
- **Performance**: Single clone per model update vs. clone per job execution
- **Consistency**: All jobs use the same model version during a batch cycle  
- **Reliability**: No git network failures during job execution
- **Concurrency Safety**: Eliminates race conditions from multiple simultaneous clones
- **Resource Efficiency**: Reduced network bandwidth and storage usage

**Implementation Notes:**
- The current `--git-repo-owner`, `--git-repo-name`, and `--git-repo-branch` parameters will be used by the MERGE Handler for the initial clone
- Job pods will receive only `--git-repo-path` pointing to the mounted pre-cloned directory
- Volume mounting strategy needs to support read-only access from multiple concurrent pods
- Cleanup strategy needed for old model versions when new commits are processed
