# SwarmRenderer

The model is kept as a python DSL which is checked in to a repository. When changes are checked in to the repository, DataSurface needs to examine the new model and render it using the various DataPlatforms that it is configured to use.

The SwarmRender is an implementation of a BrokerRenderEngine which renders DataPlatforms within a Docker Swarm environment. There are other implementations of Brokers such as a Kubernetes version (to be implemented later).

## Approach

I'm going to assume we're using a git style repository. These have merge events which are triggered by pull requests being approved and merged in to the main branch. New repository revisions are identified by a unique revision number.

### MergeHandler service

We will define a one-shot swarm service which has all the necessary secrets mapped to it and the command is to execute the merge handler of the Ecosystem. This will trigger the SwarmBrokerRenderEngine which kicks everything off.

The restart policy of this service needs to be set to on-failure. This makes it a oneshot job.

The DataPlatforms will have their service definitions generated by the merge handler. Once they are provisioned then the merge handler to invoke them as one-shot services also to handle merge events to the repository.

The assignment of DataPlatforms to intention graph DAGs is deterministic. These were already linted when the pull request handler was managing the merge, therefore, the model is valid (unless an admin forced a merge of a broken model). We validate because of that.

The merge handler will need to be able to start up docker containers wrapping each DataPlatform. Each DataPlatform container will need credentials mounted for the DataPlatform to use to access DataContainers that it requires. The model cloned to the file system will need to be mounted as a volume in the container also.

Ideally, the merge docker container would have all the credentials mounted for everything. It can then subset them for the DataPlatform containers to access. However, this is a security risk also. A previous system had all the credentials materialized on the local file system and it then used groups and a user per container to manage what each container could access.

## Container Platform

The MergeHandler will be built around Docker Swarm. It will use the Docker Python SDK to start and stop containers. The users will configure all the credentials as secrets accessible to the top level MergeHandler container. The MergeHandler will then use the Docker Python SDK to start up the DataPlatform containers and pass in the secrets needed using -v.

## Credential management

The Model has the ability to define CredentialStores and specify Credentials within those stores. These secrets are provided by the user and then provided to DataPlatforms when they need to access DataContainers. We are going to assume Docker Swarm is used as the container framework for SimpleDataPlatform. Thus, the user needs to configure the Swarm cluster with all the necessary secrets and allow the MergeHandle to access them. The MergeHandler will need to start up DataPlatforms using docker run and pass in the secrets needed using -v.

## DataPlatform state management

The MergeHandler will need a persistent volume for DataSurface state. Each DataPlatform instance will have a subfolder in that volume for its state, including terraform state. This will be persistent across container restarts. The subfolder will be named after the DataPlatform name which is unique within the model. The subfolder for each DataPlatform will be mounted as a fixed volume when the DataPlatform instance containers are started with -v.

## DataPlatform configuration

Each DataPlatform will need a docker image name for MergeHandler to use when starting it.

## MergeHandler configuration

The MergeHandler will be a docker swarm specific implementation. It should be configured within the model in git also. It needs its own configuration within the DSL along with items such as a DataPlatform type to docker container name mapping.

## Operation

The MergeHandle will execute. It will produce a docker compose file which represents the entire Ecosystem. It will use the docker deploy CLI command to start the services defined in the file or update them/add/remote services if needed.

* We will use NFS as a volume driver.
* We will have a bootstrap which invokes the MergeHandler from its container to bootstrap the services on the swarm cluster.
* We will run the MergeHandler as a one-shot service periodically.
* This will generate a docker compose file for the system and then deploy it.

Administrators will need to create/update secrets in the swarm cluster.
